---
title: "Hale_Code"
author: "BH"
date: "2023-12-07"
output: html_document
---

This RMarkdown file contains all code needed to reproduce the microbiome statistical analysis described in "Single-molecule-based characterization of the soybean rhizosphere microbiome." Phyloseq objects and sample metadata are provided in ".RData". Questions/comments pertaining to the code can be sent to Brett Hale (brett@agrigro.com) and Asela Wijeratne (awijeratne@astate.edu)

```{r}
##load packages
library(colorspace)
library(stringi)
library(rhdf5)
library(zlibbioc)
library(S4Vectors)
library(phyloseq)
library(Biostrings)
library(yaml)
library(colorspace)
library(ggplot2)
library(tibble)
library(indicspecies)
library(vegan)
library(phyloseq)
library(tidyr)
library(MetBrewer)
library(devtools)
library(dplyr)
library(ggpubr)
library(stringr)
library(Maaslin2)
library(microbiome)
library(metagenomeSeq)
#library(xlsx)
#library(readxl)
```

```{r}
##The 16S phyloseq object is called "prok_phyloseq"
##Convert phyloseq to data frame
df <- as.data.frame(sample_data(prok_phyloseq))


##Library size x sample type
df$LibrarySize <- sample_sums(prok_phyloseq)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))  

ggplot(data=df, aes(x=Index, y=LibrarySize, color=bio_or_control)) + 
  geom_point(size=2.5, alpha = 0.75) +
  theme_bw() +
  theme(axis.text = element_text(size=16)) +
  theme(axis.title = element_text(size = 16)) +
  theme(legend.title = element_text(face = "bold", size = 10)) +
  theme(legend.text = element_text(face = "bold", size = 10)) +
  scale_color_manual(name = "Sample Type", labels = c("Biological", "Control"), values = c("#2f357c","#bf3729"))

ggsave("prok_Library_size.png", width = 10, height = 10, units = "in", dpi = 900)
```

```{r}
##The 18S-ITS phyloseq object is called "fun_phyloseq"
##Convert phyloseq to data frame
fun.df <- as.data.frame(sample_data(fun_phyloseq))
str(df)

##Library size x sample type
fun.df$LibrarySize <- sample_sums(fun_phyloseq)
fun.df <- fun.df[order(fun.df$LibrarySize),]
fun.df$Index <- seq(nrow(fun.df))  

ggplot(data=fun.df, aes(x=Index, y=LibrarySize, color=bio_or_control)) + 
  geom_point(size=2.5, alpha = 0.75) +
  theme_bw() +
  theme(axis.text = element_text(size=16)) +
  theme(axis.title = element_text(size = 16)) +
  theme(legend.title = element_text(face = "bold", size = 10)) +
  theme(legend.text = element_text(face = "bold", size = 10)) +
  scale_color_manual(name = "Sample Type", labels = c("Biological", "Control"), values = c("#2f357c","#bf3729"))

ggsave("fungi_library_size.png", width = 10, height = 10, units = "in", dpi = 900)
```

```{r}
##16S decontamination
##Decontaminate samples based on kitome and sequencing negative controls
##install and load decontam package- https://link.springer.com/article/10.1186/s40168-018-0605-2
##devtools::install_github("benjjneb/decontam", force = TRUE)
##library(decontam)

##Run decontam to identify potential contaminants
sample_data(prok_phyloseq)$is.neg <- sample_data(prok_phyloseq)$bio_or_control == "control"

##Make phyloseq object of presence-absence in negative controls and true samples
prok.phyloseq.pa <- transform_sample_counts(prok_phyloseq, function(abund) 1*(abund>0))
prok.phyloseq.pa.neg <- prune_samples(sample_data(prok.phyloseq.pa)$bio_or_control == "control", prok.phyloseq.pa)
prok.phyloseq.pa.pos <- prune_samples(sample_data(prok.phyloseq.pa)$bio_or_control == "biological", prok.phyloseq.pa)

#### + try different thresholds ####
#### +++ 0.1 ####
contamdf.prev01 <- isContaminant(prok_phyloseq, method="prevalence", neg="is.neg", threshold=0.1)
##table(contamdf.prev01$contaminant)
##head(which(contamdf.prev01$contaminant))

##Make data.frame of prevalence in positive and negative samples
df.pa01 <- data.frame(pa.pos=taxa_sums(prok.phyloseq.pa.pos), pa.neg=taxa_sums(prok.phyloseq.pa.neg),
                    contaminant=contamdf.prev01$contaminant)

ggplot(data=df.pa01, aes(x=pa.neg, y=pa.pos, color=contaminant)) + 
  #geom_point(size=2.5, alpha = 0.75) +
  xlab("Prevalence (Negative Controls)") + 
  ylab("Prevalence (True Samples)") +
  theme_bw() +
  geom_jitter(size=2.5, alpha = 0.75) +
  theme(axis.text = element_text(size=16)) +
  theme(axis.title = element_text(size = 16)) +
  theme(legend.title = element_text(face = "bold", size = 10)) +
  theme(legend.text = element_text(face = "bold", size = 10)) +  
  scale_color_manual(name = "Contaminant", labels = c("False", "True"), values = c("#2f357c","#bf3729")) +
  ggtitle("Prevalence threshold = 0.1")

ggsave("Decontam_prevalence_threshold_0.1.png", width = 10, height = 10, units = "in", dpi = 900)


#### +++ 0.5 ####
contamdf.prev05 <- isContaminant(prok_phyloseq, method="prevalence", neg="is.neg", threshold=0.5)
##table(contamdf.prev05$contaminant)
##head(which(contamdf.prev05$contaminant))


##Make data.frame of prevalence in positive and negative samples
df.pa05 <- data.frame(pa.pos=taxa_sums(prok.phyloseq.pa.pos), pa.neg=taxa_sums(prok.phyloseq.pa.neg),
                      contaminant=contamdf.prev05$contaminant)

ggplot(data=df.pa05, aes(x=pa.neg, y=pa.pos, color=contaminant)) + 
  #geom_point(size=2.5, alpha = 0.75) +
  xlab("Prevalence (Negative Controls)") + 
  ylab("Prevalence (True Samples)") +
  theme_bw() +
  geom_jitter(size=2.5, alpha = 0.75) +
  theme(axis.text = element_text(size=16)) +
  theme(axis.title = element_text(size = 16)) +
  theme(legend.title = element_text(face = "bold", size = 10)) +
  theme(legend.text = element_text(face = "bold", size = 10)) +  
  scale_color_manual(name = "Contaminant", labels = c("False", "True"), values = c("#2f357c","#bf3729")) +
  ggtitle("Prevalence threshold = 0.5")

ggsave("Decontam_prevalence_threshold_0.5.png", width = 10, height = 10, units = "in", dpi = 900)

##Write out the results for final analysis 
write.csv(df.pa05, "./DecontamResults_prev05.csv")

##Remove contaminants
prok_phyloseq_decontam <- prune_taxa(!df.pa05$contaminant, prok_phyloseq)

##With contaminants removed; must mute fisher.alpha
otu_table(prok_phyloseq_decontam) 
```

```{r}
##18S-ITS decontamination
##Decontaminate samples based on kitome and sequencing negative controls
##install and load decontam package- https://link.springer.com/article/10.1186/s40168-018-0605-2
##devtools::install_github("benjjneb/decontam", force = TRUE)
##library(decontam)

##Run decontam to identify potential contaminants
sample_data(fun_phyloseq)$is.neg <- sample_data(fun_phyloseq)$bio_or_control == "control"

##Make phyloseq object of presence-absence in negative controls and true samples
fun.phyloseq.pa <- transform_sample_counts(fun_phyloseq, function(abund) 1*(abund>0))
fun.phyloseq.pa.neg <- prune_samples(sample_data(fun.phyloseq.pa)$bio_or_control == "control", fun.phyloseq.pa)
fun.phyloseq.pa.pos <- prune_samples(sample_data(fun.phyloseq.pa)$bio_or_control == "biological", fun.phyloseq.pa)

#### + try different thresholds ####
#### +++ 0.1 ####
fun.contamdf.prev01 <- isContaminant(fun_phyloseq, method="prevalence", neg="is.neg", threshold=0.1)
table(fun.contamdf.prev01$contaminant)
head(which(fun.contamdf.prev01$contaminant))

##Make data.frame of prevalence in positive and negative samples
fun.df.pa01 <- data.frame(pa.pos=taxa_sums(fun.phyloseq.pa.pos), pa.neg=taxa_sums(fun.phyloseq.pa.neg),
                    contaminant=fun.contamdf.prev01$contaminant)

ggplot(data=fun.df.pa01, aes(x=pa.neg, y=pa.pos, color=contaminant)) + 
  #geom_point(size=2.5, alpha = 0.75) +
  xlab("Prevalence (Negative Controls)") + 
  ylab("Prevalence (True Samples)") +
  theme_bw() +
  geom_jitter(size=2.5, alpha = 0.75) +
  theme(axis.text = element_text(size=16)) +
  theme(axis.title = element_text(size = 16)) +
  theme(legend.title = element_text(face = "bold", size = 10)) +
  theme(legend.text = element_text(face = "bold", size = 10)) +  
  scale_color_manual(name = "Contaminant", labels = c("False", "True"), values = c("#2f357c","#bf3729")) +
  ggtitle("Prevalence threshold = 0.1")

ggsave("fungi_decontam_prevalence_threshold_0.1.png", width = 10, height = 10, units = "in", dpi = 900)


#### +++ 0.5 ####
fun.contamdf.prev05 <- isContaminant(fun_phyloseq, method="prevalence", neg="is.neg", threshold=0.5)
table(fun.contamdf.prev05$contaminant)
head(which(fun.contamdf.prev05$contaminant))


##Make data.frame of prevalence in positive and negative samples
fun.df.pa05 <- data.frame(pa.pos=taxa_sums(fun.phyloseq.pa.pos), pa.neg=taxa_sums(fun.phyloseq.pa.neg),
                      contaminant=fun.contamdf.prev05$contaminant)

ggplot(data=fun.df.pa05, aes(x=pa.neg, y=pa.pos, color=contaminant)) + 
  #geom_point(size=2.5, alpha = 0.75) +
  xlab("Prevalence (Negative Controls)") + 
  ylab("Prevalence (True Samples)") +
  theme_bw() +
  geom_jitter(size=2.5, alpha = 0.75) +
  theme(axis.text = element_text(size=16)) +
  theme(axis.title = element_text(size = 16)) +
  theme(legend.title = element_text(face = "bold", size = 10)) +
  theme(legend.text = element_text(face = "bold", size = 10)) +  
  scale_color_manual(name = "Contaminant", labels = c("False", "True"), values = c("#2f357c","#bf3729")) +
  ggtitle("Prevalence threshold = 0.5")

ggsave("fungi_decontam_prevalence_threshold_0.5.png", width = 10, height = 10, units = "in", dpi = 900)

##Write out the results for final analysis 
write.csv(df.pa05, "./DecontamResults_prev05.csv")

##Remove contaminants- went ahead and executed the following code although no contaminants were detected/removed
fun_phyloseq_decontam <- prune_taxa(!fun.df.pa05$contaminant, fun_phyloseq)

##Remove plant 
fun_phyloseq_decontam <- subset_taxa(fun_phyloseq_decontam, kingdom != "Viridiplantae")

##Remove nematode
fun_phyloseq_decontam <- subset_taxa(fun_phyloseq_decontam, kingdom != "Metazoa")
```

```{r}
##16S
##Percent of mapped ASVs mapped to species level
# Rows where 'kingdom' has a non-empty value (not NA and not "")
valid_kingdom_rows <- taxmat1 %>% filter(kingdom != "", !is.na(kingdom))

# Rows where 'species' has a non-empty value (not NA, not "", and does not end with "sp.")
valid_species_rows <- valid_kingdom_rows %>% filter(species != "", !is.na(species), !grepl("sp.$", species))

# Calculate the percentage
percentage <- nrow(valid_species_rows) / nrow(valid_kingdom_rows) * 100

percentage ##100%

##Let's get % at strain level
# Determine the number of words in each species entry
num_words <- sapply(strsplit(as.character(valid_kingdom_rows$species), " "), length)

# Calculate the percentage of rows with more than two words
percentage <- sum(num_words > 2) / nrow(valid_kingdom_rows) * 100
percentage

# Calculate the number of rows with more than two words
total_rows <- sum(num_words > 2)
total_rows

##Let's zero in on Bradyrhizobium
# Extract the first two words from rows containing "Bradyrhizobium" and print unique values
valid_kingdom_rows %>%
  filter(str_detect(species, "Bradyrhizobium")) %>%
  mutate(first_two_words = str_extract(species, "^(\\w+\\s\\w+)")) %>%
  distinct(first_two_words) %>%
  filter(!is.na(first_two_words)) %>%
  pull(first_two_words) -> unique_two_words
print(unique_two_words)

# Extract unique species values with "Bradyrhizobium" and three or more words, then print them
valid_kingdom_rows %>%
  filter(str_detect(species, "Bradyrhizobium")) %>%
  filter(str_count(species, "\\s+") + 1 >= 3) %>%
  distinct(species) %>%
  pull(species) -> unique_three_or_more_words
print(unique_three_or_more_words)

```

```{r}
##18S-ITS
##Percent of mapped ASVs mapped to species level
fun.tax.percent <- as.data.frame(tax_table(fun_phyloseq_decontam))

##Rows where 'kingdom' has a non-empty value (not NA and not "")
valid_kingdom_rows.fun <- fun.tax.percent %>% filter(kingdom != "", !is.na(kingdom))

##Rows where 'species' has a non-empty value (not NA, not "", and does not end with "sp.")
valid_species_rows.fun <- valid_kingdom_rows.fun %>% filter(species != "", !is.na(species), !grepl("sp.$", species))

##Calculate the percentage
percentage <- nrow(valid_species_rows) / nrow(valid_kingdom_rows) * 100

percentage ##44.77318

##Let's look at the protists a bit closer
# Filter rows where 'kingdom' is not 'Fungi' and count unique rows
unique_non_fungi_rows <- valid_kingdom_rows.fun %>%
  filter(kingdom != 'Fungi') %>%
  distinct()

# Report the number of unique rows
num_unique_rows <- nrow(unique_non_fungi_rows)
num_unique_rows
```

```{r}
##16S
##Note that R2 growth stage is listed as R3 throughout this document-this is corrected in final figures
##Calculate alpha diversity metrics with prok_phyloseq_decontam
bacterial_alpha <- estimate_richness(prok_phyloseq_decontam, measures = c("Observed", "Chao1", "ACE", "Shannon", "Simpson", "InvSimpson"))

##make rownames a column
bacterial_alpha <- tibble::rownames_to_column(bacterial_alpha, "sample")

##Calculate evenness- https://rdrr.io/github/microbiome/microbiome/man/evenness.html
##library(BiocManager)
##BiocManager::install("microbiome")
##library(microbiome)

bacterial_evenness <- evenness(prok_phyloseq_decontam, 'pielou')

##make rownames a column
bacterial_evenness <- tibble::rownames_to_column(bacterial_evenness, "sample")
colnames(bacterial_evenness)[2] ="Pielou"

##merge all alpha indices
bacteria_alpha_compiled <- merge(bacterial_alpha, bacterial_evenness, by = c("sample"))

##remove negative controls
bacteria_alpha_compiled <- bacteria_alpha_compiled[-c(1, 63), ]

##make long-form in order to plot
bacteria_alpha_compiled$sample <- factor(bacteria_alpha_compiled$sample)

bacteria_alpha_compiled_long <- gather(bacteria_alpha_compiled, metric, value, Observed:Pielou, factor_key=TRUE)


##add metadata
bacteria_alpha_compiled <- merge(bacteria_alpha_compiled, sample_data, by = c("sample"))

bacteria_alpha_compiled_long <- merge(bacteria_alpha_compiled_long, sample_data, by = c("sample"))
bacteria_alpha_compiled_long$Treatment[bacteria_alpha_compiled_long$Treatment == 'Prebiotic'] <- 'Biostimulant'

##subset metrics for GLMMs
bacteria_richness <- subset(bacteria_alpha_compiled_long, metric %in% c('Observed'))

bacteria_evenness <- subset(bacteria_alpha_compiled_long, metric %in% c('Pielou'))

bacteria_simpson <- subset(bacteria_alpha_compiled_long, metric %in% c('Simpson'))

bacteria_shannon <- subset(bacteria_alpha_compiled_long, metric %in% c('Shannon'))

bacteria_chao <- subset(bacteria_alpha_compiled_long, metric %in% c('Chao1'))

##subset metrics for visualization
bacteria_alpha_plot <- subset(bacteria_alpha_compiled_long, metric %in% c('Shannon', 'Simpson', 'Chao1', 'Pielou'))
bacteria_alpha_plot$Developmental_Stage <- factor(bacteria_alpha_plot$Developmental_Stage, levels = c("V1", "V6", "R3", "R6"))


##visualize alpha diversity- boxplot 
devtools::install_github("ropenscilabs/ochRe")
library(ochRe)

ggplot(bacteria_alpha_plot, aes(x = Developmental_Stage, y = value, fill = interaction(Treatment, Cultivar))) + 
  geom_col(aes(y = 0), alpha = 0, width = 1) +  # Temporary invisible layer
  geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="V6") -0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="V6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
 geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="R6")-0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="R6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
  geom_boxplot(alpha = 0.5) +
  geom_dotplot(binaxis='y', alpha = 0.8, stackdir='center', position=position_dodge(0.75)) +
  facet_wrap(~metric, scales = "free", nrow = 1) +
  theme_bw() +
  scale_fill_manual(values=c("#749e89","#c399a2","#7d87b2", "white")) +
  theme() +
  theme(axis.title = element_blank(), axis.text.y = element_text(size = 12, color = "black"), axis.ticks = element_blank(), 
        strip.text = element_text(size = 14, face = "bold"), axis.text.x = element_text(size = 12, color = "black", face = "bold"), 
        legend.title = element_text(size = 14, face = "bold"), legend.text = element_text(size = 12), legend.position = 'top') +
  labs(fill = "Treatment + Cultivar") 

ggsave("Preliminary_alpha_3.tiff", width = 14, height = 2.5, units = "in", dpi = 900)
```

```{r}
##18S-ITS
##Calculate alpha diversity metrics with bacterial_phyloseq_decontam
fun_alpha <- estimate_richness(fun_phyloseq_decontam, measures = c("Observed", "Chao1", "ACE", "Shannon", "Simpson", "InvSimpson"))

##make rownames a column
fun_alpha <- tibble::rownames_to_column(fun_alpha, "sample")

##Calculate evenness- https://rdrr.io/github/microbiome/microbiome/man/evenness.html
##library(BiocManager)
##BiocManager::install("microbiome")
##library(microbiome)

fun_evenness <- evenness(fun_phyloseq_decontam, 'pielou')

##make rownames a column
fun_evenness <- tibble::rownames_to_column(fun_evenness, "sample")
colnames(fun_evenness)[2] ="Pielou"

##merge all alpha indices
fun_alpha_compiled <- merge(fun_alpha, fun_evenness, by = c("sample"))

##remove negative controls
fun_alpha_compiled <- fun_alpha_compiled[-c(62), ]

##make long-form in order to plot
fun_alpha_compiled$sample <- factor(fun_alpha_compiled$sample)

fun_alpha_compiled_long <- gather(fun_alpha_compiled, metric, value, Observed:Pielou, factor_key=TRUE)


##add metadata
fun_alpha_compiled <- merge(fun_alpha_compiled, fun.sample.data, by = c("sample"))

fun_alpha_compiled_long <- merge(fun_alpha_compiled_long, sample_data, by = c("sample"))
fun_alpha_compiled_long$Treatment[fun_alpha_compiled_long$Treatment == 'Prebiotic'] <- 'Biostimulant'


##subset metrics for GLMMs
fun_richness <- subset(fun_alpha_compiled_long, metric %in% c('Observed'))

fun_evenness <- subset(fun_alpha_compiled_long, metric %in% c('Pielou'))

fun_simpson <- subset(fun_alpha_compiled_long, metric %in% c('Simpson'))

fun_shannon <- subset(fun_alpha_compiled_long, metric %in% c('Shannon'))

fun_chao <- subset(fun_alpha_compiled_long, metric %in% c('Chao1'))

##subset metrics for visualization
fun_alpha_plot <- subset(fun_alpha_compiled_long, metric %in% c('Chao1','Shannon', 'Simpson', 'Pielou'))
fun_alpha_plot$Developmental_Stage <- factor(fun_alpha_plot$Developmental_Stage, levels = c("V1", "V6", "R3", "R6"))

##Visualization
ggplot(fun_alpha_plot, aes(x = Developmental_Stage, y = value, fill = interaction(Treatment, Cultivar))) + 
  geom_col(aes(y = 0), alpha = 0, width = 1) +  # Temporary invisible layer
  geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="V6") -0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="V6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
 geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="R6")-0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="R6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
  geom_boxplot(alpha = 0.5) +
  geom_dotplot(binaxis='y', alpha = 0.8, stackdir='center', position=position_dodge(0.75)) +
  facet_wrap(~metric, scales = "free", nrow = 1) +
  theme_bw() +
  scale_fill_manual(values=c("#749e89","#c399a2","#7d87b2", "white")) +
  theme() +
  theme(axis.title = element_blank(), axis.text.y = element_text(size = 12, color = "black"), axis.ticks = element_blank(), 
        strip.text = element_text(size = 14, face = "bold"), axis.text.x = element_text(size = 12, color = "black", face = "bold"), 
        legend.title = element_text(size = 14, face = "bold"), legend.text = element_text(size = 12), legend.position = 'top') +
  labs(fill = "Treatment + Cultivar") 

ggsave("fun_alpha_.tiff", width = 14, height = 2.5, units = "in", dpi = 900)
```

```{r}
##Visualize prokaryotic and eukaryotic alpha diversity together
bacteria_alpha_plot$taxtype <- 'Prokaryote'
fun_alpha_plot$taxtype <- 'Eukaryote'

alpha_combined <- rbind(bacteria_alpha_plot, fun_alpha_plot)

##Change R3 to R2, the correct sampling stage
alpha_combined$Developmental_Stage <- as.character(alpha_combined$Developmental_Stage)
alpha_combined$Developmental_Stage[alpha_combined$Developmental_Stage == "R3"] <- "R2"
alpha_combined$Developmental_Stage <- as.factor(alpha_combined$Developmental_Stage)

##Order developmental stage
alpha_combined$Developmental_Stage <- factor(alpha_combined$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))

ggplot(alpha_combined, aes(x = Developmental_Stage, y = value, fill = interaction(Treatment, Cultivar))) + 
  geom_col(aes(y = 0), alpha = 0, width = 1) +  # Temporary invisible layer
  geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="V6") -0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="V6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
 geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="R6")-0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="R6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
  #geom_boxplot(alpha = 0.5) +
  geom_boxplot() +
  #geom_dotplot(binaxis='y', stackdir='center', position=position_dodge(0.75)) +
  geom_dotplot(binaxis='y', stackdir='center', position=position_dodge(0.75)) +
  ggh4x::facet_grid2(taxtype~metric, scales = "free_y", independent = "y") +
 # facet_grid(taxtype~metric, scales = "free_y") +
  theme_bw() +
  scale_fill_manual(values=c("lightblue3", "antiquewhite", "#8c3800", "#c0c0c0")) +
  theme() +
  theme(axis.title = element_blank(), axis.text.y = element_text(size = 12, color = "black"), axis.ticks = element_blank(), 
        strip.text = element_text(size = 14, face = "bold"), axis.text.x = element_text(size = 12, color = "black", face = "bold"), 
        legend.title = element_text(size = 14, face = "bold"), legend.text = element_text(size = 12), legend.position = 'top') +
  labs(fill = "Treatment + Cultivar")

ggsave("Preliminary_alpha_combined.3.tiff", width = 14, height = 4, units = "in", dpi = 900)


##Get summary stats for manuscript
summary_table <- alpha_combined %>%
  group_by(metric, taxtype) %>%
  summarise(
    Mean = mean(value, na.rm = TRUE),
    SEM = sd(value, na.rm = TRUE) / sqrt(n())
  )

view(summary_table)
```

```{r}
##Visualize correlations
##eukaryotic shannon vs prokaryotic shannon
alpha.combined.shannon <- alpha_combined %>%
  filter(metric == "Shannon" & taxtype %in% c("Eukaryote", "Prokaryote"))

alpha.combined.shannon.2 <- alpha.combined.shannon %>%
  spread(key = taxtype, value = value)

##Get correlation
cor_result <- cor.test(alpha.combined.shannon.2$Prokaryote, alpha.combined.shannon.2$Eukaryote, method = "spearman")
cor_coeff <- cor_result$estimate
p_value <- cor_result$p.value

# Plot
ggplot(alpha.combined.shannon.2, aes(x = Prokaryote, y = Eukaryote)) +
   geom_point(shape = 21, fill = "#8c3800", color = "black", alpha = 0.8, size = 6)  + 
  theme_minimal() +
  labs(x = "Prokaryotic Shannon",
       y = "Eukaryotic Shannon") +
  theme(legend.position = 'none') +
  theme_bw() +
  theme(axis.text = element_text(size = 17, color = "black"), # 12 + 5
        axis.title = element_text(size = 17, face = "bold")) + # 12 + 5 and bolded
  annotate("text", x = -Inf, y = Inf, 
           label = sprintf("R = %.2f, p = %.2e", cor_coeff, p_value), 
           hjust = -1, vjust = 18, size = 6)

ggsave("shannon.correlation.tiff", width = 6, height = 4, units = "in", dpi = 900)


##eukaryotic chao vs prokaryotic chao
alpha.combined.chao <- alpha_combined %>%
  filter(metric == "Chao1" & taxtype %in% c("Eukaryote", "Prokaryote"))

alpha.combined.chao.2 <- alpha.combined.chao %>%
  spread(key = taxtype, value = value)

##Get correlation
cor_result.chao <- cor.test(alpha.combined.chao.2$Prokaryote, alpha.combined.chao.2$Eukaryote, method = "spearman")
cor_coeff.chao <- cor_result.chao$estimate
p_value.chao <- cor_result.chao$p.value

# Plot
ggplot(alpha.combined.chao.2, aes(x = Prokaryote, y = Eukaryote)) +
  geom_point(shape = 21, fill = "lightblue3", color = "black", alpha = 0.8, size = 6) + 
  theme_minimal() +
  labs(x = "Prokaryotic Chao1",
       y = "Eukaryotic Chao1") +
  theme(legend.position = 'none') +
  theme_bw() +
  theme(axis.text = element_text(size = 17, color = "black"), # 12 + 5
        axis.title = element_text(size = 17, face = "bold")) + # 12 + 5 and bolded
  annotate("text", x = -Inf, y = Inf, 
           label = sprintf("R = %.2f, p = %.2e", cor_coeff.chao, p_value.chao), 
           hjust = -1, vjust = 18, size = 6)

ggsave("chao.correlation.tiff", width = 6, height = 4, units = "in", dpi = 900)

##Shannon vs Chao
alpha_combined_selected <- alpha_combined %>%
  filter(metric %in% c("Shannon", "Chao1")) %>%
  spread(key = metric, value = value)

cor_result.combined <- cor.test(alpha_combined_selected$Chao1, 
                        alpha_combined_selected$Shannon, 
                        method = "spearman")

cor_coeff.combined <- cor_result.combined$estimate
p_value.combined <- cor_result.combined$p.value

custom_colors <- c(Prokaryote = "#a8c0a8", Eukaryote = "#a890a8")

ggplot(alpha_combined_selected, aes(x = Chao1, y = Shannon, fill = taxtype)) +
  geom_point(shape = 21, color = "black", size = 6, alpha = 0.8) + 
  theme_minimal() +
  labs(x = "Chao1", y = "Shannon") +
  theme_bw() +
  theme(axis.text = element_text(size = 17, color = "black"), 
        axis.title = element_text(size = 17, face = "bold"), 
        legend.text = element_text(size = 15), legend.position = 'none') + 
  scale_fill_manual(values = custom_colors) +
  annotate("text", x = -Inf, y = Inf, 
           label = sprintf("R = %.2f, p = %s", cor_coeff.combined, formatC(p_value.combined, format = "e", digits = 2)), 
           hjust = -1, vjust = 18, size = 6)

ggsave("chao.shannon.correlation.tiff", width = 6, height = 4, units = "in", dpi = 900)
```

```{r}
update.packages(ask = FALSE)
library("dplyr")
library("ggpubr")
library("car")
library("MASS")
library("lme4")
library("sjPlot")
library("sjlabelled")
library("sjmisc")
library("tibble")
library("performance")
library("AER")
library("patchwork")
##library("glmmTMB")

##16S
##Starting with Chao1. First, we want to split the dataset to assess the baseline measurement independently. This ensures that plots had similar
##alpha diversity prior to prebiotic application. Additionally, the baseline needs to be removed prior to modeling in order to see the true effect of 
#biostimulant/prebiotic application on alpha diversity. 

##Split data
bacteria_alpha_baseline <- bacteria_alpha_compiled[bacteria_alpha_compiled$Developmental_Stage %in% c('V1'), ]
  
bacteria_alpha_after_app <- bacteria_alpha_compiled[bacteria_alpha_compiled$Developmental_Stage %in% c('V6', 'R3', 'R6'), ]

##Convert "Location" to a factor variable
bacteria_alpha_after_app$Location <- factor(bacteria_alpha_after_app$Location)

##Full code for mixed model

###Checking the normality of Chao1 using Shapiro-Wilk test###
shapiro_result.chao <- shapiro.test(bacteria_alpha_after_app$Chao1) ##W = 0.67646, p-value = 5.217e-09

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.chao$p.value < 0.05) {
  #chao <- bacteria_alpha_after_app$Chao1 + 1  # Create new dataframe with Chao1 + 1
  
  distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(bacteria_alpha_after_app$Chao1, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- nbinom
}
?fitdist
###Double check with qqp###
chao <- bacteria_alpha_after_app$Chao1 + 1
nbinom.chao <- fitdistr(chao, "Negative Binomial")
qqp(chao, "nbinom", size = nbinom.chao$estimate[[1]], mu = nbinom.chao$estimate[[2]])

gamma.chao <- fitdistr(chao, "gamma")
qqp(chao, "gamma", shape = gamma.chao$estimate[[1]], rate = gamma.chao$estimate[[2]])

qqp(chao, "lnorm")
qqp(chao, "norm")

##Will use nbinom

###Constructing the GLMM with stepwise selection###
# Fit the full model with negative binomial distribution
full_model.chao <- glmmTMB(Chao1 ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location), data = bacteria_alpha_after_app, family = nbinom2, na.action = "na.fail")

##Perform model selection using dredge
#models.chao <- dredge(full_model.chao, m.min = 1)
models.chao <- dredge(full_model.chao, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.chao)

##Select the best model based on AIC
best_model.chao <- get.models(models.chao, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.chao)


###Check for overdispersion###
##library(performance)
check_overdispersion(best_model.chao) ##dispersion ratio =  1.148, Pearson's Chi-Squared = 47.060, p-value =  0.238

###Marginalize the coefficients due to the use of a nonlinear link function###
#library(margins)
##type = "response" (default)- “partial effects” (i.e., the contribution of each variable on the 
##outcome scale, conditional on the other variables involved in the link function transformation of the linear predictor)

##type = "link" - true “marginal effects” (i.e., the marginal contribution of each variable on the scale of the linear predictor)
##Because we are working with a glmmtmb object, we must call 'marginal_effects(x)' directly, and summarize manually
##https://www.rdocumentation.org/packages/margins/versions/0.3.26

chao.margins <- as.data.frame(marginal_effects(best_model.chao, type = "response"))

##Print the marginalized coefficients
summary(marg_effects.chao)
marge.chao <- as.data.frame(summary(marg_effects.chao))

##Print model output
tab_model(best_model.chao)
summary(best_model.chao)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.chao.1.trt <- powerSim(best_model.chao, fixed("Treatment"))
power.sim.chao.1.cult <- powerSim(best_model.chao, fixed("Cultivar"))
power.sim.chao.1.ds <- powerSim(best_model.chao, fixed("Developmental_Stage"))

power.sim.chao.1.trt 
power.sim.chao.1.cult 
power.sim.chao.1.ds 


###Marginalize the coefficients due to the use of a nonlinear link function###
chao.margins <- as.data.frame(marginal_effects(best_model.chao, type = "response"))
mean_est.chao<- c(mean(chao.margins$dydx_CultivarCZ4979X), mean(chao.margins$dydx_Developmental_StageR6), mean(chao.margins$dydx_Developmental_StageV6),
                  mean(chao.margins$dydx_TreatmentControl))
print(mean_est.chao)

chao.margins.2 <- chao.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
chao.margins.2$mean_est <- c(-32.23585,  50.28362,  17.05213, -31.72217)

chao.margins.2$taxtype <- 'Prokaryote'
chao.margins.2$metric <- 'Chao1'

##Decompose R2 to get contribution of each fixed effect
library(glmm.hp)

chao.r2 <- glmm.hp(best_model.chao, type = "adjR2", commonality = FALSE)

print(chao.r2)
plot.glmmhp(chao.r2)

##Add delta r2
chao.margins.2$r2 <- c(29.00,  45.14,  45.14, 25.86) ##Add twice for growth stage


##Asess model performance
##https://www.flutterbys.com.au/stats/tut/tut10.5a.html
##Residuals and empirical cumulative density function
# Calculate standardized residuals
standardized.resid.chao <- resid(best_model.chao, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.chao <- sum(resid(best_model.chao, type = "pearson")^2)
1 - pchisq(dat.resid.chao, df.residual(best_model.chao))
##does not indicate lack of fit (> 0.05)

# Assessing Deviance (G2)
deviance_value.chao <- 1 - pchisq(as.numeric(-2 * logLik(best_model.chao)), df.residual(best_model.chao))

# Simulating datasets
dat.sim.chao <- simulate(best_model.chao, nsim = 250)

# Empirical cumulative density function calculations
resid.list.chao <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1))

for (i in 1:length(dat.sim.chao)) {
    e.chao <- ecdf(dat.sim.chao[[i]] + runif(length(dat.sim.chao[[i]]), -0.5, 0.5))
    resid.list.chao[[i]] <- e.chao(resid(best_model.chao) + runif(length(resid(best_model.chao)), -0.5, 0.5))
    plot(e.chao, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.chao <- do.call(c, resid.list.chao)

# Quantile-quantile plot
qqnorm(all.resid.chao, main = "QQ Plot")
qqline(all.resid.chao, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.chao), standardized.resid.chao, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##18S-ITS
##Starting with Chao1. First, we want to split the dataset to assess the baseline measurement independently. This ensures that plots had similar
##alpha diversity prior to prebiotic application. Additionally, the baseline needs to be removed prior to modeling in order to see the true effect of 
#prebiotic application on alpha diversity. 

##Split data
fun_alpha_baseline <- fun_alpha_compiled[fun_alpha_compiled$Developmental_Stage %in% c('V1'), ]
  
fun_alpha_after_app <- fun_alpha_compiled[fun_alpha_compiled$Developmental_Stage %in% c('V6', 'R3', 'R6'), ]

###Checking the normality of Chao1 using Shapiro-Wilk test###
shapiro_result.chao.fun <- shapiro.test(fun_alpha_after_app$Chao1) ##W = 0.95713, p-value = 0.07732

###Double check with qqp###
chao.fun <- fun_alpha_after_app$Chao1 + 1

nbinom.chao.fun <- fitdistr(chao.fun, "Negative Binomial")
qqp(chao.fun, "nbinom", size = nbinom.chao.fun$estimate[[1]], mu = nbinom.chao.fun$estimate[[2]])

gamma.chao.fun <- fitdistr(chao.fun, "gamma")
qqp(chao.fun, "gamma", shape = gamma.chao.fun$estimate[[1]], rate = gamma.chao.fun$estimate[[2]])

qqp(chao.fun, "lnorm")
qqp(chao.fun, "norm")

##Will use norm

###Constructing the GLMM with stepwise selection###
# Fit the full model with negative binomial distribution
full_model.chao.fun <- glmmTMB(Chao1 ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                               data = fun_alpha_after_app,
                               family = gaussian(link = "identity"),
                               na.action = "na.fail")

##Perform model selection using dredge
#models.chao.fun <- dredge(full_model.chao.fun, m.min = 1)
models.chao.fun <- dredge(full_model.chao.fun, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.chao.fun)

##Select the best model based on AIC
best_model.chao.fun <- get.models(models.chao.fun, subset = delta < 2)[[1]] ##Convergence issue; must double-check

##Print the best model
summary(best_model.chao.fun)

###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.chao.fun) 

##Extract estimates directly from model summary, as a linear link function was used;
##quickest way is to extract manually
# Create the data frame
chao.margins.fun <- data.frame(
  Variable = c('CZ4979X vs CZ4810X', 'R6 vs R3', 'V6 vs R3', 
               'Control vs Biostimulant', "CZ4979X:R6", 
               "CZ4979X:V6"),
  mean_marg_est = c(205.38, 220.62, 30.88, 45.79, -533.75, -258.00) ##not marginalized, but will use this naming to facilitate rbind
)

chao.margins.fun$taxtype <- 'Eukaryote'
chao.margins.fun$metric <- 'Chao1'

##Print model output
tab_model(best_model.chao.fun)

##powerSim: estimate power by simulation
power.sim.chao.1.trt.fun <- powerSim(best_model.chao.fun, fixed("Treatment"))
power.sim.chao.1.cult.fun <- powerSim(best_model.chao.fun, fixed("Cultivar"))
power.sim.chao.1.ds.fun <- powerSim(best_model.chao.fun, fixed("Developmental_Stage"))

power.sim.chao.1.trt.fun 
power.sim.chao.1.cult.fun 
power.sim.chao.1.ds.fun 

##Decompose R2 to get contribution of each fixed effect
##Need to create a second model specifying interactions as fixed effects as recommended by authors
full_model.chao.fun.2 <- glmmTMB(Chao1 ~ Treatment + Developmental_Stage + Cultivar + interaction + (1 | Location),
                               data = fun_alpha_after_app,
                               family = gaussian(link = "identity"),
                               na.action = "na.fail")

##Second power analysis for interaction
power.sim.chao.1.int.fun <- powerSim(best_model.chao.fun.2, fixed("interaction"))

chao.r2.fun <- glmm.hp(full_model.chao.fun.2, type = "adjR2", commonality = FALSE)

print(chao.r2.fun)
plot.glmmhp(chao.r2.fun)

plot_model(best_model.chao.fun)
summary(best_model.chao.fun)

##Add r2
chao.margins.fun$r2 <- c(2.89, 5.42, 5.42, 3.55, 88.15, 88.15)

##Format to facilitate rbind
names(chao.margins.fun)[names(chao.margins.fun) == "mean_marg_est"] <- "mean_est"
chao.margins.fun$marg_est <- NA
chao.margins.fun$Predictor <- NA
chao.margins.fun <- chao.margins.fun[, names(shannon.margins.2.fun)]

##Asess model performance
standardized.resid.chao.fun <- resid(best_model.chao.fun, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.chao.fun <- sum(resid(best_model.chao.fun, type = "pearson")^2)
1 - pchisq(dat.resid.chao.fun, df.residual(best_model.chao.fun))


# Assessing Deviance (G2)
deviance_value.chao.fun <- 1 - pchisq(as.numeric(-2 * logLik(best_model.chao.fun)), df.residual(best_model.chao.fun))

# Simulating datasets
dat.sim.chao.fun <- simulate(best_model.chao.fun, nsim = 250)

# Empirical cumulative density function calculations
resid.list.chao.fun <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.chao.fun)) {
    e.chao.fun <- ecdf(dat.sim.chao.fun[[i]] + runif(length(dat.sim.chao.fun[[i]]), -0.5, 0.5))
    resid.list.chao.fun[[i]] <- e.chao.fun(resid(best_model.chao.fun) + runif(length(resid(best_model.chao.fun)), -0.5, 0.5))
    plot(e.chao.fun, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.chao.fun <- do.call(c, resid.list.chao.fun)

# Quantile-quantile plot
qqnorm(all.resid.chao.fun, main = "QQ Plot")
qqline(all.resid.chao.fun, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.chao.fun), standardized.resid.chao.fun, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##16S
##Chao1 for baseline
shapiro_result.base.chao <- shapiro.test(bacteria_alpha_baseline$Chao1) #W = 0.95196, p-value = 0.5214

###Double check with qqp###
base.chao <- bacteria_alpha_baseline$Chao1 + 1

nbinom.base.chao <- fitdistr(base.chao, "Negative Binomial")
qqp(base.chao, "nbinom", size = nbinom.base.chao$estimate[[1]], mu = nbinom.base.chao$estimate[[2]])

gamma.base.chao <- fitdistr(base.chao, "gamma")
qqp(base.chao, "gamma", shape = gamma.base.chao$estimate[[1]], rate = gamma.base.chao$estimate[[2]])

qqp(base.chao, "lnorm")
qqp(base.chao, "norm")

###Will use a normal distribution
##Full model
full_model.base.chao <- glmmTMB(Chao1 ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = bacteria_alpha_baseline,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.chao <- dredge(full_model.base.chao, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.chao)

##Select the best model based on AIC
best_model.base.chao <- get.models(models.base.chao, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.chao)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
##note convergence issue with and without interaction; results consistent
```

```{r}
##18S-ITS
##Chao1 for baseline
shapiro_result.base.chao.fun <- shapiro.test(fun_alpha_baseline$Chao1) #W = 0.90894, p-value = 0.1119

###Double check with qqp###
base.chao.fun <- fun_alpha_baseline$Chao1 + 1

nbinom.base.chao.fun <- fitdistr(base.chao.fun, "Negative Binomial")
qqp(base.chao.fun, "nbinom", size = nbinom.base.chao.fun$estimate[[1]], mu = nbinom.base.chao.fun$estimate[[2]])

gamma.base.chao.fun <- fitdistr(base.chao.fun, "gamma")
qqp(base.chao.fun, "gamma", shape = gamma.base.chao.fun$estimate[[1]], rate = gamma.base.chao.fun$estimate[[2]])

qqp(base.chao.fun, "lnorm")
qqp(base.chao.fun, "norm")

###Will use a normal distribution
##Full model
full_model.base.chao.fun <- glmmTMB(Chao1 ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = fun_alpha_baseline,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.chao.fun <- dredge(full_model.base.chao.fun, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.chao.fun)

##Select the best model based on AIC
best_model.base.chao.fun <- get.models(models.base.chao.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.chao.fun)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##16S
##shannon
##Full code for mixed model

###Checking the normality of Shannon using Shapiro-Wilk test###
shapiro_result.shannon <- shapiro.test(bacteria_alpha_after_app$Shannon) #W = 0.91334, p-value = 0.001746

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.shannon$p.value < 0.05) {
distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(bacteria_alpha_after_app$Shannon, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- gamma norm
}  
  
###Double check with qqp###

shannon <- bacteria_alpha_after_app$Shannon + 1  # Create new dataframe with Shannon + 1

#nbinom.shannon <- fitdistr(shannon, "Negative Binomial")
#qqp(shannon, "nbinom", size = nbinom.shannon$estimate[[1]], mu = nbinom.shannon$estimate[[2]])

gamma.shannon <- fitdistr(shannon, "gamma")
qqp(shannon, "gamma", shape = gamma.shannon$estimate[[1]], rate = gamma.shannon$estimate[[2]])

qqp(shannon, "lnorm")
qqp(shannon, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with gamma distribution
full_model.shannon <- glmmTMB(Shannon ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location), data = bacteria_alpha_after_app,
                              family = Gamma(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.shannon <- dredge(full_model.shannon, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.shannon)

##Select the best model based on AIC
best_model.shannon <- get.models(models.shannon, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.shannon)

###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.shannon) 

###Marginalize the coefficients due to the use of a nonlinear link function###
shannon.margins <- as.data.frame(marginal_effects(best_model.shannon, type = "response"))

mean_est.shannon<- c(mean(shannon.margins$dydx_CultivarCZ4979X), mean(shannon.margins$dydx_Developmental_StageR6), mean(shannon.margins$dydx_Developmental_StageV6), mean(shannon.margins$dydx_TreatmentControl))

print(mean_est.shannon)

shannon.margins.2 <- shannon.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
shannon.margins.2$mean_est <- c(-0.308713627,  0.328807384, -0.003067241, -0.199492611)

shannon.margins.2$taxtype <- 'Prokaryote'
shannon.margins.2$metric <- 'Shannon'

##Print model output
tab_model(best_model.shannon)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.shannon.1.cult <- powerSim(best_model.shannon, fixed("Cultivar"))
power.sim.shannon.1.ds <- powerSim(best_model.shannon, fixed("Developmental_Stage"))
power.sim.shannon.1.trt <- powerSim(best_model.shannon, fixed("Treatment"))

power.sim.shannon.1.cult 
power.sim.shannon.1.ds
power.sim.shannon.1.trt

##Decompose R2 to get contribution of each fixed effect
shannon.r2 <- glmm.hp(best_model.shannon, type = "adjR2", commonality = FALSE)

print(shannon.r2)
plot.glmmhp(shannon.r2)

##Add delta r2
shannon.margins.2$r2 <- c(41.3,  41.0,  41.0, 17.7) ##Add twice for growth stage

##Asess model performance
standardized.resid.shannon <- resid(best_model.shannon, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.shannon <- sum(resid(best_model.shannon, type = "pearson")^2)
1 - pchisq(dat.resid.shannon, df.residual(best_model.shannon))


# Assessing Deviance (G2)
deviance_value.shannon <- 1 - pchisq(as.numeric(-2 * logLik(best_model.shannon)), df.residual(best_model.shannon))

# Simulating datasets
dat.sim.shannon <- simulate(best_model.shannon, nsim = 250)

# Empirical cumulative density function calculations
resid.list.shannon <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.shannon)) {
    e.shannon <- ecdf(dat.sim.shannon[[i]] + runif(length(dat.sim.shannon[[i]]), -0.5, 0.5))
    resid.list.shannon[[i]] <- e.shannon(resid(best_model.shannon) + runif(length(resid(best_model.shannon)), -0.5, 0.5))
    plot(e.shannon, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.shannon <- do.call(c, resid.list.shannon)

# Quantile-quantile plot
qqnorm(all.resid.shannon, main = "QQ Plot")
qqline(all.resid.shannon, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.shannon), standardized.resid.shannon, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##18S-ITS
##shannon
###Checking the normality of Shannon using Shapiro-Wilk test###
shapiro_result.shannon.fun <- shapiro.test(fun_alpha_after_app$Shannon) #W = 0.6682, p-value = 3.799e-09

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.shannon.fun$p.value < 0.05) {
distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(fun_alpha_after_app$Shannon, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- gamma
}  
  
###Double check with qqp###
shannon.fun <- fun_alpha_after_app$Shannon + 1  # Create new dataframe with Shannon + 1

gamma.shannon.fun <- fitdistr(shannon.fun, "gamma")
qqp(shannon.fun, "gamma", shape = gamma.shannon.fun$estimate[[1]], rate = gamma.shannon.fun$estimate[[2]])

qqp(shannon.fun, "lnorm")
qqp(shannon.fun, "norm")


###Constructing the GLMM with stepwise selection###
# Fit the full model with gamma distribution
full_model.shannon.fun <- glmmTMB(Shannon ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location), data = fun_alpha_after_app,
                              family = Gamma(link = "log"), na.action = "na.fail")


##Perform model selection using dredge
#models.shannon.fun <- dredge(full_model.shannon.fun, m.min = 1)
models.shannon.fun <- dredge(full_model.shannon.fun, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.shannon.fun)

##Select the best model based on AIC
best_model.shannon.fun <- get.models(models.shannon.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.shannon.fun)

###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.shannon.fun) 

###Marginalize the coefficients due to the use of a nonlinear link function###
shannon.margins.fun <- as.data.frame(marginal_effects(best_model.shannon.fun, type = "response"))
mean_est.shannon.fun <- c(mean(shannon.margins.fun$dydx_CultivarCZ4979X), mean(shannon.margins.fun$dydx_Developmental_StageR6), mean(shannon.margins.fun$dydx_Developmental_StageV6), mean(shannon.margins.fun$dydx_TreatmentControl))

print(mean_est.shannon.fun)

shannon.margins.2.fun <- shannon.margins.fun %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
shannon.margins.2.fun$mean_est <- c(-0.12911399, -0.30507393, -0.15379925,  0.07546353)

shannon.margins.2.fun$taxtype <- 'Eukaryote'
shannon.margins.2.fun$metric <- 'Shannon'

##Print model output
tab_model(best_model.shannon.fun)

##powerSim: estimate power by simulation
power.sim.shannon.trt.fun <- powerSim(best_model.shannon.fun, fixed("Treatment"))
power.sim.shannon.cult.fun <- powerSim(best_model.shannon.fun, fixed("Cultivar"))
power.sim.shannon.ds.fun <- powerSim(best_model.shannon.fun, fixed("Developmental_Stage"))

power.sim.shannon.trt.fun 
power.sim.shannon.cult.fun 
power.sim.shannon.ds.fun 

##Decompose R2 to get contribution of each fixed effect
shannon.r2.fun <- glmm.hp(best_model.shannon.fun, type = "adjR2", commonality = FALSE)

print(shannon.r2.fun)
plot.glmmhp(shannon.r2.fun)

##Add delta r2
shannon.margins.2.fun$r2 <- c(19.72,  73.47,  73.47, 6.81) ##Add twice for growth stage

##Asess model performance
standardized.resid.shannon.fun <- resid(best_model.shannon.fun, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.shannon.fun <- sum(resid(best_model.shannon.fun, type = "pearson")^2)
1 - pchisq(dat.resid.shannon.fun, df.residual(best_model.shannon.fun))


# Assessing Deviance (G2)
deviance_value.shannon.fun <- 1 - pchisq(as.numeric(-2 * logLik(best_model.shannon.fun)), df.residual(best_model.shannon.fun))

# Simulating datasets
dat.sim.shannon.fun <- simulate(best_model.shannon.fun, nsim = 250)

# Empirical cumulative density function calculations
resid.list.shannon.fun <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.shannon.fun)) {
    e.shannon.fun <- ecdf(dat.sim.shannon.fun[[i]] + runif(length(dat.sim.shannon.fun[[i]]), -0.5, 0.5))
    resid.list.shannon.fun[[i]] <- e.shannon.fun(resid(best_model.shannon.fun) + runif(length(resid(best_model.shannon.fun)), -0.5, 0.5))
    plot(e.shannon.fun, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.shannon.fun <- do.call(c, resid.list.shannon.fun)

# Quantile-quantile plot
qqnorm(all.resid.shannon.fun, main = "QQ Plot")
qqline(all.resid.shannon.fun, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.shannon.fun), standardized.resid.shannon.fun, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##16S
##shannon for baseline
shapiro_result.base.shannon <- shapiro.test(bacteria_alpha_baseline$Shannon) ##W = 0.97766, p-value = 0.9423

###Double check with qqp###
base.shannon <- bacteria_alpha_baseline$Shannon + 1

#nbinom.base.shannon <- fitdistr(base.shannon, "Negative Binomial")
#qqp(base.shannon, "nbinom", size = nbinom.base.shannon$estimate[[1]], mu = nbinom.base.shannon$estimate[[2]])

gamma.base.shannon <- fitdistr(base.shannon, "gamma")
qqp(base.shannon, "gamma", shape = gamma.base.shannon$estimate[[1]], rate = gamma.base.shannon$estimate[[2]])

qqp(base.shannon, "lnorm")
qqp(base.shannon, "norm")

###Will use a normal distribution
##Full model
full_model.base.shannon <- glmmTMB(Shannon ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = bacteria_alpha_baseline,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.shannon <- dredge(full_model.base.shannon, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.shannon)

##Select the best model based on AIC
best_model.base.shannon <- get.models(models.base.shannon, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.shannon) ##Treatment statistically significant
##Can get ME directly from model due to distribution

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##18S-ITS
##shannon for baseline
shapiro_result.base.shannon.fun <- shapiro.test(fun_alpha_baseline$Shannon) #W = 0.96618, p-value = 0.7736

###Double check with qqp###
base.shannon.fun <- fun_alpha_baseline$Shannon + 1

#nbinom.base.shannon.fun <- fitdistr(base.shannon.fun, "Negative Binomial")
#qqp(base.shannon.fun, "nbinom", size = nbinom.base.shannon.fun$estimate[[1]], mu = nbinom.base.shannon.fun$estimate[[2]])

gamma.base.shannon.fun <- fitdistr(base.shannon.fun, "gamma")
qqp(base.shannon.fun, "gamma", shape = gamma.base.shannon.fun$estimate[[1]], rate = gamma.base.shannon.fun$estimate[[2]])

qqp(base.shannon.fun, "lnorm")
qqp(base.shannon.fun, "norm")

###Will use a normal distribution
##Full model
full_model.base.shannon.fun <- glmmTMB(Shannon ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = fun_alpha_baseline,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.shannon.fun <- dredge(full_model.base.shannon.fun, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.shannon.fun)

##Select the best model based on AIC
best_model.base.shannon.fun <- get.models(models.base.shannon.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.shannon.fun)
##Can get ME directly from model due to distribution

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##16S
##simpson
###we will use a beta distribution, since we are working with percentages represented as values bound between 0 and 1

# Fit the full model with beta distribution
full_model.simpson <- glmmTMB(Simpson ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                              family = beta_family(link = "logit"), data = bacteria_alpha_after_app, na.action = "na.fail")

##Perform model selection using dredge
models.simpson <- dredge(full_model.simpson, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.simpson)

##Select the best model based on AIC
best_model.simpson <- get.models(models.simpson, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.simpson)


###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.simpson)

##Print model output
tab_model(best_model.simpson)

###Marginalize the coefficients due to the use of a nonlinear link function###
simpson.margins <- as.data.frame(marginal_effects(best_model.simpson, type = "response"))
mean_est.simpson<- c(mean(simpson.margins$dydx_CultivarCZ4979X), mean(simpson.margins$dydx_Developmental_StageR6), mean(simpson.margins$dydx_Developmental_StageV6), mean(simpson.margins$dydx_TreatmentControl))

print(mean_est.simpson)

simpson.margins.2 <- simpson.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
simpson.margins.2$mean_est <- c(-0.016698317,  0.009327110, -0.002952705, -0.002632083)

simpson.margins.2$taxtype <- 'Prokaryote'
simpson.margins.2$metric <- 'Simpson'

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.simpson.1.cult <- powerSim(best_model.simpson, fixed("Cultivar"))
power.sim.simpson.1.ds <- powerSim(best_model.simpson, fixed("Developmental_Stage"))
power.sim.simpson.1.trt <- powerSim(best_model.simpson, fixed("Treatment"))

power.sim.simpson.1.cult 
power.sim.simpson.1.ds
power.sim.simpson.1.trt

##Decompose R2 to get contribution of each fixed effect
simpson.r2 <- glmm.hp(best_model.simpson, type = "adjR2", commonality = FALSE) ##come back to this

print(simpson.r2)
plot.glmmhp(simpson.r2)

##Add r2
simpson.margins.2$r2 <- c(57.78,  29.34,  29.34, 12.88) ##Add twice for growth stage

##Assess model performance
standardized.resid.simpson <- resid(best_model.simpson, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.simpson <- sum(resid(best_model.simpson, type = "pearson")^2)
1 - pchisq(dat.resid.simpson, df.residual(best_model.simpson))

# Assessing Deviance (G2)
deviance_value.simpson <- 1 - pchisq(as.numeric(-2 * logLik(best_model.simpson)), df.residual(best_model.simpson))

# Simulating datasets
dat.sim.simpson <- simulate(best_model.simpson, nsim = 250)

# Empirical cumulative density function calculations
resid.list.simpson <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.simpson)) {
    e.simpson <- ecdf(dat.sim.simpson[[i]] + runif(length(dat.sim.simpson[[i]]), -0.5, 0.5))
    resid.list.simpson[[i]] <- e.simpson(resid(best_model.simpson) + runif(length(resid(best_model.simpson)), -0.5, 0.5))
    plot(e.simpson, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.simpson <- do.call(c, resid.list.simpson)

# Quantile-quantile plot
qqnorm(all.resid.simpson, main = "QQ Plot")
qqline(all.resid.simpson, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.simpson), standardized.resid.simpson, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##18S-ITS
##simpson
###we will use a beta distribution, since we are working with percentages represented as values bound between 0 and 1

# Fit the full model with beta distribution
full_model.simpson.fun <- glmmTMB(Simpson ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                              family = beta_family(link = "logit"), data = fun_alpha_after_app, na.action = "na.fail")

##Perform model selection using dredge
#models.simpson.fun <- dredge(full_model.simpson.fun, m.min = 1)
models.simpson.fun <- dredge(full_model.simpson.fun, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.simpson.fun)

##Select the best model based on AIC
best_model.simpson.fun <- get.models(models.simpson.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.simpson.fun)

###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.simpson)

##Print model output
tab_model(best_model.simpson.fun)

###Marginalize the coefficients due to the use of a nonlinear link function###
simpson.margins.fun <- as.data.frame(marginal_effects(best_model.simpson.fun, type = "response"))
mean_est.simpson.fun <- c(mean(simpson.margins.fun$dydx_CultivarCZ4979X), mean(simpson.margins.fun$dydx_Developmental_StageR6), mean(simpson.margins.fun$dydx_Developmental_StageV6), mean(simpson.margins.fun$dydx_TreatmentControl))

print(mean_est.simpson.fun)

simpson.margins.2.fun <- simpson.margins.fun %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
simpson.margins.2.fun$mean_est <- c(-3.630994e-04, -2.550031e-03, -1.599351e-03,  3.447287e-05)

simpson.margins.2.fun$taxtype <- 'Eukaryote'
simpson.margins.2.fun$metric <- 'Simpson'

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.simpson.trt.fun <- powerSim(best_model.simpson.fun, fixed("Treatment"))
power.sim.simpson.cult.fun <- powerSim(best_model.simpson.fun, fixed("Cultivar"))
power.sim.simpson.ds.fun <- powerSim(best_model.simpson.fun, fixed("Developmental_Stage"))

power.sim.simpson.trt.fun 
power.sim.simpson.cult.fun 
power.sim.simpson.ds.fun 

##Decompose R2 to get contribution of each fixed effect
simpson.r2.fun <- glmm.hp(best_model.simpson.fun, type = "adjR2", commonality = FALSE) ##results not reliable, 
##but showing same variance for all fixed effects, so we will stick with this

print(simpson.r2.fun)
plot.glmmhp(simpson.r2.fun)

##Add r2
simpson.margins.2.fun$r2 <- c(33.32,  33.34,  33.34, 33.34) ##Add twice for growth stage

##Asess model performance
standardized.resid.simpson.fun <- resid(best_model.simpson.fun, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.simpson.fun <- sum(resid(best_model.simpson.fun, type = "pearson")^2)
1 - pchisq(dat.resid.simpson.fun, df.residual(best_model.simpson.fun))

# Assessing Deviance (G2)
deviance_value.simpson.fun <- 1 - pchisq(as.numeric(-2 * logLik(best_model.simpson.fun)), df.residual(best_model.simpson.fun))

# Simulating datasets
dat.sim.simpson.fun <- simulate(best_model.simpson.fun, nsim = 250)

# Empirical cumulative density function calculations
resid.list.simpson.fun <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.simpson.fun)) {
    e.simpson.fun <- ecdf(dat.sim.simpson.fun[[i]] + runif(length(dat.sim.simpson.fun[[i]]), -0.5, 0.5))
    resid.list.simpson.fun[[i]] <- e.simpson.fun(resid(best_model.simpson.fun) + runif(length(resid(best_model.simpson.fun)), -0.5, 0.5))
    plot(e.simpson.fun, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.simpson.fun <- do.call(c, resid.list.simpson.fun)

# Quantile-quantile plot
qqnorm(all.resid.simpson.fun, main = "QQ Plot")
qqline(all.resid.simpson.fun, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.simpson.fun), standardized.resid.simpson.fun, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##16S
##simpson for baseline
##Full model
full_model.base.simpson <- glmmTMB(Simpson ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = bacteria_alpha_baseline,
                          family = beta_family(link = "logit"),
                          na.action = "na.fail")

##Model selection
models.base.simpson <- dredge(full_model.base.simpson, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.simpson)

##Select the best model based on AIC
best_model.base.simpson <- get.models(models.base.simpson, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.simpson) ##Treatment statistically significant

##Get ME
base.simpson.margins <- as.data.frame(marginal_effects(best_model.base.simpson, type = "response"))

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##18S-ITS
##simpson for baseline
##Full model
full_model.base.simpson.fun <- glmmTMB(Simpson ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = fun_alpha_baseline,
                          family = beta_family(link = "logit"),
                          na.action = "na.fail")

##Model selection
models.base.simpson.fun <- dredge(full_model.base.simpson.fun, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.simpson.fun)

##Select the best model based on AIC
best_model.base.simpson.fun <- get.models(models.base.simpson.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.simpson.fun) ##Treatment statistically significant

##Get ME
base.simpson.margins.fun <- as.data.frame(marginal_effects(best_model.base.simpson.fun, type = "response"))

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##16S
##pielou
###we will use a beta distribution, since we are working with percentages represented as values bound between 0 and 1

# Fit the full model with beta distribution
full_model.pielou <- glmmTMB(Pielou ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                              family = beta_family(link = "logit"), data = bacteria_alpha_after_app, na.action = "na.fail")

##Perform model selection using dredge
models.pielou <- dredge(full_model.pielou, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.pielou)

##Select the best model based on AIC
best_model.pielou <- get.models(models.pielou, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.pielou)

###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.simpson)

##Print model output
tab_model(best_model.pielou)

###Marginalize the coefficients due to the use of a nonlinear link function###
pielou.margins <- as.data.frame(marginal_effects(best_model.pielou, type = "response"))
mean_est.pielou<- c(mean(pielou.margins$dydx_CultivarCZ4979X), mean(pielou.margins$dydx_Developmental_StageR6), mean(pielou.margins$dydx_Developmental_StageV6), mean(pielou.margins$dydx_TreatmentControl))

print(mean_est.pielou)

pielou.margins.2 <- pielou.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
pielou.margins.2$mean_est <- c(0.001083327, -0.016618599, -0.007245393,  0.006500081)

pielou.margins.2$taxtype <- 'Prokaryote'
pielou.margins.2$metric <- 'Pielou'

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.pielou.1.cult <- powerSim(best_model.pielou, fixed("Cultivar"))
power.sim.pielou.1.ds <- powerSim(best_model.pielou, fixed("Developmental_Stage"))
power.sim.pielou.1.trt <- powerSim(best_model.pielou, fixed("Treatment"))

power.sim.pielou.1.cult 
power.sim.pielou.1.ds
power.sim.pielou.1.trt

##Decompose R2 to get contribution of each fixed effect
pielou.r2 <- glmm.hp(best_model.pielou, type = "adjR2", commonality = FALSE) ##come back to this

print(pielou.r2)
plot.glmmhp(pielou.r2)

##Add r2
pielou.margins.2$r2 <- c(33.33, 33.33, 33.33, 33.33) ##Add twice for growth stage

##Assess model performance
standardized.resid.pielou <- resid(best_model.pielou, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.pielou <- sum(resid(best_model.pielou, type = "pearson")^2)
1 - pchisq(dat.resid.pielou, df.residual(best_model.pielou))

# Assessing Deviance (G2)
deviance_value.pielou <- 1 - pchisq(as.numeric(-2 * logLik(best_model.pielou)), df.residual(best_model.pielou))

# Simulating datasets
dat.sim.pielou <- simulate(best_model.pielou, nsim = 250)

# Empirical cumulative density function calculations
resid.list.pielou <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1))

for (i in 1:length(dat.sim.pielou)) {
    e.pielou <- ecdf(dat.sim.pielou[[i]] + runif(length(dat.sim.pielou[[i]]), -0.5, 0.5))
    resid.list.pielou[[i]] <- e.pielou(resid(best_model.pielou) + runif(length(resid(best_model.pielou)), -0.5, 0.5))
    plot(e.pielou, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.pielou <- do.call(c, resid.list.pielou)

# Quantile-quantile plot
qqnorm(all.resid.pielou, main = "QQ Plot")
qqline(all.resid.pielou, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.pielou), standardized.resid.pielou, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##18S-ITS
##pielou
###we will use a beta distribution, since we are working with percentages represented as values bound between 0 and 1

# Fit the full model with beta distribution
full_model.pielou.fun <- glmmTMB(Pielou ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                              family = beta_family(link = "logit"), data = fun_alpha_after_app, na.action = "na.fail")

##Perform model selection using dredge
#models.pielou.fun <- dredge(full_model.pielou.fun, m.min = 1)
models.pielou.fun <- dredge(full_model.pielou.fun, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.pielou.fun)

##Select the best model based on AIC
best_model.pielou.fun <- get.models(models.pielou.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.pielou.fun)

###Check for overdispersion###
##library(performance)
#check_overdispersion(best_model.simpson)

##Print model output
tab_model(best_model.pielou.fun)

###Marginalize the coefficients due to the use of a nonlinear link function###
pielou.margins.fun <- as.data.frame(marginal_effects(best_model.pielou.fun, type = "response"))
mean_est.pielou.fun<- c(mean(pielou.margins.fun$dydx_CultivarCZ4979X), mean(pielou.margins.fun$dydx_Developmental_StageR6), mean(pielou.margins.fun$dydx_Developmental_StageV6), mean(pielou.margins.fun$dydx_TreatmentControl))

print(mean_est.pielou.fun)

pielou.margins.2.fun <- pielou.margins.fun %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
pielou.margins.2.fun$mean_est <- c(-0.002843234, -0.022260983, -0.007583769,  0.002071269)

pielou.margins.2.fun$taxtype <- 'Eukaryote'
pielou.margins.2.fun$metric <- 'Pielou'

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.pielou.trt.fun <- powerSim(best_model.pielou.fun, fixed("Treatment"))
power.sim.pielou.cult.fun <- powerSim(best_model.pielou.fun, fixed("Cultivar"))
power.sim.pielou.ds.fun <- powerSim(best_model.pielou.fun, fixed("Developmental_Stage"))

power.sim.pielou.trt.fun 
power.sim.pielou.cult.fun 
power.sim.pielou.ds.fun 

##Decompose R2 to get contribution of each fixed effect
pielou.r2.fun <- glmm.hp(best_model.pielou.fun, type = "adjR2", commonality = FALSE) ##Issue with distribution; interpret with caution

print(pielou.r2.fun)
plot.glmmhp(pielou.r2.fun)

##Add r2
pielou.margins.2.fun$r2 <- c(1.81, 97.52, 97.52, 0.66) ##Add twice for growth stage

##Asess model performance
standardized.resid.pielou.fun <- resid(best_model.pielou.fun, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.pielou.fun <- sum(resid(best_model.pielou.fun, type = "pearson")^2)
1 - pchisq(dat.resid.pielou.fun, df.residual(best_model.pielou.fun))

# Assessing Deviance (G2)
deviance_value.pielou.fun <- 1 - pchisq(as.numeric(-2 * logLik(best_model.pielou.fun)), df.residual(best_model.pielou.fun))

# Simulating datasets
dat.sim.pielou.fun <- simulate(best_model.pielou.fun, nsim = 250)

# Empirical cumulative density function calculations
resid.list.pielou.fun <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.pielou.fun)) {
    e.pielou.fun <- ecdf(dat.sim.pielou.fun[[i]] + runif(length(dat.sim.pielou.fun[[i]]), -0.5, 0.5))
    resid.list.pielou.fun[[i]] <- e.pielou.fun(resid(best_model.pielou.fun) + runif(length(resid(best_model.pielou.fun)), -0.5, 0.5))
    plot(e.pielou.fun, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.pielou.fun <- do.call(c, resid.list.pielou.fun)

# Quantile-quantile plot
qqnorm(all.resid.pielou.fun, main = "QQ Plot")
qqline(all.resid.pielou.fun, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.pielou.fun), standardized.resid.pielou.fun, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##16S
##pielou for baseline
##Full model
full_model.base.pielou <- glmmTMB(Pielou ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = bacteria_alpha_baseline,
                          family = beta_family(link = "logit"),
                          na.action = "na.fail")

##Model selection
models.base.pielou <- dredge(full_model.base.pielou, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.pielou)

##Select the best model based on AIC
best_model.base.pielou <- get.models(models.base.pielou, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.pielou) ##Treatment statistically significant

##Get ME
base.pielou.margins <- as.data.frame(marginal_effects(best_model.base.pielou, type = "response"))

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##18S-ITS
##pielou for baseline
##Full model
full_model.base.pielou.fun <- glmmTMB(Pielou ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = fun_alpha_baseline,
                          family = beta_family(link = "logit"),
                          na.action = "na.fail")

##Model selection
models.base.pielou.fun <- dredge(full_model.base.pielou.fun, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.pielou.fun)

##Select the best model based on AIC
best_model.base.pielou.fun <- get.models(models.base.pielou.fun, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.pielou.fun)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Visualize alpha diversity
bac.alpha <- rbind(chao.margins.2, shannon.margins.2, simpson.margins.2, pielou.margins.2)
fun.alpha <- rbind(shannon.margins.2.fun, simpson.margins.2.fun, pielou.margins.2.fun, chao.margins.fun)

alpha.viz <- rbind(bac.alpha, fun.alpha)

alpha.viz$metric <- factor(alpha.viz$metric, levels = c("Chao1", "Shannon", "Simpson", "Pielou"))


##Rename terms
alpha.viz$Variable[alpha.viz$Variable == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
alpha.viz$Variable[alpha.viz$Variable  == 'dydx_Developmental_StageV6'] <- 'V6 vs R2'
alpha.viz$Variable[alpha.viz$Variable  == 'dydx_Developmental_StageR6'] <- 'R6 vs R2'
alpha.viz$Variable[alpha.viz$Variable  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'
alpha.viz$Variable[alpha.viz$Variable  == 'R6 vs R3'] <- 'R6 vs R2'
alpha.viz$Variable[alpha.viz$Variable  == 'V6 vs R3'] <- 'V6 vs R2'

##Need to add dummy values to facilitate visualization
alpha.viz.2 <- alpha.viz

##Create the new rows
new_rows <- data.frame(
  Predictor = NA,
  Variable = c("CZ4979X:R6", "CZ4979X:V6"),
  marg_est = NA,
  mean_est = NA,
  taxtype = c("Prokaryote", "Prokaryote"),
  metric = c("Chao1", "Chao1"),
  r2 = NA,
  stringsAsFactors = FALSE)

# Bind these rows to alpha.viz
alpha.viz.2 <- rbind(alpha.viz, new_rows)

##Attempt visualization
ggplot(alpha.viz.2, aes(x = Variable, y = mean_est, fill = taxtype, color = taxtype, group = taxtype)) +
  geom_col(aes(y = 0), alpha = 0.5, width = 0, color = "black") +
  geom_rect(aes(xmin = which(levels(as.factor(Variable)) == 'Control vs Biostimulant') - 0.5,
                xmax = which(levels(as.factor(Variable)) == 'Control vs Biostimulant') + 0.5,
                ymin = -Inf,
                ymax = Inf), alpha = 0.5, fill = "lightgray", color = "lightgray") +
  geom_rect(aes(xmin = which(levels(as.factor(Variable)) == 'CZ4979X:R6') - 0.5,
                xmax = which(levels(as.factor(Variable)) == 'CZ4979X:R6') + 0.5,
                ymin = -Inf,
                ymax = Inf), alpha = 0.5, fill = "lightgray", color = "lightgray") +
  geom_rect(aes(xmin = which(levels(as.factor(Variable)) == 'R6 vs R2') - 0.5,
                xmax = which(levels(as.factor(Variable)) == 'R6 vs R2') + 0.5,
                ymin = -Inf,
                ymax = Inf), alpha = 0.5, fill = "lightgray", color = "lightgray") +
  geom_col(position = position_dodge(width = 1, preserve = "single"), width = 0.3, colour = "black", size = 0.4) +
  geom_point(shape = 21, colour = "black", aes(size = r2), position = position_dodge2(width = 1)) +
  scale_size(range = c(3, 8)) +
  facet_wrap(~ metric, scales = "free_x", nrow = 1) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  scale_fill_manual(values = c("Prokaryote" = "#a8c0a8", "Eukaryote" = "#a890a8")) +
  scale_color_manual(values = c("Prokaryote" = "#a8c0a8", "Eukaryote" = "#a890a8")) +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 12, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 14, face = 'bold'),
        axis.text.x = element_text(size = 12, color = "black"),
        strip.text = element_text(size = 14, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        legend.position = 'none')

ggsave("alpha_coefficient_2.tiff", width = 10.2, height = 4.5, units = "in", dpi = 900)
```

```{r}
##16S
#prune negative controls and unmapped taxa from decontaminated phyloseq object

##prune controls
pruned_prok_decontam <- prune_samples(sample_data(prok_phyloseq_decontam)$bio_or_control == "biological", prok_phyloseq_decontam)

##Remove unmapped taxa
pruned_prok_decontam_mapped <- subset_taxa(pruned_prok_decontam, kingdom != "")
```

```{r}
##18S-ITS
#prune negative controls and unmapped taxa from decontaminated phyloseq object

##prune controls
pruned_fun_decontam <- prune_samples(sample_data(fun_phyloseq_decontam)$bio_or_control == "biological", fun_phyloseq_decontam)

##Remove unmapped taxa
pruned_fun_decontam_mapped <- subset_taxa(pruned_fun_decontam, kingdom != "")
```

```{r}
##16S
##Cumulative sum scaling (CSS) normalization of OTU abundance table and conglomerate taxa
##BiocManager::install("metagenomeSeq")
##library("metagenomeSeq")
##fitting into a Gaussian Model using metagenomeSeq

pruned_prok_decontam_norm <- phyloseq_to_metagenomeSeq(pruned_prok_decontam_mapped)
p_biom<-cumNormStat(pruned_prok_decontam_norm)
biom_quant<-cumNorm(pruned_prok_decontam_norm, p=p_biom)
normFactors(biom_quant)
pruned_prok_decontam_norm <-MRcounts(biom_quant, norm=T)

##create physeq object with normalized otu table
samples.df.2 <- head(samples_df, - 2)

samples.df.2$Location <- factor(samples.df.2$Location, levels = c("North", "Middle", "South"))

##create tax with decontaminated tax set
tax.mat.2 <- as.data.frame(tax_table(pruned_prok_decontam_mapped))

##Transform into matrix otu and tax tables (sample table can be left as data frame)
tax.mat.2 <- as.matrix(tax.mat.2)


##Transform to phyloseq objects
OTU.1 = otu_table(pruned_prok_decontam_norm, taxa_are_rows = TRUE)
  TAX.1 = tax_table(tax.mat.2)
  samples.1 = sample_data(samples.df.2)
  
prok_decontam_norm <- phyloseq(OTU.1, TAX.1, samples.1)

##Export these
##for taxonomy and otu tables, place ASVs in a column
tax.mat.3 <- as.data.frame(tax.mat.2)
tax.mat.3 <- data.frame('#TAXONOMY' = rownames(tax.mat.3), tax.mat.3)
colnames(tax.mat.3)[2:8] <- tools::toTitleCase(colnames(tax.mat.3)[2:8])

otu.function <- prok_decontam_norm
otu.function <- data.frame('#NAME' = rownames(otu.function), otu.function)

write.csv(tax.mat.3, file = "tax_mat_3.csv", row.names = TRUE)
write.csv(otu.function, file = "otu.function.csv", row.names = TRUE)
write.csv(samples.df.2, file = "samples_df_2.csv", row.names = TRUE)
```

```{r}
##18S-ITS
##Cumulative sum scaling (CSS) normalization of OTU abundance table
##BiocManager::install("metagenomeSeq")
##library("metagenomeSeq")

pruned_fun_decontam_norm <- phyloseq_to_metagenomeSeq(pruned_fun_decontam_mapped)
fun.p_biom<-cumNormStat(pruned_fun_decontam_norm)
fun.biom_quant<-cumNorm(pruned_fun_decontam_norm, p=fun.p_biom)
normFactors(fun.biom_quant)
pruned_fun_decontam_norm <-MRcounts(fun.biom_quant, norm=T)

##create physeq object with normalized otu table
##Use sample.df.2 from 16S script for new phyloseq object

##create tax with decontaminated tax set
fun.tax.mat.2 <- as.data.frame(tax_table(pruned_fun_decontam_mapped))

##Transform into matrix otu and tax tables (sample table can be left as data frame)
fun.tax.mat.2 <- as.matrix(fun.tax.mat.2)


##Transform to phyloseq objects
fun.OTU.1 = otu_table(pruned_fun_decontam_norm, taxa_are_rows = TRUE)
fun.TAX.1 = tax_table(fun.tax.mat.2)
fun.samples.1 = sample_data(samples.df.2)
  
fun_decontam_filter_norm <- phyloseq(fun.OTU.1, fun.TAX.1, fun.samples.1)
```

```{r}
##16S
##plot RA
##Follow brookeweigel comment here: https://github.com/joey711/phyloseq/issues/901 

physeq3 = transform_sample_counts(prok_decontam_norm, function(x) x / sum(x))
glom <- tax_glom(physeq3, taxrank = 'phylum')

data_glom<- psmelt(glom) # create dataframe from phyloseq object
data_glom$phylum <- as.character(data_glom$phylum) #convert to character

#simple way to rename phyla with < 1% abundance
data_glom$phylum[data_glom$Abundance < 0.05] <- "< 5% abund."

#Count phyla to set color palette-9
length(unique(data_glom$phylum))

data_glom$Developmental_Stage[data_glom$Developmental_Stage == "R3"] <- "R2"

data_glom$Developmental_Stage <- factor(data_glom$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))

##stacked barplot- phylum
ggplot(data_glom, aes(x = Developmental_Stage, y = Abundance, fill = phylum)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_grid(Cultivar~Treatment) +
  #scale_fill_manual(values=met.brewer("Monet", 9)) +
  scale_fill_ochre(palette = "mccrea") +
  theme_bw() +
  theme(axis.title = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), 
        strip.text = element_text(size = 16, face = "bold"), axis.text.x = element_text(size = 14, color = "black", face = "bold"), 
        legend.title = element_text(size = 16, face = "bold"), legend.text = element_text(size = 14)) +
  guides(fill=guide_legend(ncol=1)) +
  labs(fill = "Phylum")

#ggsave("Preliminary_bar_16S_phylum.tiff", width = 12, height = 9, units = "in", dpi = 900)

data_glom %>%
  group_by(Treatment, Cultivar, Developmental_Stage, phylum) %>%
  summarise(Abundance = sum(Abundance)) %>%
  mutate(Abundance = Abundance / sum(Abundance)) %>%
  ggplot(aes(x = factor(Developmental_Stage, levels = c("V1", "V6", "R2", "R6")), y = Abundance, fill = phylum)) +
  geom_bar(stat = "identity", position = "fill", color = "black") +
  facet_grid(Cultivar ~ Treatment) +
  scale_fill_ochre(palette = "mccrea") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +  # Set y-axis limits and format as percentage
  theme_bw() +
  theme(
    axis.title.y = element_text(size = 14, face = "bold"),
    axis.text.y = element_text(size = 12, color = "black"),
    axis.ticks = element_blank(),
    strip.text = element_text(size = 16, face = "bold"),
    axis.text.x = element_text(size = 14, color = "black", face = "bold"),
    axis.title.x = element_blank(),
    legend.title = element_text(size = 16, face = "bold"),
    legend.text = element_text(size = 14)) +
  guides(fill = guide_legend(ncol = 1)) +
  labs(fill = "Phylum", y = "Relative Abundance (%)")

##per-sample composition visualization
comp.viz <- merge(pruned_prok_decontam_norm, tax.mat.2, by.x = "row.names", by.y = "row.names", all = TRUE)
rownames(comp.viz) <- comp.viz$Row.names
comp.viz$Row.names <- NULL

# Removing columns
comp.viz_cleaned <- comp.viz %>%
  select(-c(65, 67:71))

# Pivoting longer
comp.viz_long <- comp.viz_cleaned %>%
  pivot_longer(cols = 1:64, names_to = "sample", values_to = "count")

##mutate based on data_glom
# Get unique phyla from data_glom
valid_phyla <- unique(data_glom$phylum)

# Modify phylum in comp.viz_long based on its presence in data_glom
comp.viz_long$phylum <- ifelse(comp.viz_long$phylum %in% valid_phyla, 
                                    comp.viz_long$phylum, 
                                    "< 5% abund.")

#unique(comp.viz_long$phylum)
##Summarize
data_glom.2 <- comp.viz_long %>%
  group_by(sample, phylum) %>%
  summarise(count = sum(count)) %>%
  ungroup()

##Modify and add metadata
data_glom.2 <- data_glom.2 %>%
  rename(Phylum = phylum)

data.glom.meta <- samples.df.2
data.glom.meta$sample <- rownames(data.glom.meta)

##Merge to get metadata
data_glom.2 <- merge(data.glom.meta, data_glom.2, by = c("sample"))

data_glom.2 <- data_glom.2 %>%
  rename(`Growth Stage` = Developmental_Stage) %>%
  mutate(Treatment = ifelse(Treatment == "Prebiotic", "Biostimulant", Treatment)) %>%
  mutate(`Growth Stage` = ifelse(`Growth Stage` == "R3", "R2", `Growth Stage`)) %>%
  mutate(`Growth Stage` = factor(`Growth Stage`, levels = c("V1", "V6", "R2", "R6")))

# Custom color scales
treatment_colors <- c('Control' = "#c3d6ce", 'Biostimulant' = "#c27668")
cultivar_colors <- c('CZ4979X' = "#ba7233", 'CZ4810X' = "#ced1af")
growth_stage_colors <- c('V1' = "#9b332b", 'V6' = "#697852", 'R2' = "#2b4655", 'R6' = "#a9845b")

# Base plot
p.1 <- ggplot(data_glom.2, aes(x = sample, y = count, fill = Phylum)) + 
  geom_bar(stat="identity", position="stack", color = 'black', size = 0.2) +
  scale_fill_ochre(palette = "mccrea") +
  new_scale_fill()

# Determine max radius
max_radius <- max(data_glom.2$count)
#max_radius <- max(aggregate(count ~ sample, data=data_glom.2, sum)$count)
max_count <- max(aggregate(count ~ sample, data=data_glom.2, sum)$count)


# Increment to position the annotation ring
#increment <- 0.2 # Adjust this as needed to get the desired thickness for the annotation ring
increment <- max_count * 0.05

# Position the annotation ring just outside the barplot
annotation_start <- max_count + increment
annotation_end <- annotation_start + increment

# Add Treatment annotation ring
p.2 <- p.1 + 
  geom_rect(data = distinct(data_glom.2, sample, Treatment), 
            aes(xmin = as.numeric(as.factor(sample)) - 0.5, 
                xmax = as.numeric(as.factor(sample)) + 0.5, 
                ymin = annotation_start, 
                ymax = annotation_end, 
                fill = Treatment),
            color = "black", size = 0.2, inherit.aes = FALSE) +
  scale_fill_manual(values = treatment_colors, name="Treatment") +
  new_scale_fill()

# Add Cultivar annotation ring
annotation_start.cult <- annotation_end
annotation_end.cult <- annotation_start.cult + increment

p.3 <- p.2 + 
  geom_rect(data = distinct(data_glom.2, sample, Cultivar), 
            aes(xmin = as.numeric(as.factor(sample)) - 0.5, 
                xmax = as.numeric(as.factor(sample)) + 0.5, 
                ymin = annotation_start.cult, 
                ymax = annotation_end.cult, 
                fill = Cultivar),
            color = "black", size = 0.2, inherit.aes = FALSE) +
  scale_fill_manual(values = cultivar_colors, name="Cultivar") +
  new_scale_fill()

# Add Growth Stage annotation ring
annotation_start.ds <- annotation_end.cult
annotation_end.ds <- annotation_start.ds + increment

p.4 <- p.3 + 
  geom_rect(data = distinct(data_glom.2, sample, `Growth Stage`), 
            aes(xmin = as.numeric(as.factor(sample)) - 0.5, 
                xmax = as.numeric(as.factor(sample)) + 0.5, 
                ymin = annotation_start.ds, 
                ymax = annotation_end.ds, 
                fill = `Growth Stage`),
            color = "black", size = 0.2, inherit.aes = FALSE) +
  scale_fill_manual(values = growth_stage_colors, name="Growth Stage") +
  new_scale_fill()

# Convert to circular and adjust theme elements
p.4 + 
  coord_polar(start = 0) +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    #panel.grid = element_blank(),
    plot.title = element_blank(),
    legend.position = "none")
    #legend.title = element_text(face = "bold")) 

ggsave("circle.comp.prok.tiff", width = 4, height = 4, units = "in", dpi = 900)
```

```{r}
##18S-ITS
##plot RA
##Follow brookeweigel comment here: https://github.com/joey711/phyloseq/issues/901 
mccrea = c(
        "#a8c0a8",
        "#c0c0a8",
        "#909078",
        "#a8a8a8",
        "#c0c0c0",
        "#d8c0a8",
        "#c09048",
        "#c07848")

fun.physeq3 = transform_sample_counts(fun_decontam_filter_norm, function(x) x / sum(x))
fun.glom <- tax_glom(fun.physeq3, taxrank = 'phylum')
fun.data_glom<- psmelt(fun.glom) # create dataframe from phyloseq object
fun.data_glom$phylum <- as.character(fun.data_glom$phylum) #convert to character

#simple way to rename phyla with < 1% abundance
fun.data_glom$phylum[fun.data_glom$Abundance < 0.05] <- "< 5% abund."

fun.data_glom$Treatment[fun.data_glom$Treatment == 'Prebiotic'] <- 'Biostimulant'

#Count genera to set color palette-8
length(unique(fun.data_glom$phylum))

fun.data_glom$Developmental_Stage[fun.data_glom$Developmental_Stage == "R3"] <- "R2"

fun.data_glom$Developmental_Stage <- factor(fun.data_glom$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))

##stacked barplot- genus
ggplot(fun.data_glom, aes(x = Developmental_Stage, y = Abundance, fill = phylum)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_grid(Cultivar~Treatment) +
  #scale_fill_manual(values=met.brewer("Monet", 8)) +
  scale_fill_ochre(palette = "parliament") +
  theme_bw() +
  theme(axis.title = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), 
        strip.text = element_text(size = 16, face = "bold"), axis.text.x = element_text(size = 14, color = "black", face = "bold"), 
        legend.title = element_text(size = 16, face = "bold"), legend.text = element_text(size = 14)) +
  guides(fill=guide_legend(ncol=1)) +
  labs(fill = "Phylum")

fun.data_glom %>%
  group_by(Treatment, Cultivar, Developmental_Stage, phylum) %>%
  summarise(Abundance = sum(Abundance)) %>%
  mutate(Abundance = Abundance / sum(Abundance)) %>%
  ggplot(aes(x = factor(Developmental_Stage, levels = c("V1", "V6", "R2", "R6")), y = Abundance, fill = phylum)) +
  geom_bar(stat = "identity", position = "fill", color = "black") +
  facet_grid(Cultivar ~ Treatment) +
  #scale_fill_ochre(palette = "mccrea") +
  #scale_fill_manual(values = rev(mccrea)) +
  scale_fill_manual(values=natparks.pals("Yellowstone", 8)) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +  # Set y-axis limits and format as percentage
  theme_bw() +
  theme(
    axis.title.y = element_text(size = 14, face = "bold"),
    axis.text.y = element_text(size = 12, color = "black"),
    axis.ticks = element_blank(),
    strip.text = element_text(size = 16, face = "bold"),
    axis.text.x = element_text(size = 14, color = "black", face = "bold"),
    axis.title.x = element_blank(),
    legend.title = element_text(size = 16, face = "bold"),
    legend.text = element_text(size = 14)) +
  guides(fill = guide_legend(ncol = 1)) +
  labs(fill = "Phylum", y = "Relative Abundance (%)")

ggsave("Preliminary_bar_fun_phylum.tiff", width = 12, height = 9, units = "in", dpi = 900)

##per-sample composition visualization
comp.viz.fun <- merge(pruned_fun_decontam_norm, fun.tax.mat.2, by.x = "row.names", by.y = "row.names", all = TRUE)
rownames(comp.viz.fun) <- comp.viz.fun$Row.names
comp.viz.fun$Row.names <- NULL

# Removing columns
comp.viz_cleaned.fun <- comp.viz.fun %>%
  select(-c(65, 67:71))

# Pivoting longer
comp.viz_long.fun <- comp.viz_cleaned.fun %>%
  pivot_longer(cols = 1:64, names_to = "sample", values_to = "count")

##mutate based on data_glom
# Get unique phyla from data_glom
valid_phyla.fun <- unique(fun.data_glom$phylum)

# Modify phylum in comp.viz_long based on its presence in data_glom
comp.viz_long.fun$phylum <- ifelse(comp.viz_long.fun$phylum %in% valid_phyla.fun, 
                                    comp.viz_long.fun$phylum, 
                                    "< 5% abund.")

#unique(comp.viz_long$phylum)
##Summarize
data_glom.2.fun <- comp.viz_long.fun %>%
  group_by(sample, phylum) %>%
  summarise(count = sum(count)) %>%
  ungroup()

##Modify and add metadata
data_glom.2.fun <- data_glom.2.fun %>%
  rename(Phylum = phylum)

data.glom.meta.fun <- samples.df.2
data.glom.meta.fun$sample <- rownames(data.glom.meta.fun)

##Merge to get metadata
data_glom.2.fun <- merge(data.glom.meta.fun, data_glom.2.fun, by = c("sample"))

data_glom.2.fun <- data_glom.2.fun %>%
  rename(`Growth Stage` = Developmental_Stage) %>%
  mutate(Treatment = ifelse(Treatment == "Prebiotic", "Biostimulant", Treatment)) %>%
  mutate(`Growth Stage` = ifelse(`Growth Stage` == "R3", "R2", `Growth Stage`)) %>%
  mutate(`Growth Stage` = factor(`Growth Stage`, levels = c("V1", "V6", "R2", "R6")))

# Custom color scales
#treatment_colors <- c('Control' = "#c3d6ce", 'Biostimulant' = "#c27668")
#cultivar_colors <- c('CZ4979X' = "#ba7233", 'CZ4810X' = "#ced1af")
#growth_stage_colors <- c('V1' = "#9b332b", 'V6' = "#697852", 'R2' = "#2b4655", 'R6' = "#a9845b")

# Base plot
p.1.fun <- ggplot(data_glom.2.fun, aes(x = sample, y = count, fill = Phylum)) + 
  geom_bar(stat="identity", position="stack", color = 'black', size = 0.2) +
  #scale_fill_manual(values = rev(mccrea)) +
  scale_fill_manual(values=natparks.pals("Yellowstone", 8)) +
  new_scale_fill()

# Determine max radius
max_radius.fun <- max(data_glom.2.fun$count)

max_count.fun <- max(aggregate(count ~ sample, data=data_glom.2.fun, sum)$count)


# Increment to position the annotation ring
increment.fun <- max_count.fun * 0.05

# Position the annotation ring just outside the barplot
annotation_start.fun <- max_count.fun + increment.fun
annotation_end.fun <- annotation_start.fun + increment.fun

# Add Treatment annotation ring
p.2.fun <- p.1.fun + 
  geom_rect(data = distinct(data_glom.2.fun, sample, Treatment), 
            aes(xmin = as.numeric(as.factor(sample)) - 0.5, 
                xmax = as.numeric(as.factor(sample)) + 0.5, 
                ymin = annotation_start.fun, 
                ymax = annotation_end.fun, 
                fill = Treatment),
            color = "black", size = 0.2, inherit.aes = FALSE) +
  scale_fill_manual(values = treatment_colors, name="Treatment") +
  new_scale_fill()

# Add Cultivar annotation ring
annotation_start.cult.fun <- annotation_end.fun
annotation_end.cult.fun <- annotation_start.cult.fun + increment.fun

p.3.fun <- p.2.fun + 
  geom_rect(data = distinct(data_glom.2.fun, sample, Cultivar), 
            aes(xmin = as.numeric(as.factor(sample)) - 0.5, 
                xmax = as.numeric(as.factor(sample)) + 0.5, 
                ymin = annotation_start.cult.fun, 
                ymax = annotation_end.cult.fun, 
                fill = Cultivar),
            color = "black", size = 0.2, inherit.aes = FALSE) +
  scale_fill_manual(values = cultivar_colors, name="Cultivar") +
  new_scale_fill()


# Add Growth Stage annotation ring
annotation_start.ds.fun <- annotation_end.cult.fun
annotation_end.ds.fun <- annotation_start.ds.fun + increment.fun

p.4.fun <- p.3.fun + 
  geom_rect(data = distinct(data_glom.2.fun, sample, `Growth Stage`), 
            aes(xmin = as.numeric(as.factor(sample)) - 0.5, 
                xmax = as.numeric(as.factor(sample)) + 0.5, 
                ymin = annotation_start.ds.fun, 
                ymax = annotation_end.ds.fun, 
                fill = `Growth Stage`),
            color = "black", size = 0.2, inherit.aes = FALSE) +
  scale_fill_manual(values = growth_stage_colors, name="Growth Stage") +
  new_scale_fill()

# Convert to circular and adjust theme elements
p.4.fun + 
  coord_polar(start = 0) +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    #panel.grid = element_blank(),
    plot.title = element_blank(),
    legend.position = "none")
    #legend.title = element_text(face = "bold")) 

ggsave("circle.comp.fun.tiff", width = 4, height = 4, units = "in", dpi = 900)
```

```{r}
##16S
###Polished ordinations
##ASV level
##First, let's change R3 to R2
sample_data_df <- sample_data(pruned_norm_real.ASV)
sample_data_df$Developmental_Stage[sample_data_df$Developmental_Stage == "R3"] <- "R2"
sample_data(pruned_norm_real.ASV) <- sample_data_df

##Beta diversity
real_prok_NMDS.ASV <- ordinate(pruned_norm_real.ASV, method ="NMDS", distance="bray")
real_prok_PCOA.ASV <- ordinate(pruned_norm_real.ASV, method ="PCoA", distance="bray")

ordination_scores <- as.data.frame(real_prok_PCOA.ASV$vectors)
ordination_scores.nmds <- as.data.frame(real_prok_NMDS.ASV$points)

#get metadata
metadata <- bacteria_alpha_after_app
rownames(metadata) <- metadata$sample

###PCOA
# Merge ordination scores with metadata
merged_data <- merge(ordination_scores, metadata, by = "row.names")
merged_data$Developmental_Stage[merged_data$Developmental_Stage == "R3"] <- "R2"


# Extracting variance explained
eigenvalues <- real_prok_PCOA.ASV$values$Eigenvalues
var_explained <- eigenvalues / sum(eigenvalues) * 100

x_var <- var_explained[1]
y_var <- var_explained[2]

# Create the basic plot with the necessary aesthetics
# Create the base plot
basic_ord_plot <- ggplot(data = merged_data, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") +  # assuming two cultivars
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments <- unique(merged_data$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments)) {
  basic_ord_plot <- basic_ord_plot +
    stat_ellipse(data = subset(merged_data, Treatment == treatments[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot <- basic_ord_plot +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities
final_plot <- basic_ord_plot +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot)

###NMDS
# Merge ordination scores with metadata
merged_data.nmds <- merge(ordination_scores.nmds, metadata, by = c("row.names"))
merged_data.nmds$Developmental_Stage[merged_data.nmds$Developmental_Stage == "R3"] <- "R2"

# Extracting variance explained
eigenvalues.nmds <- real_prok_NMDS.ASV$values$Eigenvalues
var_explained.nmds <- eigenvalues.nmds / sum(eigenvalues.nmds) * 100

x_var.nmds <- var_explained.nmds[1]
y_var.nmds <- var_explained.nmds[2]

# Create the basic plot with the necessary aesthetics
basic_ord_plot.nmds <- ggplot(data = merged_data.nmds, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") + 
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments <- unique(merged_data.nmds$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments)) {
  basic_ord_plot.nmds <- basic_ord_plot.nmds +
    stat_ellipse(data = subset(merged_data.nmds, Treatment == treatments[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.nmds <- basic_ord_plot.nmds +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities for NMDS plot
final_plot.nmds <- basic_ord_plot.nmds +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.nmds)


##See if composition differs by location in field
##Making two plots each to merge legends in final figure
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc <- ggplot(data = merged_data, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  scale_shape_manual(values = c(V6 = 21, R2 = 22, R6 = 23), 
                     breaks = c("V6", "R2", "R6"),
                     labels = c("V6", "R2", "R6")) +
  guides(
    fill = guide_legend(override.aes = list(shape = 21, size = 5)), # 21 is solid circle
    shape = guide_legend(title = "Growth Stage", title.position = "top", ncol = 1, override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc <- basic_ord_plot.loc +
    stat_ellipse(data = subset(merged_data, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc <- basic_ord_plot.loc +
  geom_point(aes(fill = Location, shape = Developmental_Stage, size = Shannon), color = 'black', alpha=0.9)


# Add the side densities
final_plot.loc <- basic_ord_plot.loc +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc)

##Now, location for NMDS
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.nmds <- ggplot(data = merged_data.nmds, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  scale_shape_manual(values = c(V6 = 21, R2 = 22, R6 = 23), 
                     breaks = c("V6", "R2", "R6"),
                     labels = c("V6", "R2", "R6")) +
  guides(
    fill = guide_legend(override.aes = list(shape = 21, size = 5)), # 21 is solid circle
    shape = guide_legend(title = "Growth Stage", title.position = "top", ncol = 1, override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.nmds <- basic_ord_plot.loc.nmds +
    stat_ellipse(data = subset(merged_data.nmds, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.nmds <- basic_ord_plot.loc.nmds +
  geom_point(aes(fill = Location, shape = Developmental_Stage, size = Shannon), color = 'black', alpha=0.9)


# Add the side densities
final_plot.loc.nmds <- basic_ord_plot.loc.nmds +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.nmds)
```

```{r}
##18S-ITS
###Polished ordinations
##ASV level
fun.pruned_norm_baseline.ASV <- prune_samples(sample_data(fun_decontam_filter_norm)$Developmental_Stage == "V1", fun_decontam_filter_norm)

fun.pruned_norm_real.ASV <- prune_samples(!sample_data(fun_decontam_filter_norm)$Developmental_Stage == "V1", fun_decontam_filter_norm)

##Let's change R3 to R2
sample_data_df <- sample_data(fun.pruned_norm_real.ASV)
sample_data_df$Developmental_Stage[sample_data_df$Developmental_Stage == "R3"] <- "R2"
sample_data(fun.pruned_norm_real.ASV) <- sample_data_df

real_fun_NMDS.ASV <- ordinate(fun.pruned_norm_real.ASV, method ="NMDS", distance="bray")
real_fun_PCOA.ASV <- ordinate(fun.pruned_norm_real.ASV, method ="PCoA", distance="bray")

ordination_scores.fun <- as.data.frame(real_fun_PCOA.ASV$vectors)
ordination_scores.nmds.fun <- as.data.frame(real_fun_NMDS.ASV$points)

#get metadata
metadata.fun <- fun_alpha_after_app
rownames(metadata.fun) <- metadata.fun$sample

###PCOA
# Merge ordination scores with metadata
merged_data.fun <- merge(ordination_scores.fun, metadata.fun, by = "row.names")
merged_data.fun$Developmental_Stage[merged_data.fun$Developmental_Stage == "R3"] <- "R2"

# Extracting variance explained
eigenvalues.fun <- real_fun_PCOA.ASV$values$Eigenvalues
var_explained.fun <- eigenvalues.fun / sum(eigenvalues.fun) * 100

x_var.fun <- var_explained.fun[1]
y_var.fun <- var_explained.fun[2]

# Create the basic plot with the necessary aesthetics
# Create the base plot
basic_ord_plot.fun <- ggplot(data = merged_data.fun, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var.fun, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var.fun, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") + 
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments <- unique(merged_data.fun$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments)) {
  basic_ord_plot.fun <- basic_ord_plot.fun +
    stat_ellipse(data = subset(merged_data.fun, Treatment == treatments[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.fun <- basic_ord_plot.fun +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities
final_plot.fun <- basic_ord_plot.fun +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.fun)

###NMDS
# Merge ordination scores with metadata
merged_data.nmds.fun <- merge(ordination_scores.nmds.fun, metadata.fun, by = c("row.names"))
merged_data.nmds.fun$Developmental_Stage[merged_data.nmds.fun$Developmental_Stage == "R3"] <- "R2"

# Extracting variance explained
eigenvalues.nmds.fun <- real_fun_NMDS.ASV$values$Eigenvalues
var_explained.nmds.fun <- eigenvalues.nmds.fun / sum(eigenvalues.nmds.fun) * 100

x_var.nmds.fun <- var_explained.nmds.fun[1]
y_var.nmds.fun <- var_explained.nmds.fun[2]

# Create the basic plot with the necessary aesthetics
basic_ord_plot.nmds.fun <- ggplot(data = merged_data.nmds.fun, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") + 
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments <- unique(merged_data.nmds.fun$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments)) {
  basic_ord_plot.nmds.fun <- basic_ord_plot.nmds.fun +
    stat_ellipse(data = subset(merged_data.nmds.fun, Treatment == treatments[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.nmds.fun <- basic_ord_plot.nmds.fun +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities for NMDS plot
final_plot.nmds.fun <- basic_ord_plot.nmds.fun +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.nmds.fun)


##See if composition differs by location in field
##Making two plots each to merge legends in final figure
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.fun <- ggplot(data = merged_data.fun, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var.fun, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var.fun, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  scale_shape_manual(values = c(V6 = 21, R2 = 22, R6 = 23), 
                     breaks = c("V6", "R2", "R6"),
                     labels = c("V6", "R2", "R6")) +
  guides(
    fill = guide_legend(override.aes = list(shape = 21, size = 5)), # 21 is solid circle
    shape = guide_legend(title = "Growth Stage", title.position = "top", ncol = 1, override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.fun <- basic_ord_plot.loc.fun +
    stat_ellipse(data = subset(merged_data.fun, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.fun <- basic_ord_plot.loc.fun +
  geom_point(aes(fill = Location, shape = Developmental_Stage, size = Shannon), color = 'black', alpha=0.9)

# Add the side densities
final_plot.loc.fun <- basic_ord_plot.loc.fun +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.fun)

##Now, location for NMDS
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.nmds.fun <- ggplot(data = merged_data.nmds.fun, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  scale_shape_manual(values = c(V6 = 21, R2 = 22, R6 = 23), 
                     breaks = c("V6", "R2", "R6"),
                     labels = c("V6", "R2", "R6")) +
  guides(
    fill = guide_legend(override.aes = list(shape = 21, size = 5)), # 21 is solid circle
    shape = guide_legend(title = "Growth Stage", title.position = "top", ncol = 1, override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.nmds.fun <- basic_ord_plot.loc.nmds.fun +
    stat_ellipse(data = subset(merged_data.nmds.fun, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.nmds.fun <- basic_ord_plot.loc.nmds.fun +
  geom_point(aes(fill = Location, shape = Developmental_Stage, size = Shannon), color = 'black', alpha=0.9)


# Add the side densities
final_plot.loc.nmds.fun <- basic_ord_plot.loc.nmds.fun +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.nmds.fun)
```

```{r}
##16S
###Polished ordinations- Baseline
##ASV level

pruned_norm_baseline.ASV <- prune_samples(sample_data(prok_decontam_norm)$Developmental_Stage == "V1", prok_decontam_norm)

pruned_norm_real.ASV <- prune_samples(!sample_data(prok_decontam_norm)$Developmental_Stage == "V1", prok_decontam_norm)

##Baseline
baseline_prok_NMDS.ASV <- ordinate(pruned_norm_baseline.ASV, method ="NMDS", distance="bray")
###NMDS- Warning: skipping half-change scaling: too few points below threshold
baseline_prok_PCOA.ASV <- ordinate(pruned_norm_baseline.ASV, method ="PCoA", distance="bray")

ordination_scores.base <- as.data.frame(baseline_prok_PCOA.ASV$vectors)
ordination_scores.nmds.base <- as.data.frame(baseline_prok_NMDS.ASV$points)

#get metadata
metadata.base <- bacteria_alpha_baseline
rownames(metadata.base) <- metadata.base$sample

###PCOA
# Merge ordination scores with metadata
merged_data.base <- merge(ordination_scores.base, metadata.base, by = "row.names")

# Extracting variance explained
eigenvalues.base <- baseline_prok_PCOA.ASV$values$Eigenvalues
var_explained.base <- eigenvalues.base / sum(eigenvalues.base) * 100

x_var.base <- var_explained.base[1]
y_var.base <- var_explained.base[2]

# Create the basic plot with the necessary aesthetics
# Create the base plot
basic_ord_plot.base <- ggplot(data = merged_data.base, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") +  # assuming two cultivars
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments.base <- unique(merged_data.base$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments.base)) {
  basic_ord_plot.base <- basic_ord_plot.base +
    stat_ellipse(data = subset(merged_data.base, Treatment == treatments.base[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.base <- basic_ord_plot.base +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities
final_plot.base <- basic_ord_plot.base +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.base)

###NMDS
# Merge ordination scores with metadata
merged_data.nmds.base <- merge(ordination_scores.nmds.base, metadata.base, by = c("row.names"))

# Extracting variance explained
eigenvalues.nmds.base <- baseline_prok_NMDS.ASV$values$Eigenvalues
var_explained.nmds.base <- eigenvalues.nmds.base / sum(eigenvalues.nmds.base) * 100

x_var.nmds.base <- var_explained.nmds.base[1]
y_var.nmds.base <- var_explained.nmds.base[2]

# Create the basic plot with the necessary aesthetics
basic_ord_plot.nmds.base <- ggplot(data = merged_data.nmds.base, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") + 
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments.base <- unique(merged_data.nmds.base$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments.base)) {
  basic_ord_plot.nmds.base <- basic_ord_plot.nmds.base +
    stat_ellipse(data = subset(merged_data.nmds.base, Treatment == treatments.base[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.nmds.base <- basic_ord_plot.nmds.base +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities for NMDS plot
final_plot.nmds.base <- basic_ord_plot.nmds.base +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.nmds.base)

##See if composition differs by location in field
##Making two plots each to merge legends in final figure
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.base <- ggplot(data = merged_data.base, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  guides(title.position = "top", ncol = 1, override.aes = list(size = 5)) +
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.base <- basic_ord_plot.loc.base +
    stat_ellipse(data = subset(merged_data.base, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.base <- basic_ord_plot.loc.base +
  geom_point(aes(fill = Location, size = Shannon), color = 'black', alpha=0.9, shape = 21)


# Add the side densities
final_plot.loc.base <- basic_ord_plot.loc.base +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.base)

##Now, location for NMDS
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.nmds.base <- ggplot(data = merged_data.nmds.base, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  guides(title.position = "top", ncol = 1, override.aes = list(size = 5)) +
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.nmds.base <- basic_ord_plot.loc.nmds.base +
    stat_ellipse(data = subset(merged_data.nmds.base, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.nmds.base <- basic_ord_plot.loc.nmds.base +
  geom_point(aes(fill = Location, size = Shannon), color = 'black', alpha=0.9, shape = 21)


# Add the side densities
final_plot.loc.nmds.base <- basic_ord_plot.loc.nmds.base +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.nmds.base)
```

```{r}
##18S-ITS
###Polished ordinations- Baseline
##ASV level

fun.pruned_norm_baseline.ASV <- prune_samples(sample_data(fun_decontam_filter_norm)$Developmental_Stage == "V1", fun_decontam_filter_norm)

fun.pruned_norm_real.ASV <- prune_samples(!sample_data(fun_decontam_filter_norm)$Developmental_Stage == "V1", fun_decontam_filter_norm)

##Baseline
baseline_fun_NMDS.ASV <- ordinate(fun.pruned_norm_baseline.ASV, method ="NMDS", distance="bray")
###NMDS- Warning: skipping half-change scaling: too few points below threshold
baseline_fun_PCOA.ASV <- ordinate(fun.pruned_norm_baseline.ASV, method ="PCoA", distance="bray")

ordination_scores.fun.base <- as.data.frame(baseline_fun_PCOA.ASV$vectors)
ordination_scores.nmds.fun.base <- as.data.frame(baseline_fun_NMDS.ASV$points)

#get metadata
metadata.fun.base <- fun_alpha_baseline
rownames(metadata.fun.base) <- metadata.fun.base$sample

###PCOA
# Merge ordination scores with metadata
merged_data.fun.base <- merge(ordination_scores.fun.base, metadata.fun.base, by = "row.names")

# Extracting variance explained
eigenvalues.fun.base <- baseline_fun_PCOA.ASV$values$Eigenvalues
var_explained.fun.base <- eigenvalues.fun.base / sum(eigenvalues.fun.base) * 100

x_var.fun.base <- var_explained.fun.base[1]
y_var.fun.base <- var_explained.fun.base[2]

# Create the basic plot with the necessary aesthetics
# Create the base plot
basic_ord_plot.fun.base <- ggplot(data = merged_data.fun.base, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var.fun.base, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var.fun.base, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") +  
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments <- unique(merged_data.fun.base$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments)) {
  basic_ord_plot.fun.base <- basic_ord_plot.fun.base +
    stat_ellipse(data = subset(merged_data.fun.base, Treatment == treatments[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.fun.base <- basic_ord_plot.fun.base +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities
final_plot.fun.base <- basic_ord_plot.fun.base +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.fun.base)

###NMDS
# Merge ordination scores with metadata
merged_data.nmds.fun.base <- merge(ordination_scores.nmds.fun.base, metadata.fun.base, by = c("row.names"))

# Extracting variance explained
eigenvalues.nmds.fun.base <- baseline_fun_NMDS.ASV$values$Eigenvalues
var_explained.nmds.fun.base <- eigenvalues.nmds.fun.base / sum(eigenvalues.nmds.fun.base) * 100

x_var.nmds.fun.base <- var_explained.nmds.fun.base[1]
y_var.nmds.fun.base <- var_explained.nmds.fun.base[2]

# Create the basic plot with the necessary aesthetics
basic_ord_plot.nmds.fun.base <- ggplot(data = merged_data.nmds.fun.base, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 18, color = "black", face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values=c("#c27668","#c3d6ce"), name = "Treatment") +
  scale_shape_manual(values=c(21, 22), name = "Cultivar") + 
  guides(fill = guide_legend(override.aes = list(shape = 21, size = 5)),
         shape = guide_legend(override.aes = list(size = 5)))

# Add ellipses for each Treatment
treatments <- unique(merged_data.nmds.fun.base$Treatment)
colors <- c("#c27668","#c3d6ce")

for(i in 1:length(treatments)) {
  basic_ord_plot.nmds.fun.base <- basic_ord_plot.nmds.fun.base +
    stat_ellipse(data = subset(merged_data.nmds.fun.base, Treatment == treatments[i]),
                 aes(group = Treatment, fill = Treatment),
                 color = colors[i], fill = colors[i], type = "norm", geom = "polygon",
                 alpha = 0.2, show.legend = FALSE)
}

# Add the points on top
basic_ord_plot.nmds.fun.base <- basic_ord_plot.nmds.fun.base +
  geom_point(aes(fill = Treatment, shape = Cultivar, size = Shannon), color = 'black', alpha=0.9)

# Side densities for NMDS plot
final_plot.nmds.fun.base <- basic_ord_plot.nmds.fun.base +
  ggside::geom_xsidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Treatment), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::theme_ggside_void()

print(final_plot.nmds.fun.base)

##See if composition differs by location in field
##Making two plots each to merge legends in final figure
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.fun.base <- ggplot(data = merged_data.fun.base, aes(x = Axis.1, y = Axis.2)) +
  theme_bw() +
  labs(x = paste0("PCoA 1 (", round(x_var.fun.base, 2), "%)"),
       y = paste0("PCoA 2 (", round(y_var.fun.base, 2), "%)")) +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  guides(
    fill = guide_legend(override.aes = list(shape = 21, size = 5)), # 21 is solid circle
    shape = guide_legend(title.position = "top", ncol = 1, override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.fun.base <- basic_ord_plot.loc.fun.base +
    stat_ellipse(data = subset(merged_data.fun.base, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.fun.base <- basic_ord_plot.loc.fun.base +
  geom_point(aes(fill = Location, size = Shannon), color = 'black', alpha=0.9, shape = 21)

# Add the side densities
final_plot.loc.fun.base <- basic_ord_plot.loc.fun.base +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.fun.base)

##Now, location for NMDS
# Define the ellipse aesthetics for each Location
colors <- c("#a8c0a8", "#a890a8", "#c07848")
locations <- c('North', 'Middle', 'South')

# Create the base plot
basic_ord_plot.loc.nmds.fun.base <- ggplot(data = merged_data.nmds.fun.base, aes(x = MDS1, y = MDS2)) +
  theme_bw() +
  labs(x = "NMDS 1",
       y = "NMDS 2") +
  scale_size_continuous(range = c(2, 12), guide = 'none') +
  theme(
    axis.text = element_text(size = 16, color = "black"),
    axis.title = element_text(size = 18, color = "black", face = "bold"),
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 18, color = "black")) +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South'), name = "Location") +
  guides(
    fill = guide_legend(override.aes = list(shape = 21, size = 5)), # 21 is solid circle
    shape = guide_legend(title.position = "top", ncol = 1, override.aes = list(size = 5)))

# Add ellipses for each Location individually
for(i in 1:3) {
  basic_ord_plot.loc.nmds.fun.base <- basic_ord_plot.loc.nmds.fun.base +
    stat_ellipse(data = subset(merged_data.nmds.fun.base, Location == locations[i]),
                 aes(group = Location, fill = Location),
                 color = colors[i],
                 fill = colors[i],
                 type = "norm", geom = "polygon", alpha = 0.2, show.legend = FALSE)
}

# Add the points on top of the ellipses
basic_ord_plot.loc.nmds.fun.base <- basic_ord_plot.loc.nmds.fun.base +
  geom_point(aes(fill = Location, size = Shannon), color = 'black', alpha=0.9, shape = 21)


# Add the side densities
final_plot.loc.nmds.fun.base <- basic_ord_plot.loc.nmds.fun.base +
  ggside::geom_xsidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  ggside::geom_ysidedensity(aes(fill = Location), alpha = 0.8, show.legend = FALSE, color = 'black') +
  scale_fill_manual(values = c("#a8c0a8","#a890a8", "#c07848"), breaks = c('North', 'Middle', 'South')) +
  ggside::theme_ggside_void()

print(final_plot.loc.nmds.fun.base)
```

```{r}
##16S
##ASV level
##PERMANOVA
##Bray

##Make a data frame from the sample_data
real.sample_data.ASV <- data.frame(sample_data(pruned_norm_real.ASV))  # Convert sample metadata to a data frame

##Calculate bray curtis distance matrix
real.sample_bray.ASV <- phyloseq::distance(pruned_norm_real.ASV, method = "bray")

# Set up the formula with interactions for adonis2
real.formula.ASV <- real.sample_bray.ASV ~ (Treatment + Cultivar + Developmental_Stage)^2 

# Perform PERMANOVA analysis
real.permanova_result.ASV <- adonis2(real.formula.ASV, data = real.sample_data.ASV, strata = real.sample_data.ASV$Location, permutations = 9999)

###add corrected p-value
real.permanova_result.ASV$qval <- p.adjust(real.permanova_result.ASV$`Pr(>F)`, "fdr")

# Print and export the result
print(real.permanova_result.ASV)
write.csv(real.permanova_result.ASV, "prok.real.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
real.sample_data.ASV$trt.cult <- paste(real.sample_data.ASV$Treatment, real.sample_data.ASV$Cultivar)
real.sample_data.ASV$trt.ds <- paste(real.sample_data.ASV$Treatment, real.sample_data.ASV$Developmental_Stage)
real.sample_data.ASV$cult.ds <- paste(real.sample_data.ASV$Cultivar, real.sample_data.ASV$Developmental_Stage)

##Treatment
prok.real.beta.trt.ASV <- betadisper(real.sample_bray.ASV, real.sample_data.ASV$Treatment)
plot(prok.real.beta.trt.ASV)
anova(prok.real.beta.trt.ASV, permutations = 9999)
prok.real.beta.trt.pt.ASV <- permutest(prok.real.beta.trt.ASV, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.ASV)
plot(TukeyHSD(prok.real.beta.trt.ASV), las = 1)

##Cultivar
prok.real.beta.cult.ASV <- betadisper(real.sample_bray.ASV, real.sample_data.ASV$Cultivar)
plot(prok.real.beta.cult.ASV)
anova(prok.real.beta.cult.ASV, permutations = 9999)
prok.real.beta.cult.pt.ASV <- permutest(prok.real.beta.cult.ASV, permutations = 9999, pairwise = T)
scores(prok.real.beta.cult.ASV)
plot(TukeyHSD(prok.real.beta.cult.ASV), las = 1)

##Developmental stage
prok.real.beta.ds.ASV <- betadisper(real.sample_bray.ASV, real.sample_data.ASV$Developmental_Stage)
plot(prok.real.beta.ds.ASV)
anova(prok.real.beta.ds.ASV, permutations = 9999)
permutest(prok.real.beta.ds.ASV, permutations = 9999, pairwise = T)
scores(prok.real.beta.ds.ASV)
plot(TukeyHSD(prok.real.beta.ds.ASV), las = 1)

##Treatment:Cultivar
prok.real.beta.trt.cult.ASV <- betadisper(real.sample_bray.ASV, real.sample_data.ASV$trt.cult)
plot(prok.real.beta.trt.cult.ASV)
anova(prok.real.beta.trt.cult.ASV, permutations = 9999)
permutest(prok.real.beta.trt.cult.ASV, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.cult.ASV)
plot(TukeyHSD(prok.real.beta.trt.cult.ASV), las = 1)

##Treatment:Developmental Stage
prok.real.beta.trt.ds.ASV <- betadisper(real.sample_bray.ASV, real.sample_data.ASV$trt.ds)
plot(prok.real.beta.trt.ds.ASV)
anova(prok.real.beta.trt.ds.ASV, permutations = 9999)
permutest(prok.real.beta.trt.ds.ASV, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.ds.ASV)
plot(TukeyHSD(prok.real.beta.trt.ds.ASV), las = 1)

##Cultivar:Developmental Stage
prok.real.beta.cult.ds.ASV <- betadisper(real.sample_bray.ASV, real.sample_data.ASV$cult.ds)
plot(prok.real.beta.cult.ds.ASV)
anova(prok.real.beta.cult.ds.ASV, permutations = 9999)
permutest(prok.real.beta.cult.ds.ASV, permutations = 9999, pairwise = T)
scores(prok.real.beta.cult.ds.ASV)
plot(TukeyHSD(prok.real.beta.cult.ds.ASV), las = 1)
```

```{r}
##18S-ITS
##ASV level
##PERMANOVA
##Bray

##Make a data frame from the sample_data
fun.real.sample_data.ASV <- data.frame(sample_data(fun.pruned_norm_real.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
fun.real.sample_bray.ASV <- phyloseq::distance(fun.pruned_norm_real.ASV, method = "bray")

# Set up the formula with interactions for adonis2
fun.real.formula.ASV <- fun.real.sample_bray.ASV ~ (Treatment + Cultivar + Developmental_Stage)^2 

# Perform PERMANOVA analysis
fun.real.permanova_result.ASV <- adonis2(fun.real.formula.ASV, data = fun.real.sample_data.ASV, strata = fun.real.sample_data.ASV$Location, permutations = 9999)

###add corrected p-value
fun.real.permanova_result.ASV$qval <- p.adjust(fun.real.permanova_result.ASV$`Pr(>F)`, "fdr")

# Print and export the result
print(fun.real.permanova_result.ASV)
write.csv(fun.real.permanova_result.ASV, "fun.prok.real.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
fun.real.sample_data.ASV$trt.cult <- paste(fun.real.sample_data.ASV$Treatment, fun.real.sample_data.ASV$Cultivar)
fun.real.sample_data.ASV$trt.ds <- paste(fun.real.sample_data.ASV$Treatment, fun.real.sample_data.ASV$Developmental_Stage)
fun.real.sample_data.ASV$cult.ds <- paste(fun.real.sample_data.ASV$Cultivar, fun.real.sample_data.ASV$Developmental_Stage)

##Treatment
fun.real.beta.trt.ASV <- betadisper(fun.real.sample_bray.ASV, fun.real.sample_data.ASV$Treatment)
plot(fun.real.beta.trt.ASV)
anova(fun.real.beta.trt.ASV, permutations = 9999)
fun.real.beta.trt.pt.ASV <- permutest(fun.real.beta.trt.ASV, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.ASV)
plot(TukeyHSD(fun.real.beta.trt.ASV), las = 1)

##Cultivar
fun.real.beta.cult.ASV <- betadisper(fun.real.sample_bray.ASV, fun.real.sample_data.ASV$Cultivar)
plot(fun.real.beta.cult.ASV)
anova(fun.real.beta.cult.ASV, permutations = 9999)
fun.real.beta.cult.pt.ASV <- permutest(fun.real.beta.cult.ASV, permutations = 9999, pairwise = T)
scores(fun.real.beta.cult.ASV)
plot(TukeyHSD(fun.real.beta.cult.ASV), las = 1)

##Developmental stage
fun.real.beta.ds.ASV <- betadisper(fun.real.sample_bray.ASV, fun.real.sample_data.ASV$Developmental_Stage)
plot(fun.real.beta.ds.ASV)
anova(fun.real.beta.ds.ASV, permutations = 9999)
permutest(fun.real.beta.ds.ASV, permutations = 9999, pairwise = T)
scores(fun.real.beta.ds.ASV)
plot(TukeyHSD(fun.real.beta.ds.ASV), las = 1)

##Treatment:Cultivar
fun.real.beta.trt.cult.ASV <- betadisper(fun.real.sample_bray.ASV, fun.real.sample_data.ASV$trt.cult)
plot(fun.real.beta.trt.cult.ASV)
anova(fun.real.beta.trt.cult.ASV, permutations = 9999)
permutest(fun.real.beta.trt.cult.ASV, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.cult.ASV)
plot(TukeyHSD(fun.real.beta.trt.cult.ASV), las = 1)

##Treatment:Developmental Stage
fun.real.beta.trt.ds.ASV <- betadisper(fun.real.sample_bray.ASV, fun.real.sample_data.ASV$trt.ds)
plot(fun.real.beta.trt.ds.ASV)
anova(fun.real.beta.trt.ds.ASV, permutations = 9999)
permutest(fun.real.beta.trt.ds.ASV, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.ds.ASV)
plot(TukeyHSD(fun.real.beta.trt.ds.ASV), las = 1)

##Cultivar:Developmental Stage
fun.real.beta.cult.ds.ASV <- betadisper(fun.real.sample_bray.ASV, fun.real.sample_data.ASV$cult.ds)
plot(fun.real.beta.cult.ds.ASV)
anova(fun.real.beta.cult.ds.ASV, permutations = 9999)
permutest(fun.real.beta.cult.ds.ASV, permutations = 9999, pairwise = T)
scores(fun.real.beta.cult.ds.ASV)
plot(TukeyHSD(fun.real.beta.cult.ds.ASV), las = 1)
```

```{r}
##16S
##ASV level
##PERMANOVA
##Jaccard

##Make a data frame from the sample_data
real.sample_data.jac <- data.frame(sample_data(pruned_norm_real.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
real.sample_jac <- phyloseq::distance(pruned_norm_real.ASV, method = "jaccard")


# Set up the formula with interactions for adonis2
real.formula.jac <- real.sample_jac ~ (Treatment + Cultivar + Developmental_Stage)^2 

# Perform PERMANOVA analysis
real.permanova_result.jac <- adonis2(real.formula.jac, data = real.sample_data.jac, strata = real.sample_data.jac$Location, permutations = 9999)

###add corrected p-value
real.permanova_result.jac$qval <- p.adjust(real.permanova_result.jac$`Pr(>F)`, "fdr")

# Print and export the result
print(real.permanova_result.jac)
write.csv(real.permanova_result.jac, "prok.real.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
real.sample_data.jac$trt.cult <- paste(real.sample_data.jac$Treatment, real.sample_data.jac$Cultivar)
real.sample_data.jac$trt.ds <- paste(real.sample_data.jac$Treatment, real.sample_data.jac$Developmental_Stage)
real.sample_data.jac$cult.ds <- paste(real.sample_data.jac$Cultivar, real.sample_data.jac$Developmental_Stage)

##Treatment
prok.real.beta.trt.jac <- betadisper(real.sample_jac, real.sample_data.jac$Treatment)
plot(prok.real.beta.trt.jac)
anova(prok.real.beta.trt.jac, permutations = 9999)
prok.real.beta.trt.pt.jac <- permutest(prok.real.beta.trt.jac, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.jac)
plot(TukeyHSD(prok.real.beta.trt.jac), las = 1)

##Cultivar
prok.real.beta.cult.jac <- betadisper(real.sample_jac, real.sample_data.jac$Cultivar)
plot(prok.real.beta.cult.jac)
anova(prok.real.beta.cult.jac, permutations = 9999)
prok.real.beta.cult.pt.jac <- permutest(prok.real.beta.cult.jac, permutations = 9999, pairwise = T)
scores(prok.real.beta.cult.jac)
plot(TukeyHSD(prok.real.beta.cult.jac), las = 1)

##Developmental stage
prok.real.beta.ds.jac <- betadisper(real.sample_jac, real.sample_data.jac$Developmental_Stage)
plot(prok.real.beta.ds.jac)
anova(prok.real.beta.ds.jac, permutations = 9999)
permutest(prok.real.beta.ds.jac, permutations = 9999, pairwise = T)
scores(prok.real.beta.ds.jac)
plot(TukeyHSD(prok.real.beta.ds.jac), las = 1)

##Treatment:Cultivar
prok.real.beta.trt.cult.jac <- betadisper(real.sample_jac, real.sample_data.jac$trt.cult)
plot(prok.real.beta.trt.cult.jac)
anova(prok.real.beta.trt.cult.jac, permutations = 9999)
permutest(prok.real.beta.trt.cult.jac, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.cult.jac)
plot(TukeyHSD(prok.real.beta.trt.cult.jac), las = 1)

##Treatment:Developmental Stage
prok.real.beta.trt.ds.jac <- betadisper(real.sample_jac, real.sample_data.jac$trt.ds)
plot(prok.real.beta.trt.ds.jac)
anova(prok.real.beta.trt.ds.jac, permutations = 9999)
permutest(prok.real.beta.trt.ds.jac, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.ds.jac)
plot(TukeyHSD(prok.real.beta.trt.ds.jac), las = 1)

##Cultivar:Developmental Stage
prok.real.beta.cult.ds.jac <- betadisper(real.sample_jac, real.sample_data.jac$cult.ds)
plot(prok.real.beta.cult.ds.jac)
anova(prok.real.beta.cult.ds.jac, permutations = 9999)
permutest(prok.real.beta.cult.ds.jac, permutations = 9999, pairwise = T)
scores(prok.real.beta.cult.ds.jac)
plot(TukeyHSD(prok.real.beta.cult.ds.jac), las = 1)
```

```{r}
##18S-ITS
##ASV level
##PERMANOVA
##Jaccard

##Make a data frame from the sample_data
fun.real.sample_data.jac <- data.frame(sample_data(fun.pruned_norm_real.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
fun.real.sample_jac <- phyloseq::distance(fun.pruned_norm_real.ASV, method = "jaccard")


# Set up the formula with interactions for adonis2
fun.real.formula.jac <- fun.real.sample_jac ~ (Treatment + Cultivar + Developmental_Stage)^2 

# Perform PERMANOVA analysis
fun.real.permanova_result.jac <- adonis2(fun.real.formula.jac, data = fun.real.sample_data.jac, strata = fun.real.sample_data.jac$Location, permutations = 9999)

###add corrected p-value
fun.real.permanova_result.jac$qval <- p.adjust(fun.real.permanova_result.jac$`Pr(>F)`, "fdr")

# Print and export the result
print(fun.real.permanova_result.jac)
write.csv(fun.real.permanova_result.jac, "fun.real.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
fun.real.sample_data.jac$trt.cult <- paste(fun.real.sample_data.jac$Treatment, fun.real.sample_data.jac$Cultivar)
fun.real.sample_data.jac$trt.ds <- paste(fun.real.sample_data.jac$Treatment, fun.real.sample_data.jac$Developmental_Stage)
fun.real.sample_data.jac$cult.ds <- paste(fun.real.sample_data.jac$Cultivar, fun.real.sample_data.jac$Developmental_Stage)

##Treatment
fun.real.beta.trt.jac <- betadisper(fun.real.sample_jac, fun.real.sample_data.jac$Treatment)
plot(fun.real.beta.trt.jac)
anova(fun.real.beta.trt.jac, permutations = 9999)
fun.real.beta.trt.pt.jac <- permutest(fun.real.beta.trt.jac, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.jac)
plot(TukeyHSD(fun.real.beta.trt.jac), las = 1)

##Cultivar
fun.real.beta.cult.jac <- betadisper(fun.real.sample_jac, fun.real.sample_data.jac$Cultivar)
plot(fun.real.beta.cult.jac)
anova(fun.real.beta.cult.jac, permutations = 9999)
fun.real.beta.cult.pt.jac <- permutest(fun.real.beta.cult.jac, permutations = 9999, pairwise = T)
scores(fun.real.beta.cult.jac)
plot(TukeyHSD(fun.real.beta.cult.jac), las = 1)

##Developmental stage
fun.real.beta.ds.jac <- betadisper(fun.real.sample_jac, fun.real.sample_data.jac$Developmental_Stage)
plot(fun.real.beta.ds.jac)
anova(fun.real.beta.ds.jac, permutations = 9999)
permutest(fun.real.beta.ds.jac, permutations = 9999, pairwise = T)
scores(fun.real.beta.ds.jac)
plot(TukeyHSD(fun.real.beta.ds.jac), las = 1)

##Treatment:Cultivar
fun.real.beta.trt.cult.jac <- betadisper(fun.real.sample_jac, fun.real.sample_data.jac$trt.cult)
plot(fun.real.beta.trt.cult.jac)
anova(fun.real.beta.trt.cult.jac, permutations = 9999)
permutest(fun.real.beta.trt.cult.jac, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.cult.jac)
plot(TukeyHSD(fun.real.beta.trt.cult.jac), las = 1)

##Treatment:Developmental Stage
fun.real.beta.trt.ds.jac <- betadisper(fun.real.sample_jac, fun.real.sample_data.jac$trt.ds)
plot(fun.real.beta.trt.ds.jac)
anova(fun.real.beta.trt.ds.jac, permutations = 9999)
permutest(fun.real.beta.trt.ds.jac, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.ds.jac)
plot(TukeyHSD(fun.real.beta.trt.ds.jac), las = 1)

##Cultivar:Developmental Stage
fun.real.beta.cult.ds.jac <- betadisper(fun.real.sample_jac, fun.real.sample_data.jac$cult.ds)
plot(fun.real.beta.cult.ds.jac)
anova(fun.real.beta.cult.ds.jac, permutations = 9999)
permutest(fun.real.beta.cult.ds.jac, permutations = 9999, pairwise = T)
scores(fun.real.beta.cult.ds.jac)
plot(TukeyHSD(fun.real.beta.cult.ds.jac), las = 1)
```

```{r}
##16S
##ASV level
##PERMANOVA
##Euclidean

##Make a data frame from the sample_data
real.sample_data.euc <- data.frame(sample_data(pruned_norm_real.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
real.sample_euc <- phyloseq::distance(pruned_norm_real.ASV, method = "euclidean")

# Set up the formula with interactions for adonis2
real.formula.euc <- real.sample_euc ~ (Treatment + Cultivar + Developmental_Stage)^2 

# Perform PERMANOVA analysis
real.permanova_result.euc <- adonis2(real.formula.euc, data = real.sample_data.euc, strata = real.sample_data.euc$Location, permutations = 9999)

###add corrected p-value
real.permanova_result.euc$qval <- p.adjust(real.permanova_result.euc$`Pr(>F)`, "fdr")

# Print and export the result
print(real.permanova_result.euc)
write.csv(real.permanova_result.euc, "prok.real.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
real.sample_data.euc$trt.cult <- paste(real.sample_data.euc$Treatment, real.sample_data.euc$Cultivar)
real.sample_data.euc$trt.ds <- paste(real.sample_data.euc$Treatment, real.sample_data.euc$Developmental_Stage)
real.sample_data.euc$cult.ds <- paste(real.sample_data.euc$Cultivar, real.sample_data.euc$Developmental_Stage)

##Treatment
prok.real.beta.trt.euc <- betadisper(real.sample_euc, real.sample_data.euc$Treatment)
plot(prok.real.beta.trt.euc)
anova(prok.real.beta.trt.euc, permutations = 9999)
prok.real.beta.trt.pt.euc <- permutest(prok.real.beta.trt.euc, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.euc)
plot(TukeyHSD(prok.real.beta.trt.euc), las = 1)

##Cultivar
prok.real.beta.cult.euc <- betadisper(real.sample_euc, real.sample_data.euc$Cultivar)
plot(prok.real.beta.cult.euc)
anova(prok.real.beta.cult.euc, permutations = 9999)
set.seed(2)
prok.real.beta.cult.pt.euc <- permutest(prok.real.beta.cult.euc, permutations = 9999, pairwise = T)
scores(prok.real.beta.cult.euc)
plot(TukeyHSD(prok.real.beta.cult.euc), las = 1)

##Developmental stage
prok.real.beta.ds.euc <- betadisper(real.sample_euc, real.sample_data.euc$Developmental_Stage)
plot(prok.real.beta.ds.euc)
anova(prok.real.beta.ds.euc, permutations = 9999)
permutest(prok.real.beta.ds.euc, permutations = 9999, pairwise = T)
scores(prok.real.beta.ds.euc)
plot(TukeyHSD(prok.real.beta.ds.euc), las = 1)

##Treatment:Cultivar
prok.real.beta.trt.cult.euc <- betadisper(real.sample_euc, real.sample_data.euc$trt.cult)
plot(prok.real.beta.trt.cult.euc)
anova(prok.real.beta.trt.cult.euc, permutations = 9999)
permutest(prok.real.beta.trt.cult.euc, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.cult.euc)
plot(TukeyHSD(prok.real.beta.trt.cult.euc), las = 1)

##Treatment:Developmental Stage
prok.real.beta.trt.ds.euc <- betadisper(real.sample_euc, real.sample_data.euc$trt.ds)
plot(prok.real.beta.trt.ds.euc)
anova(prok.real.beta.trt.ds.euc, permutations = 9999)
permutest(prok.real.beta.trt.ds.euc, permutations = 9999, pairwise = T)
scores(prok.real.beta.trt.ds.euc)
plot(TukeyHSD(prok.real.beta.trt.ds.euc), las = 1)

##Cultivar:Developmental Stage
prok.real.beta.cult.ds.euc <- betadisper(real.sample_euc, real.sample_data.euc$cult.ds)
plot(prok.real.beta.cult.ds.euc)
anova(prok.real.beta.cult.ds.euc, permutations = 9999)
permutest(prok.real.beta.cult.ds.euc, permutations = 9999, pairwise = T)
scores(prok.real.beta.cult.ds.euc)
plot(TukeyHSD(prok.real.beta.cult.ds.euc), las = 1)
```

```{r}
##18S-ITS
##ASV level
##PERMANOVA
##Euclidean

##Make a data frame from the sample_data
fun.real.sample_data.euc <- data.frame(sample_data(fun.pruned_norm_real.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
fun.real.sample_euc <- phyloseq::distance(fun.pruned_norm_real.ASV, method = "euclidean")


# Set up the formula with interactions for adonis2
fun.real.formula.euc <- fun.real.sample_euc ~ (Treatment + Cultivar + Developmental_Stage)^2 

# Perform PERMANOVA analysis
fun.real.permanova_result.euc <- adonis2(fun.real.formula.euc, data = fun.real.sample_data.euc, strata = fun.real.sample_data.euc$Location, permutations = 9999)

###add corrected p-value
fun.real.permanova_result.euc$qval <- p.adjust(fun.real.permanova_result.euc$`Pr(>F)`, "fdr")

# Print and export the result
print(fun.real.permanova_result.euc)
write.csv(fun.real.permanova_result.euc, "fun.real.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
fun.real.sample_data.euc$trt.cult <- paste(fun.real.sample_data.euc$Treatment, fun.real.sample_data.euc$Cultivar)
fun.real.sample_data.euc$trt.ds <- paste(fun.real.sample_data.euc$Treatment, fun.real.sample_data.euc$Developmental_Stage)
fun.real.sample_data.euc$cult.ds <- paste(fun.real.sample_data.euc$Cultivar, fun.real.sample_data.euc$Developmental_Stage)

##Treatment
fun.real.beta.trt.euc <- betadisper(fun.real.sample_euc, fun.real.sample_data.euc$Treatment)
plot(fun.real.beta.trt.euc)
anova(fun.real.beta.trt.euc, permutations = 9999)
fun.real.beta.trt.pt.euc <- permutest(fun.real.beta.trt.euc, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.euc)
plot(TukeyHSD(fun.real.beta.trt.euc), las = 1)

##Cultivar
fun.real.beta.cult.euc <- betadisper(fun.real.sample_euc, fun.real.sample_data.euc$Cultivar)
plot(fun.real.beta.cult.euc)
anova(fun.real.beta.cult.euc, permutations = 9999)
set.seed(2)
fun.real.beta.cult.pt.euc <- permutest(fun.real.beta.cult.euc, permutations = 9999, pairwise = T)
scores(fun.real.beta.cult.euc)
plot(TukeyHSD(fun.real.beta.cult.euc), las = 1)

##Developmental stage
fun.real.beta.ds.euc <- betadisper(fun.real.sample_euc, fun.real.sample_data.euc$Developmental_Stage)
plot(fun.real.beta.ds.euc)
anova(fun.real.beta.ds.euc, permutations = 9999)
permutest(fun.real.beta.ds.euc, permutations = 9999, pairwise = T)
scores(fun.real.beta.ds.euc)
plot(TukeyHSD(fun.real.beta.ds.euc), las = 1)

##Treatment:Cultivar
fun.real.beta.trt.cult.euc <- betadisper(fun.real.sample_euc, fun.real.sample_data.euc$trt.cult)
plot(fun.real.beta.trt.cult.euc)
anova(fun.real.beta.trt.cult.euc, permutations = 9999)
permutest(fun.real.beta.trt.cult.euc, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.cult.euc)
plot(TukeyHSD(fun.real.beta.trt.cult.euc), las = 1)

##Treatment:Developmental Stage
fun.real.beta.trt.ds.euc <- betadisper(fun.real.sample_euc, fun.real.sample_data.euc$trt.ds)
plot(fun.real.beta.trt.ds.euc)
anova(fun.real.beta.trt.ds.euc, permutations = 9999)
permutest(fun.real.beta.trt.ds.euc, permutations = 9999, pairwise = T)
scores(fun.real.beta.trt.ds.euc)
plot(TukeyHSD(fun.real.beta.trt.ds.euc), las = 1)

##Cultivar:Developmental Stage
fun.real.beta.cult.ds.euc <- betadisper(fun.real.sample_euc, fun.real.sample_data.euc$cult.ds)
plot(fun.real.beta.cult.ds.euc)
anova(fun.real.beta.cult.ds.euc, permutations = 9999)
permutest(fun.real.beta.cult.ds.euc, permutations = 9999, pairwise = T)
scores(fun.real.beta.cult.ds.euc)
plot(TukeyHSD(fun.real.beta.cult.ds.euc), las = 1)
```

```{r}
##16S
##ASV level
##PERMANOVA
##Baseline
##Bray

##Make a data frame from the sample_data
baseline.sample_data.ASV <- data.frame(sample_data(pruned_norm_baseline.ASV))  # Convert sample metadata to a data frame

##Calculate bray curtis distance matrix
baseline.sample_bray.ASV <- phyloseq::distance(pruned_norm_baseline.ASV, method = "bray")

# Set up the formula with interactions for adonis2
baseline.formula.ASV <- baseline.sample_bray.ASV ~ (Treatment + Cultivar)^2 

# Perform PERMANOVA analysis
baseline.permanova_result.ASV <- adonis2(baseline.formula.ASV, data = baseline.sample_data.ASV, strata = baseline.sample_data.ASV$Location, permutations = 9999)

###add corrected p-value
baseline.permanova_result.ASV$qval <- p.adjust(baseline.permanova_result.ASV$`Pr(>F)`, "fdr")

# Print and export the result
print(baseline.permanova_result.ASV)
write.csv(baseline.permanova_result.ASV, "prok.baseline.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
baseline.sample_data.ASV$trt.cult <- paste(baseline.sample_data.ASV$Treatment, baseline.sample_data.ASV$Cultivar)

##Treatment
prok.baseline.beta.trt.ASV <- betadisper(baseline.sample_bray.ASV, baseline.sample_data.ASV$Treatment)
plot(prok.baseline.beta.trt.ASV)
anova(prok.baseline.beta.trt.ASV, permutations = 9999)
prok.baseline.beta.trt.pt.ASV <- permutest(prok.baseline.beta.trt.ASV, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.trt.ASV)
plot(TukeyHSD(prok.baseline.beta.trt.ASV), las = 1)

##Cultivar
prok.baseline.beta.cult.ASV <- betadisper(baseline.sample_bray.ASV, baseline.sample_data.ASV$Cultivar)
plot(prok.baseline.beta.cult.ASV)
anova(prok.baseline.beta.cult.ASV, permutations = 9999)
prok.baseline.beta.cult.pt.ASV <- permutest(prok.baseline.beta.cult.ASV, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.cult.ASV)
plot(TukeyHSD(prok.baseline.beta.cult.ASV), las = 1)

##Treatment:Cultivar
prok.baseline.beta.trt.cult.ASV <- betadisper(baseline.sample_bray.ASV, baseline.sample_data.ASV$trt.cult)
plot(prok.baseline.beta.trt.cult.ASV)
anova(prok.baseline.beta.trt.cult.ASV, permutations = 9999)
permutest(prok.baseline.beta.trt.cult.ASV, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.trt.cult.ASV)
plot(TukeyHSD(prok.baseline.beta.trt.cult.ASV), las = 1)
```

```{r}
##18S-ITS
##ASV level
##PERMANOVA
##Baseline
##Bray

##Make a data frame from the sample_data
fun.baseline.sample_data.ASV <- data.frame(sample_data(fun.pruned_norm_baseline.ASV))  # Convert sample metadata to a data frame

##Calculate bray curtis distance matrix
fun.baseline.sample_bray.ASV <- phyloseq::distance(fun.pruned_norm_baseline.ASV, method = "bray")

# Set up the formula with interactions for adonis2
fun.baseline.formula.ASV <- fun.baseline.sample_bray.ASV ~ (Treatment + Cultivar)^2 

# Perform PERMANOVA analysis
fun.baseline.permanova_result.ASV <- adonis2(fun.baseline.formula.ASV, data = fun.baseline.sample_data.ASV, strata = fun.baseline.sample_data.ASV$Location, permutations = 9999)

###add corrected p-value
fun.baseline.permanova_result.ASV$qval <- p.adjust(fun.baseline.permanova_result.ASV$`Pr(>F)`, "fdr")

# Print and export the result
print(fun.baseline.permanova_result.ASV)
write.csv(fun.baseline.permanova_result.ASV, "fun.prok.baseline.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
fun.baseline.sample_data.ASV$trt.cult <- paste(fun.baseline.sample_data.ASV$Treatment, fun.baseline.sample_data.ASV$Cultivar)

##Treatment
fun.baseline.beta.trt.ASV <- betadisper(fun.baseline.sample_bray.ASV, fun.baseline.sample_data.ASV$Treatment)
plot(fun.baseline.beta.trt.ASV)
anova(fun.baseline.beta.trt.ASV, permutations = 9999)
fun.baseline.beta.trt.pt.ASV <- permutest(fun.baseline.beta.trt.ASV, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.trt.ASV)
plot(TukeyHSD(fun.baseline.beta.trt.ASV), las = 1)

##Cultivar
fun.baseline.beta.cult.ASV <- betadisper(fun.baseline.sample_bray.ASV, fun.baseline.sample_data.ASV$Cultivar)
plot(fun.baseline.beta.cult.ASV)
anova(fun.baseline.beta.cult.ASV, permutations = 9999)
fun.baseline.beta.cult.pt.ASV <- permutest(fun.baseline.beta.cult.ASV, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.cult.ASV)
plot(TukeyHSD(fun.baseline.beta.cult.ASV), las = 1)

##Treatment:Cultivar
fun.baseline.beta.trt.cult.ASV <- betadisper(fun.baseline.sample_bray.ASV, fun.baseline.sample_data.ASV$trt.cult)
plot(fun.baseline.beta.trt.cult.ASV)
anova(fun.baseline.beta.trt.cult.ASV, permutations = 9999)
permutest(fun.baseline.beta.trt.cult.ASV, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.trt.cult.ASV)
plot(TukeyHSD(fun.baseline.beta.trt.cult.ASV), las = 1)
```

```{r}
##16S
##ASV level
##PERMANOVA
##Baseline
##Jaccard

baseline.sample_data.jac <- data.frame(sample_data(pruned_norm_baseline.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
baseline.sample_jac <- phyloseq::distance(pruned_norm_baseline.ASV, method = "jaccard")


# Set up the formula with interactions for adonis2
baseline.formula.jac <- baseline.sample_jac ~ (Treatment + Cultivar)^2 

# Perform PERMANOVA analysis
baseline.permanova_result.jac <- adonis2(baseline.formula.jac, data = baseline.sample_data.jac, strata = baseline.sample_data.jac$Location, permutations = 9999)

###add corrected p-value
baseline.permanova_result.jac$qval <- p.adjust(baseline.permanova_result.jac$`Pr(>F)`, "fdr")

# Print and export the result
print(baseline.permanova_result.jac)
write.csv(baseline.permanova_result.jac, "prok.baseline.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
baseline.sample_data.jac$trt.cult <- paste(baseline.sample_data.jac$Treatment, baseline.sample_data.jac$Cultivar)

##Treatment
prok.baseline.beta.trt.jac <- betadisper(baseline.sample_jac, baseline.sample_data.jac$Treatment)
plot(prok.baseline.beta.trt.jac)
anova(prok.baseline.beta.trt.jac, permutations = 9999)
prok.baseline.beta.trt.pt.jac <- permutest(prok.baseline.beta.trt.jac, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.trt.jac)
plot(TukeyHSD(prok.baseline.beta.trt.jac), las = 1)

##Cultivar
prok.baseline.beta.cult.jac <- betadisper(baseline.sample_jac, baseline.sample_data.jac$Cultivar)
plot(prok.baseline.beta.cult.jac)
anova(prok.baseline.beta.cult.jac, permutations = 9999)
prok.baseline.beta.cult.pt.jac <- permutest(prok.baseline.beta.cult.jac, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.cult.jac)
plot(TukeyHSD(prok.baseline.beta.cult.jac), las = 1)

##Treatment:Cultivar
prok.baseline.beta.trt.cult.jac <- betadisper(baseline.sample_jac, baseline.sample_data.jac$trt.cult)
plot(prok.baseline.beta.trt.cult.jac)
anova(prok.baseline.beta.trt.cult.jac, permutations = 9999)
permutest(prok.baseline.beta.trt.cult.jac, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.trt.cult.jac)
plot(TukeyHSD(prok.baseline.beta.trt.cult.jac), las = 1)
```

```{r}
##18S-ITS
##ASV level
##PERMANOVA
##Baseline
##Jaccard

fun.baseline.sample_data.jac <- data.frame(sample_data(fun.pruned_norm_baseline.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
fun.baseline.sample_jac <- phyloseq::distance(fun.pruned_norm_baseline.ASV, method = "jaccard")

# Set up the formula with interactions for adonis2
fun.baseline.formula.jac <- fun.baseline.sample_jac ~ (Treatment + Cultivar)^2 

# Perform PERMANOVA analysis
fun.baseline.permanova_result.jac <- adonis2(fun.baseline.formula.jac, data = fun.baseline.sample_data.jac, strata = fun.baseline.sample_data.jac$Location, permutations = 9999)

###add corrected p-value
fun.baseline.permanova_result.jac$qval <- p.adjust(fun.baseline.permanova_result.jac$`Pr(>F)`, "fdr")

# Print and export the result
print(fun.baseline.permanova_result.jac)
write.csv(fun.baseline.permanova_result.jac, "fun.baseline.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
fun.baseline.sample_data.jac$trt.cult <- paste(fun.baseline.sample_data.jac$Treatment, fun.baseline.sample_data.jac$Cultivar)

##Treatment
fun.baseline.beta.trt.jac <- betadisper(fun.baseline.sample_jac, fun.baseline.sample_data.jac$Treatment)
plot(fun.baseline.beta.trt.jac)
anova(fun.baseline.beta.trt.jac, permutations = 9999)
fun.baseline.beta.trt.pt.jac <- permutest(fun.baseline.beta.trt.jac, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.trt.jac)
plot(TukeyHSD(fun.baseline.beta.trt.jac), las = 1)

##Cultivar
fun.baseline.beta.cult.jac <- betadisper(fun.baseline.sample_jac, fun.baseline.sample_data.jac$Cultivar)
plot(fun.baseline.beta.cult.jac)
anova(fun.baseline.beta.cult.jac, permutations = 9999)
fun.baseline.beta.cult.pt.jac <- permutest(fun.baseline.beta.cult.jac, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.cult.jac)
plot(TukeyHSD(fun.baseline.beta.cult.jac), las = 1)

##Treatment:Cultivar
fun.baseline.beta.trt.cult.jac <- betadisper(fun.baseline.sample_jac, fun.baseline.sample_data.jac$trt.cult)
plot(fun.baseline.beta.trt.cult.jac)
anova(fun.baseline.beta.trt.cult.jac, permutations = 9999)
permutest(fun.baseline.beta.trt.cult.jac, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.trt.cult.jac)
plot(TukeyHSD(fun.baseline.beta.trt.cult.jac), las = 1)
```

```{r}
##16S
##ASV level
##PERMANOVA
##Baseline
##Euclidean

##Make a data frame from the sample_data
baseline.sample_data.euc <- data.frame(sample_data(pruned_norm_baseline.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
baseline.sample_euc <- phyloseq::distance(pruned_norm_baseline.ASV, method = "euclidean")

# Set up the formula with interactions for adonis2
baseline.formula.euc <- baseline.sample_euc ~ (Treatment + Cultivar)^2 

# Perform PERMANOVA analysis
baseline.permanova_result.euc <- adonis2(baseline.formula.euc, data = baseline.sample_data.euc, strata = baseline.sample_data.euc$Location, permutations = 9999)

###add corrected p-value
baseline.permanova_result.euc$qval <- p.adjust(baseline.permanova_result.euc$`Pr(>F)`, "fdr")

# Print and export the result
print(baseline.permanova_result.euc)
write.csv(baseline.permanova_result.euc, "prok.baseline.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
baseline.sample_data.euc$trt.cult <- paste(baseline.sample_data.euc$Treatment, baseline.sample_data.euc$Cultivar)

##Treatment
prok.baseline.beta.trt.euc <- betadisper(baseline.sample_euc, baseline.sample_data.euc$Treatment)
plot(prok.baseline.beta.trt.euc)
anova(prok.baseline.beta.trt.euc, permutations = 9999)
prok.baseline.beta.trt.pt.euc <- permutest(prok.baseline.beta.trt.euc, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.trt.euc)
plot(TukeyHSD(prok.baseline.beta.trt.euc), las = 1)

##Cultivar
prok.baseline.beta.cult.euc <- betadisper(baseline.sample_euc, baseline.sample_data.euc$Cultivar)
plot(prok.baseline.beta.cult.euc)
anova(prok.baseline.beta.cult.euc, permutations = 9999)
set.seed(2)
prok.baseline.beta.cult.pt.euc <- permutest(prok.baseline.beta.cult.euc, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.cult.euc)
plot(TukeyHSD(prok.baseline.beta.cult.euc), las = 1)

##Treatment:Cultivar
prok.baseline.beta.trt.cult.euc <- betadisper(baseline.sample_euc, baseline.sample_data.euc$trt.cult)
plot(prok.baseline.beta.trt.cult.euc)
anova(prok.baseline.beta.trt.cult.euc, permutations = 9999)
permutest(prok.baseline.beta.trt.cult.euc, permutations = 9999, pairwise = T)
scores(prok.baseline.beta.trt.cult.euc)
plot(TukeyHSD(prok.baseline.beta.trt.cult.euc), las = 1)
```

```{r}
##18S-ITS
##ASV level
##PERMANOVA
##Baseline
##Euclidean

##Make a data frame from the sample_data
fun.baseline.sample_data.euc <- data.frame(sample_data(fun.pruned_norm_baseline.ASV))  # Convert sample metadata to a data frame

##Calculate distance matrix
fun.baseline.sample_euc <- phyloseq::distance(fun.pruned_norm_baseline.ASV, method = "euclidean")

# Set up the formula with interactions for adonis2
fun.baseline.formula.euc <- fun.baseline.sample_euc ~ (Treatment + Cultivar)^2 

# Perform PERMANOVA analysis
fun.baseline.permanova_result.euc <- adonis2(fun.baseline.formula.euc, data = fun.baseline.sample_data.euc, strata = fun.baseline.sample_data.euc$Location, permutations = 9999)

###add corrected p-value
fun.baseline.permanova_result.euc$qval <- p.adjust(fun.baseline.permanova_result.euc$`Pr(>F)`, "fdr")

# Print and export the result
print(fun.baseline.permanova_result.euc)
write.csv(fun.baseline.permanova_result.euc, "fun.baseline.permanova.csv", row.names = T)

##Homogeneity of dispersion test
##Add columns for interactions
fun.baseline.sample_data.euc$trt.cult <- paste(fun.baseline.sample_data.euc$Treatment, fun.baseline.sample_data.euc$Cultivar)

##Treatment
fun.baseline.beta.trt.euc <- betadisper(fun.baseline.sample_euc, fun.baseline.sample_data.euc$Treatment)
plot(fun.baseline.beta.trt.euc)
anova(fun.baseline.beta.trt.euc, permutations = 9999)
fun.baseline.beta.trt.pt.euc <- permutest(fun.baseline.beta.trt.euc, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.trt.euc)
plot(TukeyHSD(fun.baseline.beta.trt.euc), las = 1)

##Cultivar
fun.baseline.beta.cult.euc <- betadisper(fun.baseline.sample_euc, fun.baseline.sample_data.euc$Cultivar)
plot(fun.baseline.beta.cult.euc)
anova(fun.baseline.beta.cult.euc, permutations = 9999)
set.seed(2)
fun.baseline.beta.cult.pt.euc <- permutest(fun.baseline.beta.cult.euc, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.cult.euc)
plot(TukeyHSD(fun.baseline.beta.cult.euc), las = 1)

##Treatment:Cultivar
fun.baseline.beta.trt.cult.euc <- betadisper(fun.baseline.sample_euc, fun.baseline.sample_data.euc$trt.cult)
plot(fun.baseline.beta.trt.cult.euc)
anova(fun.baseline.beta.trt.cult.euc, permutations = 9999)
permutest(fun.baseline.beta.trt.cult.euc, permutations = 9999, pairwise = T)
scores(fun.baseline.beta.trt.cult.euc)
plot(TukeyHSD(fun.baseline.beta.trt.cult.euc), las = 1)
```

```{r}
##16S
##Obtain R2 from Bray-Curtis PERMANOVA for figure
# Create a new dataframe 'prok.var.ASV' from 'real.permanova_result.ASV'
prok.var.ASV <- real.permanova_result.ASV

# Make the rownames a column called 'effect'
prok.var.ASV$effect <- rownames(prok.var.ASV)

# Remove rows with 'Residual' and 'Total' in the 'effect' column
prok.var.ASV <- prok.var.ASV[!(prok.var.ASV$effect %in% c('Residual', 'Total')), ]

# Replace "Developmental_Stage" with "Growth Stage" in the 'effect' column
prok.var.ASV$effect <- gsub("Developmental_Stage", "Growth Stage", prok.var.ASV$effect)

# Create a new column 'variance' with values from the 'R2' column multiplied by 100
prok.var.ASV$variance <- prok.var.ASV$R2 * 100

ggplot(prok.var.ASV, aes(x = reorder(effect, variance), y = variance)) +
  geom_bar(stat = "identity", fill = "#a890a8", color = "black", size = 0.5, alpha = 0.7) +
  coord_flip() +
  geom_text(aes(label = effect), color = "black", size = 4.5, fontface = "bold", position=position_stack(vjust=0.75)) +
  labs(x = NULL, y = "Variance Explained (%)") +
  theme_bw() +
  theme(
    axis.text.y = element_blank(),
    axis.text.x = element_text(size = 16, color = "black", hjust = 0),  # Adjust hjust here
    axis.title.x = element_text(size = 18, face = "bold"),
    axis.ticks = element_blank()) +
  expand_limits(y = 6.1)

##as point
# Create size_category as a factor
prok.var.ASV$size_category <- cut(prok.var.ASV$`Pr(>F)`, 
                             breaks=c(-Inf, 0.01, 0.05, Inf), 
                             labels=c(7, 10, 13))

# Ensure size_category has all levels even if they are not present in the data
prok.var.ASV$size_category <- factor(prok.var.ASV$size_category, levels = c(13, 10, 7)) #7, 10, 13

# Create the plot
 ggplot(prok.var.ASV, aes(x = reorder(effect, R2), y = R2)) +
  geom_point(aes(size = size_category), color = "black", fill = "#a8c0a8", pch = 21) +
  coord_flip() +
  labs(x = NULL, y = expression(bold(R^"2"))) + 
  theme_bw() +
  theme(
    axis.text.x = element_text(size = 12, color = "black", hjust = 0),
    axis.title.x = element_text(size = 18, face = "bold"),
    axis.text.y = element_text(size = 12, color = 'black', face = 'bold'),
    axis.ticks = element_blank(),
    legend.position = c(0.95, 0.01),
    legend.justification = c(1, 0),
    legend.background = element_blank(),
    legend.box.background = element_blank(),
    legend.title = element_markdown()) +
  scale_size_discrete(name = "<b><i>p</i>-value</b>",
                      breaks=c(7, 10, 13), #13, 10, 7
                      labels=c("< 0.01", "< 0.05", "> 0.05"),
                      drop = FALSE,
                      range = c(5, 9))

ggsave("prok.variance.explained.2.tiff", width = 5.5, height = 4, units = "in", dpi = 900)
```

```{r}
##18S-ITS
##ASV level
##Obtain R2 from Bray-Curtis PERMANOVA for figure
# Create a new dataframe 'fun.var.ASV' from 'fun.real.permanova_result'
fun.var.ASV <- fun.real.permanova_result.ASV

# Make the rownames a column called 'effect'
fun.var.ASV$effect <- rownames(fun.var.ASV)

# Remove rows with 'Residual' and 'Total' in the 'effect' column
fun.var.ASV <- fun.var.ASV[!(fun.var.ASV$effect %in% c('Residual', 'Total')), ]

# Replace "Developmental_Stage" with "Growth Stage" in the 'effect' column
fun.var.ASV$effect <- gsub("Developmental_Stage", "Growth Stage", fun.var.ASV$effect)

# Create a new column 'variance' with values from the 'R2' column multiplied by 100
fun.var.ASV$variance <- fun.var.ASV$R2 * 100


ggplot(fun.var.ASV, aes(x = reorder(effect, variance), y = variance)) +
  geom_bar(stat = "identity", fill = "#a8c0a8", color = "black", size = 0.5, alpha = 0.7) +
  coord_flip() +
  #geom_text(aes(label = effect), color = "black", size = 4.5, fontface = "bold", position=position_stack(vjust=0.72)) +
  labs(x = NULL, y = "Variance Explained (%)") +
  theme_bw() +
  theme(
    axis.text.y = element_blank(),
    axis.text.x = element_text(size = 16, color = "black", hjust = 0),  # Adjust hjust here
    axis.title.x = element_text(size = 18, face = "bold"),
    axis.ticks = element_blank())

ggsave("fun.variance.explained.ASV.tiff", width = 3, height = 7, units = "in", dpi = 900)
```

```{r}
##16S
##Simper analysis
##We need to remake bray-curtis matrices with vegan
##This will store the sample metadata as well, 
##Which is needed for the simper analysis
##pruned_norm_real.ASV
##Can use this metadata- real.sample_data.ASV

prok.real.otu.df <- as.data.frame(t(otu_table((pruned_norm_real.ASV))))
prok.real.tax.df <- as.data.frame(tax_table(pruned_norm_real.ASV))

treatment <- real.sample_data.ASV$Treatment
cultivar <- real.sample_data.ASV$Cultivar
ds <- real.sample_data.ASV$Developmental_Stage

simper.prok.trt  <- simper(prok.real.otu.df, treatment, distance = "bray", permutations = 9999)
print(simper.prok.trt)
contributing.taxa.prok.trt <- as.data.frame(simper.prok.trt$Control_Biostimulant)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.prok.trt <- contributing.taxa.prok.trt[order(-contributing.taxa.prok.trt$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.prok.trt$taxa <- rownames(sort.contributing.taxa.prok.trt)
#prok.real.tax.df$taxa <- rownames(prok.real.tax.df)
# Merge the two data frames by the "taxa" column
simper.prok.trt.tax <- merge(sort.contributing.taxa.prok.trt, prok.real.tax.df, by = "taxa")
view(simper.prok.trt.tax)
simper.prok.trt.tax$comp <- 'Biostimulant vs Control'

##Cultivar
simper.prok.cult  <- simper(prok.real.otu.df, cultivar, distance = "bray", permutations = 9999)
contributing.taxa.prok.cult <- as.data.frame(simper.prok.cult$CZ4979X_CZ4810X)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.prok.cult <- contributing.taxa.prok.cult[order(-contributing.taxa.prok.cult$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.prok.cult$taxa <- rownames(sort.contributing.taxa.prok.cult)
# Merge the two data frames by the "taxa" column
simper.prok.cult.tax <- merge(sort.contributing.taxa.prok.cult, prok.real.tax.df, by = "taxa")
view(simper.prok.cult.tax)
simper.prok.cult.tax$comp <- 'CZ4979X vs CZ4810X'

##Growth stage- V6 vs R2
simper.prok.ds  <- simper(prok.real.otu.df, ds, distance = "bray", permutations = 9999)
contributing.taxa.prok.ds.v6.r2 <- as.data.frame(simper.prok.ds$V6_R3)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.prok.ds.v6.r2 <- contributing.taxa.prok.ds.v6.r2[order(-contributing.taxa.prok.ds.v6.r2$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.prok.ds.v6.r2$taxa <- rownames(sort.contributing.taxa.prok.ds.v6.r2)
# Merge the two data frames by the "taxa" column
simper.prok.ds.v6.r2.tax <- merge(sort.contributing.taxa.prok.ds.v6.r2, prok.real.tax.df, by = "taxa")
view(simper.prok.ds.v6.r2.tax)
simper.prok.ds.v6.r2.tax$comp <- 'R2 vs V6'

##Growth stage- V6 vs R6
contributing.taxa.prok.ds.v6.r6 <- as.data.frame(simper.prok.ds$V6_R3)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.prok.ds.v6.r6 <- contributing.taxa.prok.ds.v6.r6[order(-contributing.taxa.prok.ds.v6.r6$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.prok.ds.v6.r6$taxa <- rownames(sort.contributing.taxa.prok.ds.v6.r6)
# Merge the two data frames by the "taxa" column
simper.prok.ds.v6.r6.tax <- merge(sort.contributing.taxa.prok.ds.v6.r6, prok.real.tax.df, by = "taxa")
view(simper.prok.ds.v6.r6.tax)
simper.prok.ds.v6.r6.tax$comp <- 'R6 vs V6'

##Growth stage- R2 vs R6
contributing.taxa.prok.ds.r3.r6 <- as.data.frame(simper.prok.ds$R3_R6)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.prok.ds.r3.r6 <- contributing.taxa.prok.ds.r3.r6[order(-contributing.taxa.prok.ds.r3.r6$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.prok.ds.r3.r6$taxa <- rownames(sort.contributing.taxa.prok.ds.r3.r6)
# Merge the two data frames by the "taxa" column
simper.prok.ds.r3.r6.tax <- merge(sort.contributing.taxa.prok.ds.r3.r6, prok.real.tax.df, by = "taxa")
view(simper.prok.ds.r3.r6.tax)
simper.prok.ds.r3.r6.tax$comp <- 'R6 vs R2'

simper.prok <- rbind(simper.prok.trt.tax, simper.prok.cult.tax, simper.prok.ds.v6.r2.tax, simper.prok.ds.v6.r6.tax, simper.prok.ds.r3.r6.tax)
view(simper.prok)
simper.prok$Domain <- 'Prokaryote'

simper.prok$q<- p.adjust(simper.prok$p, "fdr")

##trim
simper.prok.2 <- simper.prok[, c(3, 11, 18:21)]
view(simper.prok.2)
```

```{r}
##18S-ITS
##Simper analysis
##We need to remake bray-curtis matrices with vegan
##This will store the sample metadata as well, 
##Which is needed for the simper analysis
##fun.pruned_norm_real.ASV
##Can use this metadata- real.sample_data.ASV

fun.real.otu.df <- as.data.frame(t(otu_table((fun.pruned_norm_real.ASV))))
fun.real.tax.df <- as.data.frame(tax_table(fun.pruned_norm_real.ASV))

#treatment <- real.sample_data.ASV$Treatment
#cultivar <- real.sample_data.ASV$Cultivar
#ds <- real.sample_data.ASV$Developmental_Stage

simper.fun.trt  <- simper(fun.real.otu.df, treatment, distance = "bray", permutations = 9999)
#print(simper.fun.trt)
contributing.taxa.fun.trt <- as.data.frame(simper.fun.trt$Control_Biostimulant)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.fun.trt <- contributing.taxa.fun.trt[order(-contributing.taxa.fun.trt$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.fun.trt$taxa <- rownames(sort.contributing.taxa.fun.trt)
fun.real.tax.df$taxa <- rownames(fun.real.tax.df)
# Merge the two data frames by the "taxa" column
simper.fun.trt.tax <- merge(sort.contributing.taxa.fun.trt, fun.real.tax.df, by = "taxa")
view(simper.fun.trt.tax)
simper.fun.trt.tax$comp <- 'Biostimulant vs Control'

##Cultivar
simper.fun.cult  <- simper(fun.real.otu.df, cultivar, distance = "bray", permutations = 9999)
contributing.taxa.fun.cult <- as.data.frame(simper.fun.cult$CZ4979X_CZ4810X)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.fun.cult <- contributing.taxa.fun.cult[order(-contributing.taxa.fun.cult$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.fun.cult$taxa <- rownames(sort.contributing.taxa.fun.cult)
# Merge the two data frames by the "taxa" column
simper.fun.cult.tax <- merge(sort.contributing.taxa.fun.cult, fun.real.tax.df, by = "taxa")
view(simper.fun.cult.tax)
simper.fun.cult.tax$comp <- 'CZ4979X vs CZ4810X'

##Growth stage- V6 vs R2
simper.fun.ds  <- simper(fun.real.otu.df, ds, distance = "bray", permutations = 9999)
contributing.taxa.fun.ds.v6.r2 <- as.data.frame(simper.fun.ds$V6_R3)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.fun.ds.v6.r2 <- contributing.taxa.fun.ds.v6.r2[order(-contributing.taxa.fun.ds.v6.r2$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.fun.ds.v6.r2$taxa <- rownames(sort.contributing.taxa.fun.ds.v6.r2)
# Merge the two data frames by the "taxa" column
simper.fun.ds.v6.r2.tax <- merge(sort.contributing.taxa.fun.ds.v6.r2, fun.real.tax.df, by = "taxa")
view(simper.fun.ds.v6.r2.tax)
simper.fun.ds.v6.r2.tax$comp <- 'R2 vs V6'

##Growth stage- V6 vs R6
contributing.taxa.fun.ds.v6.r6 <- as.data.frame(simper.fun.ds$V6_R3)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.fun.ds.v6.r6 <- contributing.taxa.fun.ds.v6.r6[order(-contributing.taxa.fun.ds.v6.r6$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.fun.ds.v6.r6$taxa <- rownames(sort.contributing.taxa.fun.ds.v6.r6)
# Merge the two data frames by the "taxa" column
simper.fun.ds.v6.r6.tax <- merge(sort.contributing.taxa.fun.ds.v6.r6, fun.real.tax.df, by = "taxa")
view(simper.fun.ds.v6.r6.tax)
simper.fun.ds.v6.r6.tax$comp <- 'R6 vs V6'

##Growth stage- R2 vs R6
contributing.taxa.fun.ds.r3.r6 <- as.data.frame(simper.fun.ds$R3_R6)

##take the top 5 ASVs and get taxonomic classification
sort.contributing.taxa.fun.ds.r3.r6 <- contributing.taxa.fun.ds.r3.r6[order(-contributing.taxa.fun.ds.r3.r6$average)[1:5],]
# Convert rownames to a new column "taxa" for both data frames
sort.contributing.taxa.fun.ds.r3.r6$taxa <- rownames(sort.contributing.taxa.fun.ds.r3.r6)
# Merge the two data frames by the "taxa" column
simper.fun.ds.r3.r6.tax <- merge(sort.contributing.taxa.fun.ds.r3.r6, fun.real.tax.df, by = "taxa")
view(simper.fun.ds.r3.r6.tax)
simper.fun.ds.r3.r6.tax$comp <- 'R6 vs R2'

simper.fun <- rbind(simper.fun.trt.tax, simper.fun.cult.tax, simper.fun.ds.v6.r2.tax, simper.fun.ds.v6.r6.tax, simper.fun.ds.r3.r6.tax)
view(simper.fun)
simper.fun$Domain <- 'Eukaryote'

simper.fun$q<- p.adjust(simper.fun$p, "fdr")

##trim
simper.fun.2 <- simper.fun[, c(3, 11, 18:21)]

##combine with simper.prok.2
simper.combined <- rbind(simper.fun.2, simper.prok.2)
```

```{r}
##Create simper heatmap
##using simper.combined
simper.combined$per <- simper.combined$average * 100 ##get percentage

simp.heat <- simper.combined %>%
  select(comp, species.y, per) %>%
  pivot_wider(names_from = species.y, values_from = per)

simp.heat <- as.data.frame(simp.heat)
rownames(simp.heat) <- simp.heat$comp
simp.heat$comp <- NULL

##color
min_val <- min(simper.combined$per)
max_val <- max(simper.combined$per)
simper_palette <- colorRamp2(c(min_val, max_val), c("antiquewhite", "lightblue3"))

##legend
lgd_list.simper <- list(title_gp = gpar(fontsize = 12, fontface = 'bold'), 
        labels_gp = gpar(fontsize = 10), 
         title = "% Contribution", 
       border = "black", lwd = 1, direction = "horizontal", title_position = "topcenter",
       #at = c(0, 0.2, 0.4, 0.6, 0.8, 1), 
       legend_width = unit(10, "cm"),
       x = unit(0.5, "cm"), y = unit(0.5, "cm"),
      legend_height = unit(5, "cm"))

##bottom annotation
simp.bottom <- simper.combined[, c(3, 5)]
simp.bottom <- as.data.frame(simp.bottom)
simp.bottom <- simp.bottom[!duplicated(simp.bottom$species.y), ]
rownames(simp.bottom) <- simp.bottom$species.y
simp.bottom$species.y <- NULL

colours.simp <- list('Domain' = c('Prokaryote' = "#a8c0a8", 'Eukaryote' = "#a890a8"))

bottom.simp <- HeatmapAnnotation(df = simp.bottom,
  which = 'column',
  col = colours.simp,
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), simple_anno_size = unit(0.2, "cm"), show_legend = FALSE, show_annotation_name = FALSE)

simper.heatmap <- Heatmap(simp.heat, cluster_columns = FALSE, cluster_rows = FALSE,
        col = simper_palette,
        border = TRUE,
        column_names_gp = gpar(fontface = "italic", fontsize = 10),
        column_names_rot = 45,
        row_names_gp = gpar(fontface = 'bold'),
        rect_gp = gpar(col = "black"),
        heatmap_legend_param = lgd_list.simper,
        column_split = simp.bottom$Domain,
        column_title = NULL,
        bottom_annotation = bottom.simp)

simper.heatmap.2 <- draw(simper.heatmap, heatmap_legend_side = "top", padding = unit(c(0, 20, 5, 0), "mm"))

tiff("simp.heatmap.tiff", width = 7, height = 5, units = "in", res = 900)
simper.heatmap.2
dev.off() 
```

```{r}
##16S
##Let's deconstruct the phyloseq object for community membership analysis
##We must combine strain-level info to do analyses at species level
##will use prok_decontam_norm

## First, tax table
comm.prok.otu <- as.data.frame(tax_table(prok_decontam_norm))

# Store row names in a separate column
comm.prok.otu$rowname <- rownames(comm.prok.otu)

comm.prok.otu <- comm.prok.otu %>%
  # Retain only the first two words in 'species'
  mutate(species = gsub("^(\\w+ \\w+).*", "\\1", species)) %>%
  group_by(species) %>%
  summarise(rowname = first(rowname), across(everything(), ~first(.[!is.na(.)]))) %>%
  ungroup()

# Remove 'first_' prefix
names(comm.prok.otu) <- sub("^first_", "", names(comm.prok.otu))
comm.prok.otu <- as.data.frame(comm.prok.otu)
rownames(comm.prok.otu) <- comm.prok.otu$rowname
comm.prok.otu$rowname <- NULL

comm.prok.otu.2 <- as.data.frame(tax_table(prok_decontam_norm))
comm.prok.otu.2$asv <- rownames(comm.prok.otu.2)

comm.prok.tax <- as.data.frame(otu_table(prok_decontam_norm))
comm.prok.tax$asv <- rownames(comm.prok.tax)

# Store the row order from comm.prok.otu
order_otu <- rownames(comm.prok.otu)

comm.prok.tax.2 <- merge(comm.prok.otu.2, comm.prok.tax, by = c("asv"))
rownames(comm.prok.tax.2) <- comm.prok.tax.2$asv
comm.prok.tax.2$asv <- NULL

# Ensure the order is the same as order_otu before the collapse
comm.prok.tax.2 <- comm.prok.tax.2[order(match(rownames(comm.prok.tax.2), order_otu)), ]

# Remove first 6 columns
comm.prok.tax.2 <- comm.prok.tax.2[,-(1:6)]

# Store row names
comm.prok.tax.2$rowname <- rownames(comm.prok.tax.2)

comm.prok.tax.2 <- comm.prok.tax.2 %>%
  mutate(species = gsub("^(\\w+ \\w+).*", "\\1", species)) %>%
  group_by(species) %>%
  summarise(rowname = first(rowname), across(-rowname, sum, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(match(rowname, order_otu))

# Adjust row names
comm.prok.tax.2 <- as.data.frame(comm.prok.tax.2) 
rownames(comm.prok.tax.2) <- comm.prok.tax.2$rowname
comm.prok.tax.2$rowname <- NULL
comm.prok.tax.2$species <- NULL

##Reconstruct phyloseq object
comm.prok.otu <- as.matrix(comm.prok.otu)

OTU.collapse = otu_table(comm.prok.tax.2, taxa_are_rows = TRUE)
TAX.collapse = tax_table(comm.prok.otu)

########
prok_collapse <- phyloseq(OTU.collapse, TAX.collapse, samples.1)
#######
```

```{r}
##16S
##Differential abundance analysis; make sure to set normalization
##to none since our data is already normalized
##This is to find tax influenced by biostimulant application; will do separately for cultivar/developmental stage effects
##Used counts rather than relative abundance in order to account for zero inflation (ZINB can only be ran on counts; see link below)
##https://forum.biobakery.org/t/maaslin2-linear-model-all-significant-results-associated-with-one-factor-level/582/2
##BiocManager::install("Maaslin2")
##library(Maaslin2)
##https://forum.biobakery.org/t/maaslin2-vs-mmuphin/4466/2
##https://forum.biobakery.org/t/maaslin2-glm-fit-error/2571/5 
##https://forum.biobakery.org/t/choosing-analysis-method-for-maaslin2/2107 
##Use prok_collapse 

pruned_prok_decontam_mas <- prune_samples(sample_data(prok_collapse)$bio_or_control == "biological", prok_collapse)
pruned_norm_real_mas <- prune_samples(!sample_data(pruned_prok_decontam_mas)$Developmental_Stage == "V1", pruned_prok_decontam_mas)

##merge at species level
#pruned_norm_real_mas <- tax_glom(pruned_norm_real_mas, taxrank = 'species')

##Count df formatted for Maaslin- look only at timepoints after application (pruned_norm_real_mas)
mas_count_df <- as.data.frame(otu_table(pruned_norm_real_mas))
mas_count_df <- tibble::rownames_to_column(mas_count_df, "otu")
##Get taxa for merge
comm.prok.otu <- as.data.frame(comm.prok.otu)
comm.prok.maas <- comm.prok.otu
comm.prok.maas$otu <- rownames(comm.prok.maas)
mas_tax_df <- merge(mas_count_df, comm.prok.maas, by = c("otu"))

##filter out unmapped taxa
##already done in early steps
#mas_tax_df <- mas_tax_df[!(!is.na(mas_tax_df$kingdom) & mas_tax_df$kingdom==""), ]

mas_tax_df <- subset(mas_tax_df, select = -c(1, 51:56))

####Get rid of species not present in samples succeeding application
rownames(mas_tax_df) <- mas_tax_df$species
mas_tax_df$species <- NULL
mas_tax_df.new <- mas_tax_df[rowSums(mas_tax_df[])>0,]
#mas_tax_df.new <- tibble::rownames_to_column(mas_tax_df.new, "species")

##long format
#mas_count_df_long <- gather(mas_tax_df.new, sample, value, sample_17:sample_64, factor_key=TRUE)

##used reshape; worked much faster than spread for some reason
#mas_count_try <- reshape(mas_count_df_long, idvar = "sample", timevar = "species", direction = "wide")

##mas_count_new <- tidyr::spread(mas_count_df_long, species, value)

#rownames(mas_count_try) <- mas_count_try[,1]
#mas_count_new <- mas_count_try[,-1]

#names(mas_count_new)=str_sub(names(mas_count_new),7)

##Change 'biostimulant' back to 'prebiotic'. Reasoning is that maaslin takes the first group as the baseline, 
##despite trying to set the control as the baseline. Can change back for visualization. 

samples.df.2$Treatment[samples.df.2$Treatment == 'Biostimulant'] <- 'Prebiotic'

##remove baseline metadata
maas_meta_real <- samples.df.2[-(1:16),]

##Use this- ZINB has limited convergence issues and finds more issues than other models
bac_fit_data.1 = Maaslin2(
    input_data = mas_tax_df.new, 
    input_metadata = maas_meta_real, 
    output = "prelim_bac_output_without_singletons_ZINB.Location", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","V6"),
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")

###############################
##Let's look at differences preceding biostimulant application
pruned_norm_baseline_mas <- prune_samples(sample_data(pruned_prok_decontam_mas)$Developmental_Stage == "V1", pruned_prok_decontam_mas)

##merge at species level
pruned_norm_baseline_mas <- tax_glom(pruned_norm_baseline_mas, taxrank = 'species')

mas_count_df.base <- as.data.frame(otu_table(pruned_norm_baseline_mas))
mas_count_df.base <- tibble::rownames_to_column(mas_count_df.base, "otu")
mas_tax_df.base <- merge(mas_count_df.base, comm.prok.maas, by = c("otu"))

##filter out unmapped taxa
mas_tax_df.base <- mas_tax_df.base[!(!is.na(mas_tax_df.base$kingdom) & mas_tax_df.base$kingdom==""), ]

mas_tax_df.base <- subset(mas_tax_df.base, select = -c(1, 19:24))

####Get rid of species not present in samples preceding application
rownames(mas_tax_df.base) <- mas_tax_df.base$species
mas_tax_df.base$species <- NULL
mas_tax_df.new.base <- mas_tax_df.base[rowSums(mas_tax_df.base[])>0,]
#mas_tax_df.new.base <- tibble::rownames_to_column(mas_tax_df.new.base, "species")

##long format
#mas_count_df_long.base <- gather(mas_tax_df.new.base, sample, value, sample_1:sample_16, factor_key=TRUE)

##used reshape; worked much faster than spread for some reason
#mas_count_try.base <- reshape(mas_count_df_long.base, idvar = "sample", timevar = "species", direction = "wide")

##mas_count_new <- tidyr::spread(mas_count_df_long, species, value)

#rownames(mas_count_try.base) <- mas_count_try.base[,1]
#mas_count_new.base <- mas_count_try.base[,-1]

#names(mas_count_new.base)=str_sub(names(mas_count_new.base),7)

##remove metadata after baseline
maas_meta_base <- samples.df.2[(1:16),]

##Use this 
bac_fit_data.1.base = Maaslin2(
    input_data = mas_tax_df.new.base, 
    input_metadata = maas_meta_base, 
    output = "prelim_bac_output_without_singletons_ZINB.Location.base", 
    fixed_effects = c("Treatment", "Cultivar"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")
```

```{r}
##18S-ITS
##Differential abundance analysis; make sure to set normalization
##to none since our data is already normalized
##This is to find tax influenced by biostimulant application; will do separately for cultivar/developmental stage effects
##BiocManager::install("Maaslin2")
##library(Maaslin2)
##https://forum.biobakery.org/t/maaslin2-vs-mmuphin/4466/2

pruned_fun_decontam_mas <- prune_samples(sample_data(fun_decontam_filter_norm)$bio_or_control == "biological", fun_decontam_filter_norm)
pruned_norm_real_mas.fun <- prune_samples(!sample_data(pruned_fun_decontam_mas)$Developmental_Stage == "V1", pruned_fun_decontam_mas)

##merge at genus level
pruned_norm_real_mas.fun <- tax_glom(pruned_norm_real_mas.fun, taxrank = 'genus')

##Count df formatted for Maaslin- look only at timepoints after application (pruned_norm_real_mas)
mas_count_df.fun <- as.data.frame(otu_table(pruned_norm_real_mas.fun))
mas_count_df.fun <- tibble::rownames_to_column(mas_count_df.fun, "otu")
mas_tax_df.fun <- merge(mas_count_df.fun, fun.taxmat1, by = c("otu"))

##filter out unmapped taxa
mas_tax_df.fun <- mas_tax_df.fun[!(!is.na(mas_tax_df.fun$kingdom) & mas_tax_df.fun$kingdom==""), ]

mas_tax_df.fun <- subset(mas_tax_df.fun, select = -c(1, 50:54, 56))

####Get rid of species not present in samples succeeding application
rownames(mas_tax_df.fun) <- mas_tax_df.fun[,49]
mas_tax_df.fun <- mas_tax_df.fun[,-49]
mas_tax_df.new.fun <- mas_tax_df.fun[rowSums(mas_tax_df.fun[])>0,]
mas_tax_df.new.fun <- tibble::rownames_to_column(mas_tax_df.new.fun, "genus")


##long format
mas_count_df_long.fun <- gather(mas_tax_df.new.fun, sample, value, sample_17:sample_64, factor_key=TRUE)



##used reshape; worked much faster than spread for some reason
mas_count_try.fun <- reshape(mas_count_df_long.fun, idvar = "sample", timevar = "genus", direction = "wide")

##mas_count_new <- tidyr::spread(mas_count_df_long, genus, value)

rownames(mas_count_try.fun) <- mas_count_try.fun[,1]
mas_count_new.fun <- mas_count_try.fun[,-1]

names(mas_count_new.fun)=str_sub(names(mas_count_new.fun),7)

##Change 'biostimulant' back to 'prebiotic'. Reasoning is that maaslin takes the first group as the baseline, 
##despite trying to set the control as the baseline. Can change back for visualization. 
##This is copied from 16S code; no need to rerun, thus commented out. 

##samples.df.2$Treatment[samples.df.2$Treatment == 'Biostimulant'] <- 'Prebiotic'

##remove baseline metadata
##maas_meta_real <- samples.df.2[-(1:16),]


##Use this- ZINB has limited convergence issues and finds more issues than other models
fun_fit_data.1 = Maaslin2(
    input_data = mas_count_new.fun, 
    input_metadata = maas_meta_real, 
    output = "prelim_bac_output_without_singletons_ZINB.Location.fun.genus", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","V6"),
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")

###############################
##Let's look at differences preceding biostimulant application
pruned_norm_baseline_mas.fun <- prune_samples(sample_data(pruned_fun_decontam_mas)$Developmental_Stage == "V1", pruned_fun_decontam_mas)

##merge at genus level
pruned_norm_baseline_mas.fun <- tax_glom(pruned_norm_baseline_mas.fun, taxrank = 'genus')

mas_count_df.base.fun <- as.data.frame(otu_table(pruned_norm_baseline_mas.fun))
mas_count_df.base.fun <- tibble::rownames_to_column(mas_count_df.base.fun, "otu")
mas_tax_df.base.fun <- merge(mas_count_df.base.fun, fun.taxmat1, by = c("otu"))

##filter out unmapped taxa
mas_tax_df.base.fun <- mas_tax_df.base.fun[!(!is.na(mas_tax_df.base.fun$kingdom) & mas_tax_df.base.fun$kingdom==""), ]

mas_tax_df.base.fun <- subset(mas_tax_df.base.fun, select = -c(1, 18:22, 24))

####Get rid of species not present in samples preceding application
rownames(mas_tax_df.base.fun) <- mas_tax_df.base.fun[,17]
mas_tax_df.base.fun <- mas_tax_df.base.fun[,-17]
mas_tax_df.new.base.fun <- mas_tax_df.base.fun[rowSums(mas_tax_df.base.fun[])>0,]
mas_tax_df.new.base.fun <- tibble::rownames_to_column(mas_tax_df.new.base.fun, "genus")


##long format
mas_count_df_long.base.fun <- gather(mas_tax_df.new.base.fun, sample, value, sample_1:sample_16, factor_key=TRUE)



##used reshape; worked much faster than spread for some reason
mas_count_try.base.fun <- reshape(mas_count_df_long.base.fun, idvar = "sample", timevar = "genus", direction = "wide")

##mas_count_new <- tidyr::spread(mas_count_df_long, species, value)

rownames(mas_count_try.base.fun) <- mas_count_try.base.fun[,1]
mas_count_new.base.fun <- mas_count_try.base.fun[,-1]

names(mas_count_new.base.fun)=str_sub(names(mas_count_new.base.fun),7)

##remove metadata after baseline
##This is copied from 16S code; no need to rerun, thus commented out. 
##maas_meta_base <- samples.df.2[(1:16),]

##Use this 
fun_fit_data.1.base = Maaslin2(
    input_data = mas_count_new.base.fun, 
    input_metadata = maas_meta_base, 
    output = "prelim_bac_output_without_singletons_ZINB.Location.base.fun.genus", 
    fixed_effects = c("Treatment", "Cultivar"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")
```

```{r}
##16S
##Find real differentially abundant taxa by comparing real output to baseline output

##Get results in dataframe
real.da.taxa <- bac_fit_data.1$results
base.da.taxa <- bac_fit_data.1.base$results

##Remove those not DA after correction
##"A q value (FDR-corrected P value) <0.25 was considered statistically significant, consistent with prior microbiome studies. [21, 22]"
##https://www.microbiologyresearch.org/content/journal/jmm/10.1099/jmm.0.001512
real.da.taxa.1 <- filter(real.da.taxa,qval <= 0.25)
base.da.taxa.1 <- filter(base.da.taxa,qval <= 0.25)

##See how many baseline taxa are DA after application
maas.prok.overlap <- merge(real.da.taxa.1, base.da.taxa.1, by = c("feature", "metadata"))

##Now, see if x and y coefficients are both > or < 0. if true, remove that row. 
##Will do this manually since there are only 5 features. 
##It appears we must remove 3 features (both x and y coefficients were negative)

##Remove defined feature- Nitrosomonas.europaea - metadata = treatment
real.da.taxa.2 <- real.da.taxa.1 %>%  filter(!feature=='Nitrosomonas.europaea' | !metadata == 'Treatment')
real.da.taxa.2 <- real.da.taxa.2 %>%  filter(!feature=='Bifidobacterium.adolescentis' | !metadata == 'Treatment')
real.da.taxa.2 <- real.da.taxa.2 %>%  filter(!feature=='Sphingomonas.limnosediminicola' | !metadata == 'Treatment')

##add column specifying prokaryote- this will be useful for visualization
real.da.taxa.3 <- real.da.taxa.2 %>% mutate(taxtype = 'Prokaryote')

##calculate association signficance as done by maaslin algorithm
real.da.taxa.3$signficance <- with(real.da.taxa.3, -log(qval)*sign(coef))

##Keep only those da by treatment
real.da.taxa.3.trt <- subset(real.da.taxa.3, metadata == 'Treatment')

real.da.taxa.3.trt$value[real.da.taxa.3.trt$value == 'Prebiotic'] <- 'Biostimulant vs Control'
```

```{r}
##18S-ITS
##Find real differentially abundant taxa by comparing real output to baseline output

##Get results in dataframe
real.da.taxa.fun <- fun_fit_data.1$results
base.da.taxa.fun <- fun_fit_data.1.base$results

##Remove those not DA after correction
##"A q value (FDR-corrected P value) <0.25 was considered statistically significant, consistent with prior microbiome studies. [21, 22]"
##https://www.microbiologyresearch.org/content/journal/jmm/10.1099/jmm.0.001512

real.da.taxa.1.fun <- filter(real.da.taxa.fun,qval <= 0.25)
base.da.taxa.1.fun <- filter(base.da.taxa.fun,qval <= 0.25)

##See how many baseline taxa are DA after application
maas.fun.overlap <- merge(real.da.taxa.1.fun, base.da.taxa.1.fun, by = c("feature", "metadata"))

##Now, see if x and y coefficients are both > or < 0 for those DA by treatment. if true, remove that row. 
##Will do this manually. 
##It appears we must remove 2 features (both x and y coefficients were either positive or negative)

real.da.taxa.2.fun <- real.da.taxa.1.fun %>%  filter(!feature=='Phallus' | !metadata == 'Treatment')
real.da.taxa.2.fun <- real.da.taxa.2.fun %>%  filter(!feature=='Podospora' | !metadata == 'Treatment')

##add column specifying prokaryote- this will be useful for visualization
real.da.taxa.3.fun <- real.da.taxa.2.fun %>% mutate(taxtype = 'Eukaryote')

##calculate association signficance as done by maaslin algorithm
real.da.taxa.3.fun$signficance <- with(real.da.taxa.3.fun, -log(qval)*sign(coef))

##Keep only those da by treatment
real.da.taxa.3.fun.trt <- subset(real.da.taxa.3.fun, metadata == 'Treatment')

real.da.taxa.3.fun.trt$value[real.da.taxa.3.fun.trt$value == 'Prebiotic'] <- 'Biostimulant vs Control'
```

```{r}
##16S
##Let's find taxa DA between cultivars and developmental stages
##Will keep treatment in the model as a fixed effect to be considered

##First, we need a modified metadata file to implicate that all samples were control at V1 stage; 
##in other words, no biostimulant had been applied

maas.2 <- within(samples.df.2, Treatment[Treatment == 'Prebiotic' & Developmental_Stage == 'V1'] <- 'Control')

##will use pruned_prok_decontam_mas
##merge at species level
#pruned_prok_decontam_mas_glom <- tax_glom(pruned_prok_decontam_mas, taxrank = 'species')

##Count df formatted for Maaslin
mas_count_df.ds <- as.data.frame(otu_table(pruned_prok_decontam_mas))
mas_count_df.ds <- tibble::rownames_to_column(mas_count_df.ds, "otu")
mas_tax_df.ds <- merge(mas_count_df.ds, comm.prok.maas, by = c("otu"))

##filter out unmapped taxa
mas_tax_df.ds <- mas_tax_df.ds[!(!is.na(mas_tax_df.ds$kingdom) & mas_tax_df.ds$kingdom==""), ]

mas_tax_df.ds <- subset(mas_tax_df.ds, select = -c(1, 67:72))

rownames(mas_tax_df.ds) <- mas_tax_df.ds$species
mas_tax_df.ds$species <- NULL
mas_tax_df.new.ds <- mas_tax_df.ds[rowSums(mas_tax_df.ds[])>0,]
#mas_tax_df.new.ds <- tibble::rownames_to_column(mas_tax_df.new.ds, "species")

##long format
#mas_count_df_long.ds <- gather(mas_tax_df.new.ds, sample, value, sample_1:sample_64, factor_key=TRUE)

##used reshape; worked much faster than spread for some reason
#mas_count_try.ds <- reshape(mas_count_df_long.ds, idvar = "sample", timevar = "species", direction = "wide")

##mas_count_new <- tidyr::spread(mas_count_df_long, species, value)

#rownames(mas_count_try.ds) <- mas_count_try.ds[,1]
#mas_count_new.ds <- mas_count_try.ds[,-1]

#names(mas_count_new.ds)=str_sub(names(mas_count_new.ds),7)

##Use this 
bac_ds.base = Maaslin2(
    input_data = mas_tax_df.new.ds, 
    input_metadata = maas.2, 
    output = "prelim_bac_output_without_singletons_ZINB.ds.cult", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","V1"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")

##V6 as baseline
bac_ds.base.v6 = Maaslin2(
    input_data = mas_tax_df.new.ds, 
    input_metadata = maas.2, 
    output = "prelim_bac_output_without_singletons_ZINB.ds.cult.V6", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","V6"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")


##R3 as baseline
bac_ds.base.R3 = Maaslin2(
    input_data = mas_tax_df.new.ds, 
    input_metadata = maas.2, 
    output = "prelim_bac_output_without_singletons_ZINB.ds.cult.R3", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","R3"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")
```

```{r}
##18S-ITS
##Let's find taxa DA between cultivars and developmental stages
##Will keep treatment in the model as a fixed effect to be considered

##First, we need a modified metadata file to implicate that all samples were control at V1 stage; 
##in other words, no biostimulant had been applied
##This is copied from 16S code; no need to rerun, thus commented out. 
##maas.2 <- within(samples.df.2, Treatment[Treatment == 'Prebiotic' & Developmental_Stage == 'V1'] <- 'Control')

##will use pruned_fun_decontam_mas
##merge at species level
pruned_fun_decontam_mas_glom <- tax_glom(pruned_fun_decontam_mas, taxrank = 'genus')

##Count df formatted for Maaslin
mas_count_df.ds.fun <- as.data.frame(otu_table(pruned_fun_decontam_mas_glom))
mas_count_df.ds.fun <- tibble::rownames_to_column(mas_count_df.ds.fun, "otu")
mas_tax_df.ds.fun <- merge(mas_count_df.ds.fun, fun.taxmat1, by = c("otu"))

##filter out unmapped taxa
mas_tax_df.ds.fun <- mas_tax_df.ds.fun[!(!is.na(mas_tax_df.ds.fun$kingdom) & mas_tax_df.ds.fun$kingdom==""), ]

mas_tax_df.ds.fun <- subset(mas_tax_df.ds.fun, select = -c(1, 66:70, 72))

rownames(mas_tax_df.ds.fun) <- mas_tax_df.ds.fun[,65]
mas_tax_df.ds.fun <- mas_tax_df.ds.fun[,-65]
mas_tax_df.new.ds.fun <- mas_tax_df.ds.fun[rowSums(mas_tax_df.ds.fun[])>0,]
mas_tax_df.new.ds.fun <- tibble::rownames_to_column(mas_tax_df.new.ds.fun, "species")

##long format
mas_count_df_long.ds.fun <- gather(mas_tax_df.new.ds.fun, sample, value, sample_1:sample_64, factor_key=TRUE)

##used reshape; worked much faster than spread for some reason
mas_count_try.ds.fun <- reshape(mas_count_df_long.ds.fun, idvar = "sample", timevar = "species", direction = "wide")

##mas_count_new <- tidyr::spread(mas_count_df_long, species, value)

rownames(mas_count_try.ds.fun) <- mas_count_try.ds.fun[,1]
mas_count_new.ds.fun <- mas_count_try.ds.fun[,-1]

names(mas_count_new.ds.fun)=str_sub(names(mas_count_new.ds.fun),7)

##Use this 
fun_ds.base = Maaslin2(
    input_data = mas_count_new.ds.fun, 
    input_metadata = maas.2, 
    output = "prelim_fun_output_without_singletons_ZINB.ds.cult.genus", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","V1"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")

##V6 as baseline
fun_ds.base.v6 = Maaslin2(
    input_data = mas_count_new.ds.fun, 
    input_metadata = maas.2, 
    output = "prelim_fun_output_without_singletons_ZINB.ds.cult.V6", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","V6"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")

##R3 as baseline
fun_ds.base.R3 = Maaslin2(
    input_data = mas_count_new.ds.fun, 
    input_metadata = maas.2, 
    output = "prelim_fun_output_without_singletons_ZINB.ds.cult.R3", 
    fixed_effects = c("Treatment", "Cultivar", "Developmental_Stage"), 
    reference = c("Developmental_Stage","R3"), 
    random_effects = c("Location"),
    normalization = "NONE",
    transform = "NONE",
    analysis_method = "ZINB")
```

```{r}
##16S
ds.da.taxa <- bac_ds.base$results

##Remove those not DA after correction
ds.da.taxa.1 <- filter(ds.da.taxa,qval <= 0.25)

##add column specifying prokaryote- this will be useful for visualization
ds.da.taxa.2 <- ds.da.taxa.1 %>% mutate(taxtype = 'Prokaryote')

##calculate association signficance as done by maaslin algorithm
ds.da.taxa.2$signficance <- with(ds.da.taxa.2, -log(qval)*sign(coef))

##filter out those da by treatment
ds.da.taxa.4 <- subset(ds.da.taxa.2, metadata != 'Treatment')

ds.da.taxa.4$value[ds.da.taxa.4$value == 'R6'] <- 'R6 vs V1'
ds.da.taxa.4$value[ds.da.taxa.4$value == 'R3'] <- 'R3 vs V1'
ds.da.taxa.4$value[ds.da.taxa.4$value == 'V6'] <- 'V6 vs V1'
ds.da.taxa.4$value[ds.da.taxa.4$value == 'CZ4979X'] <- 'CZ4979X vs CZ4810X'

###Now process V6 baseline data###
ds.da.taxa.v6 <- bac_ds.base.v6$results
##Remove those not DA after correction
ds.da.taxa.1.v6 <- filter(ds.da.taxa.v6,qval <= 0.25)

##add column specifying prokaryote- this will be useful for visualization
ds.da.taxa.2.v6 <- ds.da.taxa.1.v6 %>% mutate(taxtype = 'Prokaryote')

##calculate association signficance as done by maaslin algorithm
ds.da.taxa.2.v6$signficance <- with(ds.da.taxa.2.v6, -log(qval)*sign(coef))
ds.da.taxa.2.v6$value[ds.da.taxa.2.v6$value == 'R6'] <- 'R6 vs V6'
ds.da.taxa.2.v6$value[ds.da.taxa.2.v6$value == 'R3'] <- 'R3 vs V6'

##filter out those da by treatment
ds.da.taxa.3.v6 <- subset(ds.da.taxa.2.v6, metadata != 'Treatment')

##filter out those da by Cultivar
ds.da.taxa.3.v6 <- subset(ds.da.taxa.3.v6, metadata != 'Cultivar')

##filter out V1 vs V6
ds.da.taxa.3.v6 <- subset(ds.da.taxa.3.v6, value != 'V1')

###Now process R3 baseline data###
ds.da.taxa.r3 <- bac_ds.base.R3$results
##Remove those not DA after correction
ds.da.taxa.1.r3 <- filter(ds.da.taxa.r3,qval <= 0.25)

##add column specifying prokaryote- this will be useful for visualization
ds.da.taxa.2.r3 <- ds.da.taxa.1.r3 %>% mutate(taxtype = 'Prokaryote')

##calculate association signficance as done by maaslin algorithm
ds.da.taxa.2.r3$signficance <- with(ds.da.taxa.2.r3, -log(qval)*sign(coef))
ds.da.taxa.2.r3$value[ds.da.taxa.2.r3$value == 'R6'] <- 'R6 vs R3'

##filter out those da by treatment
ds.da.taxa.3.r3 <- subset(ds.da.taxa.2.r3, metadata != 'Treatment')

##filter out those da by Cultivar
ds.da.taxa.3.r3 <- subset(ds.da.taxa.3.r3, metadata != 'Cultivar')

##filter out V1 vs V6
ds.da.taxa.3.r3 <- subset(ds.da.taxa.3.r3, value != 'V1')
ds.da.taxa.3.r3 <- subset(ds.da.taxa.3.r3, value != 'V6')

##Get all da prokaryotes in one dataframe
da_prok_all <- rbind(real.da.taxa.3.trt, ds.da.taxa.4, ds.da.taxa.3.r3, ds.da.taxa.3.v6)

da_prok_all$name <- NULL
da_prok_all$N.not.zero <- NULL
```

```{r}
##18S-ITS
ds.da.taxa.fun <- fun_ds.base$results

##Remove those not DA after correction
ds.da.taxa.1.fun <- filter(ds.da.taxa.fun,qval <= 0.25)

##add column specifying prokaryote- this will be useful for visualization
ds.da.taxa.2.fun <- ds.da.taxa.1.fun %>% mutate(taxtype = 'Eukaryote')

##calculate association signficance as done by maaslin algorithm
ds.da.taxa.2.fun$signficance <- with(ds.da.taxa.2.fun, -log(qval)*sign(coef))

##filter out those da by treatment
ds.da.taxa.4.fun <- subset(ds.da.taxa.2.fun, metadata != 'Treatment')

ds.da.taxa.4.fun$value[ds.da.taxa.4.fun$value == 'R6'] <- 'R6 vs V1'
ds.da.taxa.4.fun$value[ds.da.taxa.4.fun$value == 'R3'] <- 'R3 vs V1'
ds.da.taxa.4.fun$value[ds.da.taxa.4.fun$value == 'V6'] <- 'V6 vs V1'
ds.da.taxa.4.fun$value[ds.da.taxa.4.fun$value == 'CZ4979X'] <- 'CZ4979X vs CZ4810X'

###Now process V6 baseline data###
ds.da.taxa.v6.fun <- fun_ds.base.v6$results
##Remove those not DA after correction
ds.da.taxa.1.v6.fun <- filter(ds.da.taxa.v6.fun,qval <= 0.25)

##add column specifying prokaryote- this will be useful for visualization
ds.da.taxa.2.v6.fun <- ds.da.taxa.1.v6.fun %>% mutate(taxtype = 'Eukaryote')

##calculate association signficance as done by maaslin algorithm
ds.da.taxa.2.v6.fun$signficance <- with(ds.da.taxa.2.v6.fun, -log(qval)*sign(coef))
ds.da.taxa.2.v6.fun$value[ds.da.taxa.2.v6.fun$value == 'R6'] <- 'R6 vs V6'
ds.da.taxa.2.v6.fun$value[ds.da.taxa.2.v6.fun$value == 'R3'] <- 'R3 vs V6'

##filter out those da by treatment
ds.da.taxa.3.v6.fun <- subset(ds.da.taxa.2.v6.fun, metadata != 'Treatment')

##filter out those da by Cultivar
ds.da.taxa.3.v6.fun <- subset(ds.da.taxa.3.v6.fun, metadata != 'Cultivar')

##filter out V1 vs V6
ds.da.taxa.3.v6.fun <- subset(ds.da.taxa.3.v6.fun, value != 'V1')

###Now process R3 baseline data###
ds.da.taxa.r3.fun <- fun_ds.base.R3$results
##Remove those not DA after correction
ds.da.taxa.1.r3.fun <- filter(ds.da.taxa.r3.fun,qval <= 0.25)

##add column specifying prokaryote- this will be useful for visualization
ds.da.taxa.2.r3.fun <- ds.da.taxa.1.r3.fun %>% mutate(taxtype = 'Eukaryote')

##calculate association signficance as done by maaslin algorithm
ds.da.taxa.2.r3.fun$signficance <- with(ds.da.taxa.2.r3.fun, -log(qval)*sign(coef))
ds.da.taxa.2.r3.fun$value[ds.da.taxa.2.r3.fun$value == 'R6'] <- 'R6 vs R3'

##filter out those da by treatment
ds.da.taxa.3.r3.fun <- subset(ds.da.taxa.2.r3.fun, metadata != 'Treatment')

##filter out those da by Cultivar
ds.da.taxa.3.r3.fun <- subset(ds.da.taxa.3.r3.fun, metadata != 'Cultivar')

##filter out V1 vs V6
ds.da.taxa.3.r3.fun <- subset(ds.da.taxa.3.r3.fun, value != 'V1')
ds.da.taxa.3.r3.fun <- subset(ds.da.taxa.3.r3.fun, value != 'V6')

##Get all da in one dataframe
da_fun_all <- rbind(real.da.taxa.3.fun.trt, ds.da.taxa.4.fun, ds.da.taxa.3.r3.fun, ds.da.taxa.3.v6.fun)
da_fun_all$N.not.0 <- NULL

da_taxa <- rbind(da_fun_all, da_prok_all)
da_taxa$feature <- str_replace(da_taxa$feature, "[.]", " ")
```

```{r}
##Before visualizing community membership, we will obtain functional annotations from Funguild and BacDive for eukaryotes and prokaryotes, respectively
##We will use this info to contextualize community membership as well as the microbiome dataset as a whole. 

##Get all eukaryotic taxa at genus level
fun_decontam_norm_glom <- tax_glom(fun_decontam_filter_norm, taxrank = 'genus')
euk.fun <- as.data.frame(tax_table(fun_decontam_norm_glom))

##Start with fungaltraits for eukaryotes
##https://rdrr.io/github/traitecoevo/fungaltraits/
##https://link.springer.com/article/10.1007/s13225-020-00466-2
##Obatained database from here: https://docs.google.com/spreadsheets/d/1cxImJWMYVTr6uIQXcTLwK1YNNzQvKJJifzzNpKCM6O0/edit#gid=33668129 
FungalTraits_1.2_ver_16Dec_2020 <- read.csv("/Volumes/USB20FD/PhD Research/Chapter_4/FungalTraits_1.2_ver_16Dec_2020.csv") ##replace with name where db is stored

##Rename genus column, then merge
FungalTraits_1.2_ver_16Dec_2020 <- FungalTraits_1.2_ver_16Dec_2020 %>%
  rename(genus = GENUS)

euk.fun.2 <- merge(FungalTraits_1.2_ver_16Dec_2020, euk.fun, by = c("genus"), all.y = TRUE)

##Now for prokaryotes
##Will try to use BacDive
##Note that the github link below is inoperable as of 2021; must follow API tutorial
##https://github.com/TIBHannover/BacDiveR
install.packages("BacDive", repos = "http://R-Forge.R-project.org")
library(BacDive)

##initialization
bacdive <- open_bacdive("email", "password") ##replace with email and password

##Now we must obtain prokaryotic species in our dataset
##can use tax.mat.3$species
##Will write loop for this

##Extracting the species from tax.mat.3:
species_list <- unique(tax.mat.3$species)

##now, let's remove strain information to increase the likelihood of retrieving info
species_list_cleaned <- sapply(species_list, function(name) {
  parts <- strsplit(name, " ")[[1]]
  return(paste(parts[1:min(2, length(parts))], collapse = " "))
})

##Remove duplicates if any
species_list_cleaned <- unique(species_list_cleaned)


##Use the lapply function to send requests for each species and store results in a list; 
#bacdive.tax <- request(object = bacdive, query = "taxon name", search = "taxon")

##Try retrieval
retrieval_list <- lapply(species_list_cleaned, function(specie) {
  retrieve(object = bacdive, query = specie, search = "taxon")
})

##Name the list with the species for easy reference:
names(retrieval_list) <- species_list_cleaned

retrieval_list[["Specific_species_name"]]

df_list <- lapply(retrieval_list, function(entry) {
  # Check if the entry is non-empty and can be unlisted to a vector
  if (is.list(entry) && !is.null(entry) && length(unlist(entry)) > 0) {
    temp_df <- data.frame(matrix(unlist(entry), nrow=1, byrow=TRUE))
    return(temp_df)
  } else {
    return(NULL) # return NULL for problematic entries
  }
})

# Remove NULL entries
df_list <- df_list[!sapply(df_list, is.null)]

##Dfs are large, with many columns containing the same value
##We will clean this up to speed up the processing
##Define a function to remove duplicate columns from a data frame
remove_duplicate_columns <- function(df) {
  # Get indices of unique columns
  unique_cols <- !duplicated(as.list(df))
  # Subset the data frame to retain only the unique columns
  df[, unique_cols]
}

# Create a copy of df_list
df_list_copy <- df_list

# Apply the function to each data frame in df_list_copy
df_list_cleaned <- lapply(df_list_copy, remove_duplicate_columns)

combined_df <- bind_rows(df_list_cleaned)

combined_df.t <- as.data.frame(t(combined_df))

##The output is messy, without headers
##Let's try and search for keywords to help annotate the microbes
##Based off of this: https://bacdive.dsmz.de/dashboard
##We will split the columns of combined_df.t (which represent microbiota) back
##into individual dfs, and filter by keywords

# Create a list of data frames, where each data frame corresponds to a column in combined_df.t
df_list <- lapply(names(df_list_cleaned), function(col_name) {
  df <- data.frame(species = df_list_cleaned[, col_name, drop = FALSE])
  colnames(df) <- col_name
  return(df)
})

# Define a function to filter rows based on the provided criteria
filter_rows_values <- function(df) {
  criteria <- c(species_list_cleaned, "philic", "aerobe", "pathogen", "nutrient", "symbio", "gram")
  criteria_pattern <- paste0(criteria, collapse = "|")
  # Check if any row's value matches any part of the criteria pattern
  filtered_rows <- apply(df, 1, function(row) {
    any(grepl(criteria_pattern, row, ignore.case = TRUE))
  })
  filtered_df <- df[filtered_rows, , drop = FALSE]
  return(filtered_df)
}

# Apply the modified filter function to each data frame in df_list
filtered_df_list_cleaned <- lapply(df_list_cleaned, filter_rows_values)

###########
#species_list_cleaned,
filter_columns_values <- function(df) {
  criteria <- c("philic", "aerobe", "pathogen", "nutrient", "symbio", "gram")
  criteria_pattern <- paste0(criteria, collapse = "|")
  
  # Check if any column's value matches any part of the criteria pattern
  filtered_columns <- apply(df, 2, function(col) {
    any(grepl(criteria_pattern, col, ignore.case = TRUE))
  })
  
  filtered_df <- df[, filtered_columns, drop = FALSE]
  return(filtered_df)
}

# Apply the modified filter function to each data frame in df_list_cleaned
filtered_df_list_cleaned_columns <- lapply(df_list_cleaned, filter_columns_values)

##Format this further
# Transpose each data frame in the list
transposed_dfs <- lapply(names(filtered_df_list_cleaned_columns), function(df_name) {
  df <- filtered_df_list_cleaned_columns[[df_name]]
  t_df <- as.data.frame(t(df))
  colnames(t_df) <- df_name
  return(t_df)
})

##combine
max_rows <- max(sapply(transposed_dfs, nrow))

# Make all data frames the same size by padding with NAs
padded_dfs <- lapply(transposed_dfs, function(df) {
  additional_rows <- max_rows - nrow(df)
  if(additional_rows > 0) {
    padding <- as.data.frame(matrix(NA, nrow = additional_rows, ncol = ncol(df)))
    colnames(padding) <- colnames(df)
    return(rbind(df, padding))
  } else {
    return(df)
  }
})

# Combine them side-by-side
combined_df_cleaned <- do.call(cbind, padded_dfs)


##Now, let's retrieve only the info of interest
##Start with pathogenicity info 
check_pathogen <- function(col) {
  matches <- grepl("pathogen", combined_df_cleaned[[col]], ignore.case = TRUE)
  
  if (any(matches)) {
    data.frame(
      column_name = col,
      value = combined_df_cleaned[[col]][matches]
    )
  } else {
    NULL
  }
}

# Apply the function to each column
pathogen_info <- do.call(rbind, lapply(colnames(combined_df_cleaned), check_pathogen))

filtered_pathogen_info <- pathogen_info[pathogen_info$value %in% c('human pathogen', 'animal pathogen', 'plant pathogen', 'phytopathogen') | 
                                        grepl("human pathogen", pathogen_info$value, ignore.case = TRUE) |
                                        grepl("animal pathogen", pathogen_info$value, ignore.case = TRUE) |
                                        grepl("plant pathogen", pathogen_info$value, ignore.case = TRUE) |
                                        grepl("phytopathogen", pathogen_info$value, ignore.case = TRUE), ]

##Get cohesive naming
filtered_pathogen_info <- filtered_pathogen_info %>%
  mutate(value = ifelse(grepl("human pathogen", value, ignore.case = TRUE), "human/animal pathogen", value),
         value = ifelse(grepl("animal pathogen", value, ignore.case = TRUE), "human/animal pathogen", value),
         value = ifelse(grepl("plant pathogen", value, ignore.case = TRUE), "plant pathogen", value))

##final list
filtered_pathogen_info.2 <- filtered_pathogen_info %>%
  filter(grepl("human/animal pathogen|plant pathogen", value)) %>%
  distinct()


##Now, for O2 tolerance
check_aerobe <- function(col) {
  matches <- grepl("aerobe", combined_df_cleaned[[col]], ignore.case = TRUE)
  
  if (any(matches)) {
    data.frame(
      column_name = col,
      value = combined_df_cleaned[[col]][matches]
    )
  } else {
    NULL
  }
}

# Apply the function to each column
aerobe_info <- do.call(rbind, lapply(colnames(combined_df_cleaned), check_aerobe))

##Get cohesive naming
aerobe_info.2 <- aerobe_info %>%
  mutate(
         value = ifelse(grepl("\\bfacultative aerobe\\b", value, ignore.case = TRUE), "facultative aerobe", value),
         value = ifelse(grepl("\\bfacultative anaerobe\\b", value, ignore.case = TRUE), "facultative anaerobe", value),
         value = ifelse(grepl("\\bobligate aerobe\\b", value, ignore.case = TRUE), "obligate aerobe", value),
         value = ifelse(grepl("\\bmicroaerotolerant\\b", value, ignore.case = TRUE), "microaerotolerant", value),
         value = ifelse(grepl("\\baerotolerant\\b", value, ignore.case = TRUE), "aerotolerant", value),
         value = ifelse(grepl("\\bobligate anaerobe\\b", value, ignore.case = TRUE), "obligate anaerobe", value),
         value = ifelse(grepl("\\bmicroaerophile\\b", value, ignore.case = TRUE), "microaerophile", value),
         # Now for the more general terms:
         value = ifelse(grepl("\\banaerobe\\b", value, ignore.case = TRUE) & !grepl("\\bobligate anaerobe\\b|\\bfacultative anaerobe\\b", value, ignore.case = TRUE), "anaerobe", value),
         value = ifelse(grepl("\\baerobe\\b", value, ignore.case = TRUE) & !grepl("\\bobligate aerobe\\b|\\bfacultative aerobe\\b", value, ignore.case = TRUE), "aerobe", value))

##Final list
desired_values <- c("anaerobe", "aerobe", "obligate aerobe", "facultative anaerobe", "facultative aerobe", "obligate anaerobe")

aerobe_info_filtered <- aerobe_info.2 %>%
  filter(value %in% desired_values) %>%
  distinct()

##some have multiple matches; let's concatenate
aerobe_info_collapsed <- aerobe_info_filtered %>%
  group_by(column_name) %>%
  summarise(value = paste(unique(value), collapse = "/")) %>%
  ungroup()

aerobe_info_collapsed <- aerobe_info_collapsed %>%
  mutate(value = sapply(value, function(x) {
    vals <- sort(unlist(strsplit(x, "/")))
    paste(vals, collapse = "/")
  }))

##Now, for gram positive or negative 
check_gram <- function(col) {
  matches <- grepl("gram", combined_df_cleaned[[col]], ignore.case = TRUE)
  
  if (any(matches)) {
    data.frame(
      column_name = col,
      value = combined_df_cleaned[[col]][matches]
    )
  } else {
    NULL
  }
}

# Apply the function to each column
gram_info <- do.call(rbind, lapply(colnames(combined_df_cleaned), check_gram))

##Get cohesive naming
filtered_gram_info <- gram_info %>%
  mutate(value = ifelse(grepl("gram-positive", value, ignore.case = TRUE), "gram-positive", value),
         value = ifelse(grepl("gram-negative", value, ignore.case = TRUE), "gram-negative", value))

##Final list
desired_values.gram <- c("gram-positive", "gram-negative")

filtered_gram_info.2 <- filtered_gram_info %>%
  filter(value %in% desired_values.gram) %>%
  distinct()

##some have multiple matches; let's concatenate
gram_info_collapsed <- filtered_gram_info.2 %>%
  group_by(column_name) %>%
  summarise(value = paste(unique(value), collapse = "/")) %>%
  ungroup()

gram_info_collapsed <- gram_info_collapsed %>%
  mutate(value = sapply(value, function(x) {
    vals <- sort(unlist(strsplit(x, "/")))
    paste(vals, collapse = "/")
  }))

##Now, for temp range
check_philic <- function(col) {
  matches <- grepl("philic", combined_df_cleaned[[col]], ignore.case = TRUE)
  
  if (any(matches)) {
    data.frame(
      column_name = col,
      value = combined_df_cleaned[[col]][matches]
    )
  } else {
    NULL
  }
}

# Apply the function to each column
philic_info <- do.call(rbind, lapply(colnames(combined_df_cleaned), check_philic))

##Get cohesive naming
filtered_philic_info <- philic_info %>%
   mutate(value = ifelse(grepl("hyperthermophilic", value, ignore.case = TRUE), "hyperthermophilic", value),
          value = ifelse(grepl("thermophilic", value, ignore.case = TRUE), "thermophilic", value),
          value = ifelse(grepl("mesophilic", value, ignore.case = TRUE), "mesophilic", value),
          value = ifelse(grepl("psychrophilic", value, ignore.case = TRUE), "psychrophilic", value))

##Final list
desired_values.philic <- c("hyperthermophilic", "thermophilic", "mesophilic", "psychrophilic")

filtered_philic_info.2 <- filtered_philic_info %>%
  filter(value %in% desired_values.philic) %>%
  distinct()

##some have multiple matches; let's concatenate
philic_info_collapsed <- filtered_philic_info.2 %>%
  group_by(column_name) %>%
  summarise(value = paste(unique(value), collapse = "/")) %>%
  ungroup()

philic_info_collapsed <- philic_info_collapsed %>%
  mutate(value = sapply(value, function(x) {
    vals <- sort(unlist(strsplit(x, "/")))
    paste(vals, collapse = "/")
  }))

#####Now, let's combine prokaryote info, keeping all names for all columns
##philic_info_collapsed, gram_info_collapsed, aerobe_info_collapsed, filtered_pathogen_info.2

##Get unique values of 'column_name' from all dataframes
all_column_names <- bind_rows(
  select(philic_info_collapsed, column_name),
  select(gram_info_collapsed, column_name),
  select(aerobe_info_collapsed, column_name),
  select(filtered_pathogen_info.2, column_name)
) %>%
distinct(column_name)

##Merge all dataframes by 'column_name'
prok.function <- all_column_names %>%
  left_join(philic_info_collapsed, by = "column_name") %>%
  left_join(gram_info_collapsed, by = "column_name") %>%
  left_join(aerobe_info_collapsed, by = "column_name") %>%
  left_join(filtered_pathogen_info.2, by = "column_name")

##Combine with list of all prokaryotes
##This is tricky, given we have species-level info in one df and 
##strain-level in another
##Will add a column to tax.mat.3 to facilitate merge
prok.tax.full <- tax.mat.3 %>%
  mutate(column_name = str_extract(species, "^(\\w+ \\w+)"))

# Now, merge prok.function with prok.tax.full by column_name
prok.function.2 <- merge(prok.function, prok.tax.full, by = "column_name", all = TRUE)

prok.function.3 <- prok.function.2 %>% distinct()

##Add domain
prok.function.3$Domain <- 'Prokaryote'
euk.fun.2$Domain <- 'Eukaryote'


##Modify both to facilitate bind
euk.fun.2 <- euk.fun.2 %>%
  mutate_at(vars(primary_lifestyle, Secondary_lifestyle), 
            ~str_replace_all(., "_", " "))

euk.fun.2 <- euk.fun.2 %>%
  mutate(Lifestyle_Pathogenicity = ifelse((primary_lifestyle == "" & Secondary_lifestyle == "") | (primary_lifestyle == "NA" & Secondary_lifestyle == "NA"), "NA",
                                          ifelse(primary_lifestyle == "" | primary_lifestyle == "NA", Secondary_lifestyle,
                                                 ifelse(Secondary_lifestyle == "" | Secondary_lifestyle == "NA", primary_lifestyle,
                                              paste(primary_lifestyle, Secondary_lifestyle, sep = "/")))))

##Rename columns and values
euk.fun.2 <- euk.fun.2 %>%
  rename(
    #Environment = Aquatic_habitat_template,
    #Growth = Growth_form_template,
    #Structure = Fruitbody_type_template,
    `Lifestyle/Pathogenicity` = Lifestyle_Pathogenicity
  )

euk.fun.2 <- euk.fun.2 %>%
  mutate(
    Environment = if_else(Environment == "partly_freshwater_(partly_non-aquatic)", "partly_aquatic", Environment),
    Environment = str_replace_all(Environment, "_", " "), # Replace underscores with spaces

    Growth = str_replace_all(Growth, "_", " "),
    Growth = str_extract(Growth, "^[\\w]+ [\\w]+"), # Extracts the first two words
    
    Structure = str_extract(Structure, "^[^_]+"),  # Extracts the word before the underscore
    Structure = str_replace_all(Structure, "_", " ")
  )

##a bit more
euk.fun.2 <- euk.fun.2 %>%
  mutate(
    Environment = case_when(
      Environment == "partly marine (partly non-aquatic)" ~ "partly_aquatic",
      Environment == "marine" ~ "aquatic",
      TRUE ~ Environment
    ),
    Environment = str_replace_all(Environment, "_", " "), # Replace underscores with spaces

    Growth = str_replace_all(Growth, "_", " "),
    Growth = str_extract(Growth, "^[\\w]+ [\\w]+"), # Extracts the first two words
    
    Structure = str_extract(Structure, "^[^_]+"),  # Extracts the word before the underscore
    Structure = str_replace_all(Structure, "_", " ")
  )

##Retain only columns of interest
euk.fun.3 <- euk.fun.2 %>%
  select(genus, Environment, Growth, Structure, Domain, `Lifestyle/Pathogenicity`)
      
##Now for prokaryotes
prok.function.4 <- prok.function.3 %>%
  rename(genus.x = genus,
    genus = species,##to faciliate rbind
    Growth = value.x,
    Structure = value.y,
    Environment = value.x.x,
    `Lifestyle/Pathogenicity` = value.y.y
  ) %>%
  select(genus, Environment, Growth, Structure, Domain, `Lifestyle/Pathogenicity`)

######combine#####
taxon.function <- bind_rows(prok.function.4, euk.fun.3)

##Use this
taxon.function.1 <- taxon.function %>%
  mutate_at(vars(Environment, Growth, Structure, Domain, `Lifestyle/Pathogenicity`), 
            ~ifelse(. %in% c("", "none", NA), "unreported", .)) %>%
  mutate(`Lifestyle/Pathogenicity` = ifelse(str_detect(`Lifestyle/Pathogenicity`, "^(?!.*mycoparasite).*parasite.*$"), 
                                            str_replace(`Lifestyle/Pathogenicity`, "parasite", "pathogen"), 
                                            `Lifestyle/Pathogenicity`))


##Coming back later.. let's only retain species 
taxon.function.2 <- taxon.function.1
taxon.function.2$genus <- gsub("^(\\w+ \\w+).*", "\\1", taxon.function.2$genus)

taxon.function.2 <- taxon.function.2 %>%
  distinct(genus, .keep_all = TRUE)


##Let's see what functions are most present in the rhizosphere
prok.profile <- merge(taxon.function.1, taxmat1, by.x = "genus", by.y = "species")

# For column 2
col2_values <- table(prok.profile[, 2])
col2_most_abundant <- sort(col2_values, decreasing = TRUE)[1]

# For column 3
col3_values <- table(prok.profile[, 3])
col3_most_abundant <- sort(col3_values, decreasing = TRUE)[1]

# For column 4
col4_values <- table(prok.profile[, 4])
col4_most_abundant <- sort(col4_values, decreasing = TRUE)[1]

# For column 6
col6_values <- table(prok.profile[, 6])
col6_most_abundant <- sort(col6_values, decreasing = TRUE)[1]

# Print the results
print(col2_most_abundant)
print(col3_most_abundant)
print(col4_most_abundant)
print(col6_most_abundant)

##Now for eukaryotes
euk.profile <- merge(fun.tax, taxon.function.1, by = "genus")

# For column 8
top_5_values_8 <- head(sort(table(euk.profile[,8]), decreasing = TRUE), 5)

# For column 9
top_5_values_9 <- head(sort(table(euk.profile[,9]), decreasing = TRUE), 5)

# For column 10
top_5_values_10 <- head(sort(table(euk.profile[,10]), decreasing = TRUE), 5)

# For column 12
top_5_values_12 <- head(sort(table(euk.profile[,12]), decreasing = TRUE), 5)

# Print the results
cat("Top 5 values for column 8:\n")
print(top_5_values_8)

cat("\nTop 5 values for column 9:\n")
print(top_5_values_9)

cat("\nTop 5 values for column 10:\n")
print(top_5_values_10)

cat("\nTop 5 values for column 12:\n")
print(top_5_values_12)
```

```{r}
##visualize and get summary for DA taxa
##a couple different visualizations options are presented here
##library(ComplexHeatmap)

#da_taxa_hm <- subset(da_taxa, select = -c(2, 4:11))
da_taxa_hm <- subset(da_taxa, select = -c(2, 4:9))

##Get summary
##feature per comparison
metadata_summary <- da_taxa %>%
  group_by(value) %>%
  summarise(Total_features = n_distinct(feature)) 


##Breakdown of taxtype
taxtype_feature_summary <- da_taxa %>%
  filter(taxtype %in% c('Prokaryote', 'Eukaryote')) %>%
  group_by(value, taxtype) %>%
  summarise(Total_rows = n())

##Breakdown of direction
significance_feature_summary <- da_taxa %>%
  group_by(value, sign_category = ifelse(signficance > 0, "Positive", "Negative")) %>%
  summarise(Total_rows = n())

##Modify for heatmap
da_taxa_wide <- spread(da_taxa_hm, value, signficance)

##Rename R3 to R2
da_taxa_wide <- da_taxa_wide %>%
  rename(
    `R2 vs V1` = `R3 vs V1`,
    `R2 vs V6` = `R3 vs V6`,
    `R6 vs R2` = `R6 vs R3`)

da_taxa_wide %>% mutate_at(c('Biostimulant vs Control', 'R6 vs V1', 'R2 vs V1', 'V6 vs V1', 'CZ4979X vs CZ4810X', 'R2 vs V6', 'R6 vs R2', 'R6 vs V6'), as.numeric)

rownames(da_taxa_wide) <- da_taxa_wide[,1]
da_taxa_wide <- da_taxa_wide[,-1]

##reorder columns
da_taxa_wide.2 <- da_taxa_wide[, c(8, 3, 6, 4, 7, 5, 2, 1)] 

##Add annotation
##get tax type for each microbe
ann <- subset(da_taxa, select = -c(2:8, 10))
ann <- ann[!duplicated(ann$feature), ]
##get taxa in alphabetical order
ann_2 <- as.data.frame(ann[order(ann$feature), ])
rownames(ann_2) <- ann_2$feature
ann_2$feature <- NULL
colnames(ann_2) <- c('Domain')

##add additional functional info
da.ann.function <- merge(ann, taxon.function.2, by.x = "feature", by.y = "genus")

da.ann.function <- da.ann.function %>%
  mutate(
    `Lifestyle/Pathogenicity` = case_when(
      grepl("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", `Lifestyle/Pathogenicity`) & grepl("pathogen", `Lifestyle/Pathogenicity`) ~ gsub("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", "saprotroph", `Lifestyle/Pathogenicity`),
      grepl("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", `Lifestyle/Pathogenicity`) ~ "saprotroph",
      `Lifestyle/Pathogenicity` == "NA" ~ "unreported",  # Handle the string "NA"
      TRUE ~ `Lifestyle/Pathogenicity`
    ),
    `Lifestyle/Pathogenicity` = ifelse(
      grepl("saprotroph/.+pathogen|.+pathogen/saprotroph", `Lifestyle/Pathogenicity`),
      sapply(strsplit(`Lifestyle/Pathogenicity`, "/"), function(x) paste(sort(x), collapse="/")),
      `Lifestyle/Pathogenicity`
    ),
    Structure = ifelse(Structure %in% c("None", "other"), "unreported", Structure),
    Environment = case_when(
      Environment == "facultative anaerobe" ~ "anaerobe",
      Environment == "aerobe/anaerobe/facultative aerobe" ~ "aerobe/anaerobe",
      Environment == "facultative aerobe" ~ "aerobe",
      Environment == "aerobe/obligate aerobe" ~ "aerobe",
      Environment == "aerobe/facultative anaerobe" ~ "aerobe/anaerobe",
      Environment == "aerobe/facultative aerobe" ~ "aerobe",
      Environment == "anaerobe/obligate anaerobe" ~ "anaerobe",
      Environment == "obligate aerobe" ~ "aerobe",
      Environment == "freshwater" ~ "aquatic",
      TRUE ~ Environment
    )
  )

da.ann.function <- da.ann.function %>%
  mutate(`Lifestyle/Pathogenicity` = if_else(`Lifestyle/Pathogenicity` == "plant pathogen/unsepcified saprotroph",
                                             "plant pathogen/saprotroph",
                                             `Lifestyle/Pathogenicity`))

rownames(da.ann.function) <- da.ann.function$feature    
da.ann.function$feature <- NULL
da.ann.function$taxtype <- NULL
da.ann.function <- da.ann.function[, names(core.ann.function)] ##core.ann.function is below

rowAnn <- HeatmapAnnotation(df = da.ann.function, 
  which = 'row',
  col = colours, ##from unique taxon analysis below
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), simple_anno_size = unit(0.2, "cm"), show_legend = FALSE, show_annotation_name = FALSE)

lgd_list <- list(title_gp = gpar(fontsize = 16), 
       labels_gp = gpar(fontsize = 12), 
        title = "log-normalized FDR", 
     border = "black", lwd = 1, direction = "horizontal", title_position = "topcenter",
      at = c(-20, -10, 0, 10, 20), legend_width = unit(10, "cm"),
      x = unit(0.5, "cm"), y = unit(0.5, "cm"),
      legend_height = unit(5, "cm"))

lgd_list <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12), 
         title = "log-normalized FDR", 
       border = "black", lwd = 1, direction = "horizontal", title_position = "topcenter",
       at = c(-10, -5, 0, 5, 10), legend_width = unit(10, "cm"),
       x = unit(0.5, "cm"), y = unit(0.5, "cm"),
      legend_height = unit(5, "cm"))
     
     
col_fun = colorRamp2(c(-10, 0, 10), c("lightblue3", "antiquewhite", "#8c3800"))

##Let's create top annotation #1
da.top.1 <- da_taxa %>%
  group_by(value) %>%
  summarise(Prokaryote = sum(taxtype == "Prokaryote"),
            Eukaryote = sum(taxtype == "Eukaryote"))

da.top.1 <- as.data.frame(da.top.1)

##Rename R3 to R2
da.top.1$value[da.top.1$value == "R3 vs V1"] <- "R2 vs V1"
da.top.1$value[da.top.1$value == "R3 vs V6"] <- "R2 vs V6"
da.top.1$value[da.top.1$value == "R6 vs R3"] <- "R6 vs R2"


da.top.1 <- da.top.1[order(factor(da.top.1$value, levels = c('V6 vs V1', 'R2 vs V1', 'R6 vs V1', 'R2 vs V6', 
                              'R6 vs V6', 'R6 vs R2', 'CZ4979X vs CZ4810X', 'Biostimulant vs Control'))), ]

rownames(da.top.1) <- da.top.1[, 1]
da.top.1 <- da.top.1[, -1]

da.heat.top.1 = HeatmapAnnotation("# unique obs"= anno_barplot(da.top.1, gp = gpar(fill = c("#a8c0a8","#a890a8"))), 
                               which = c("column"), show_annotation_name = FALSE)

##Let's create top annotation #2
##Need to get phylum info separately since da analysis was at different levels
da.fun.tax <- as.data.frame(tax_table(pruned_norm_real_mas.fun))
da.fun.tax.1 <- da_fun_all
da.fun.tax.1 <- da.fun.tax.1 %>% rename(genus = feature)
da.fun.tax.2 <- merge(da.fun.tax, da.fun.tax.1, by = c("genus"))

##prok
da.prok.tax <- as.data.frame(tax_table(pruned_norm_real_mas))
da.prok.tax.1 <- da_prok_all
##Modify names
da.prok.tax.1$feature <- str_replace(da.prok.tax.1$feature, "[.]", " ")
da.prok.tax.1 <- da.prok.tax.1 %>% rename(species = feature)
da.prok.tax.2 <- merge(da.prok.tax, da.prok.tax.1, by = c("species"))

##rbind and cleanup
da.top.2 <- rbind(da.fun.tax.2, da.prok.tax.2)

da.top.2.1 <- da.top.2 %>%
  count(value, phylum) %>%
  pivot_wider(names_from = phylum, values_from = n, values_fill = 0)

da.top.2.1 <- da.top.2.1[order(factor(da.top.2.1$value, levels = c('V6 vs V1', 'R2 vs V1', 'R6 vs V1', 'R2 vs V6', 
              'R6 vs V6', 'R6 vs R2', 'CZ4979X vs CZ4810X', 'Biostimulant vs Control'))), ]

da.top.2.1 <- as.data.frame(da.top.2.1)
rownames(da.top.2.1) <- da.top.2.1[, 1]
da.top.2.1 <- da.top.2.1[, -1]

da.heat.top.2 = HeatmapAnnotation("# unique obs"= anno_barplot(da.top.2.1, gp = gpar(fill = c("#701c00", 
        "#8c3800", 
        "#c47000",
        "#c48c1c",
        "#e0a81c",
        "#e0c48c", 
        "#e0e0e0",
        "#8ca8c4", 
        "#708ca8",
        "#1c54a8",
        "#385438",
        "#38381c"))),  which = c("column"), show_annotation_name = FALSE)


da.top.3 <- HeatmapAnnotation("# unique obs.2"= anno_barplot(da.top.2.1, show_legend = TRUE, gp = gpar(fill = c("#701c00", 
        "#8c3800", 
        "#c47000",
        "#c48c1c",
        "#e0a81c",
        "#e0c48c", 
        "#e0e0e0",
        "#8ca8c4", 
        "#708ca8",
        "#1c54a8",
        "#385438",
        "#38381c"))), "# unique obs"= anno_barplot(da.top.1, gp = gpar(fill = c("#a8c0a8","#a890a8"))),
         which = c("column"), show_annotation_name = FALSE, show_legend = TRUE)


##Right annotation
# Calculate the number of non-NA values in each row
non_na_counts <- rowSums(!is.na(da_taxa_wide.2))
# Create the da.anno.right dataframe
da.anno.right <- data.frame(number = non_na_counts)

da.anno.right.2 <- HeatmapAnnotation("# net"= anno_barplot(da.anno.right, axis_param = list(side = "bottom"), 
                      gp = gpar(fill = c("lightblue3"), lwd = 0)), 
                       which = c("row"),   show_annotation_name = FALSE)


heatmap <- Heatmap(da_taxa_wide.2, cluster_columns = FALSE, cluster_rows = FALSE, border = TRUE, col=col_fun, na_col = "gray57",
        row_names_gp = gpar(fontface = "italic", fontsize = 10), rect_gp = gpar(col = "black", lwd = 1),
        column_names_gp = gpar(fontsize = 16),
        heatmap_legend_param = lgd_list, row_split = ann_2$Domain,
        left_annotation = rowAnn,
        #right_annotation = da.anno.right.2,
        row_gap = unit(c(2), "mm"),
        column_names_rot = 45,
        row_names_side = "right",
        #heatmap_width = unit(18, "cm"),
        row_title_gp = gpar(fontsize = 18), row_title = NULL, 
        top_annotation = da.heat.top.1) 
  
#heatmap.1 <- heatmap + da.anno.right.2

tiff("da_heatmap.4.tiff", width = 8, height = 15, units = "in", res = 900)
heatmap2<- draw(heatmap, heatmap_legend_side = "top", padding = unit(c(0, 8, 0, 8), "mm"))
dev.off()

 tiff("da_heatmap.4.tiff", width = 8, height = 15, units = "in", res = 900)
heatmap2
dev.off()

##Modify for export
export <- da.ann.function

export$feature <- rownames(export)

export.2 <- merge(export, da_taxa, by = c("feature"))

write.csv(export.2, "/Users/brett/Documents/da.taxa.csv", row.names = FALSE)
```

```{r}
##16S
##Core microbiome is important for plant fitness and microbiome structure-let's take a look
##Core taxa with microbiome package- modified from here: https://microbiome.github.io/tutorials/Core.html 

prok_rel <- microbiome::transform(prok_collapse, "compositional")

prok_rel_df <- as.data.frame(otu_table(prok_rel))
##prok_rel_df <- tibble::rownames_to_column(prok_rel_df, "core.taxa") ##named this to make merging easier in subsequent steps


##A full phyloseq object of the core microbiota is obtained as follows:
prok.core <- core(prok_rel, detection = 0, prevalence = .5)

##Retrieving the core taxa names from the phyloseq object:
core.taxa.df <- as.data.frame(tax_table(prok.core))

##Get dataframe with core taxa and their relative abundance
prok_core_taxa <- merge(prok_rel_df, core.taxa.df,
                          by = 'row.names')

prok_core_taxa.1 <- subset(prok_core_taxa, select = -c(1, 67:72))

##lets remove unmapped taxa
##prok_core_taxa.1  <-  prok_core_taxa.1[!(is.na(prok_core_taxa.1$species) | prok_core_taxa.1$species==""), ]

rownames(prok_core_taxa.1) <- prok_core_taxa.1$species
prok_core_taxa.1$species <- NULL

pro_core_taxa.meta <- prok_core_taxa.1 %>% rownames_to_column(var = "species") %>%
  subset(select = -c(2:65)) %>% mutate(taxtype = 'Prokaryote')
```

```{r}
##18S-ITS
##Core taxa with microbiome package- modified from here: https://microbiome.github.io/tutorials/Core.html 
fun_rel <- microbiome::transform(fun_decontam_norm_glom, "compositional")

fun_rel_df <- as.data.frame(otu_table(fun_rel))

##A full phyloseq object of the core microbiota is obtained as follows:
fun.core <- core(fun_rel, detection = 0, prevalence = .5)

##Retrieving the core taxa names from the phyloseq object:
core.taxa.df.fun <- as.data.frame(tax_table(fun.core))

##Get dataframe with core taxa and their relative abundance
fun_core_taxa <- merge(fun_rel_df, core.taxa.df.fun,
                          by = 'row.names')

fun_core_taxa.1 <- subset(fun_core_taxa, select = -c(1, 66:70, 72))

##lets remove unmapped taxa
fun_core_taxa.1  <-  fun_core_taxa.1[!(is.na(fun_core_taxa.1$genus) | fun_core_taxa.1$genus==""), ]

##Go from wide to long format and aggregate RA at genus level
fun_core_taxa.2 <- melt(fun_core_taxa.1, id.vars=c("genus"))

fun_core_taxa.3 <- fun_core_taxa.2 %>%                                     
  group_by(genus, variable) %>%
  dplyr::summarise(value = sum(value)) %>% 
  as.data.frame()

##back to long format
fun_core_taxa.wide <- spread(fun_core_taxa.3, variable, value)

rownames(fun_core_taxa.wide) <- fun_core_taxa.wide[,1]
fun_core_taxa.wide <- fun_core_taxa.wide[,-1]

col_order.2 <- c("sample_1", "sample_2","sample_3", "sample_4", "sample_5", "sample_6", "sample_7", "sample_8", "sample_9", "sample_10",
"sample_11", "sample_12", "sample_13", "sample_14", "sample_15", "sample_16", "sample_17", "sample_18", "sample_19", "sample_20", "sample_21", "sample_22",
"sample_23", "sample_24", "sample_25", "sample_26", "sample_27", "sample_28", "sample_29", "sample_30", "sample_31", "sample_32", "sample_33", "sample_34",
"sample_35", "sample_36", "sample_37", "sample_38", "sample_39", "sample_40", "sample_41", "sample_42", "sample_43", "sample_44", "sample_45", "sample_46",
"sample_47", "sample_48", "sample_49", "sample_50", "sample_51", "sample_52", "sample_53", "sample_54", "sample_55", "sample_56", "sample_57", "sample_58",
"sample_59", "sample_60", "sample_61", "sample_62", "sample_63", "sample_64")

fun_core_taxa.wide <- fun_core_taxa.wide[, col_order.2]

core_taxa_wide <- rbind(fun_core_taxa.wide, prok_core_taxa.1)
```

```{r}
##plot relative abundance of core taxa
##Add annotation
##get tax type for each microbe
fun_core_taxa.meta <- fun_core_taxa.wide %>% rownames_to_column(var = "species") %>% ##calling genus species to facilitate rbind
  subset(select = -c(2:65)) %>% mutate(taxtype = 'Eukaryote')

##metadata file
core_meta <- rbind(fun_core_taxa.meta, pro_core_taxa.meta)

##get taxa in alphabetical order
core_ann <- core_meta[order(core_meta$species), ]
core_taxa_wide <- core_taxa_wide[order(row.names(core_taxa_wide)), ]

##Add all metadata
core.ann.function <- merge(core_ann, taxon.function.1, by.x = "species", by.y = "genus")
core.ann.function <- core.ann.function %>%
  mutate(
    `Lifestyle/Pathogenicity` = case_when(
      grepl("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", `Lifestyle/Pathogenicity`) & grepl("pathogen", `Lifestyle/Pathogenicity`) ~ gsub("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", "saprotroph", `Lifestyle/Pathogenicity`),
      grepl("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", `Lifestyle/Pathogenicity`) ~ "saprotroph",
      TRUE ~ `Lifestyle/Pathogenicity`
    ),
    `Lifestyle/Pathogenicity` = ifelse(
      grepl("saprotroph/.+pathogen|.+pathogen/saprotroph", `Lifestyle/Pathogenicity`),
      sapply(strsplit(`Lifestyle/Pathogenicity`, "/"), function(x) paste(sort(x), collapse="/")),
      `Lifestyle/Pathogenicity`
    ),
    Structure = ifelse(Structure %in% c("None", "other"), "unreported", Structure),
    Environment = case_when(
      Environment == "facultative anaerobe" ~ "anaerobe",
      Environment == "aerobe/anaerobe/facultative aerobe" ~ "aerobe/anaerobe",
      Environment == "facultative aerobe" ~ "aerobe",
      Environment == "aerobe/obligate aerobe" ~ "aerobe",
      Environment == "aerobe/facultative anaerobe" ~ "aerobe/anaerobe",
      Environment == "aerobe/facultative aerobe" ~ "aerobe",
      Environment == "anaerobe/obligate anaerobe" ~ "anaerobe",
      Environment == "obligate aerobe" ~ "aerobe",
      Environment == "freshwater" ~ "aquatic",
      TRUE ~ Environment
    )
  )

core.ann.function <- core.ann.function %>%
  mutate(`Lifestyle/Pathogenicity` = if_else(`Lifestyle/Pathogenicity` == "plant pathogen/unsepcified saprotroph",
                                             "plant pathogen/saprotroph",
                                             `Lifestyle/Pathogenicity`))

##row metadata
rownames(core.ann.function) <- core.ann.function$species
core.ann.function$species <- NULL
core.ann.function$taxtype <- NULL    
reversed_colnames <- names(unique_species.3) ##from unique taxon analysis below

# Reorder the columns in core.ann.function
core.ann.function <- core.ann.function %>%
  select(all_of(reversed_colnames))

rowAnn_2 <- HeatmapAnnotation(df = core.ann.function,
  which = 'row',
  col = colours, ##from unique taxon analysis below
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"), show_legend = FALSE) ##will change

##Column metadata
col_metadata <- samples.df.2 %>% rownames_to_column(var = "sample")
col_metadata$Treatment[col_metadata$Treatment == 'Prebiotic'] <- 'Biostimulant'
col_metadata$Developmental_Stage[col_metadata$Developmental_Stage == 'R3'] <- 'R2'

col_metadata$Developmental_Stage <- factor(col_metadata$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))

core.col.ann <- data.frame(col_metadata$Treatment, col_metadata$Cultivar, col_metadata$Developmental_Stage)
colnames(core.col.ann) <- c('Treatment', 'Cultivar', 'Growth Stage')

core.colours <- list('Treatment' = c('Control' = "#c3d6ce", 'Biostimulant' = "#c27668"),
  'Cultivar' = c('CZ4979X' = "#ba7233", 'CZ4810X' = "#ced1af"),
  'Growth Stage' =c('V1' = "#9b332b", 'V6' = "#697852", 'R2' = "#2b4655", 'R6' = "#a9845b"))

col_fun_2 = colorRamp2(c(0, 0.4311377),c("antiquewhite", "#8c3800"))

core.colAnn <- HeatmapAnnotation(df = core.col.ann,
  which = 'col',
  col = core.colours,
  annotation_width = unit(c(1, 2), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"), show_legend = FALSE) ##will change

col.2 <- scale_color_brewer(palette = "Purples")

lgd_list_2 <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12), 
          title = "Relative Abundance", 
        border = "black", lwd = 1, direction = "horizontal", title_position = "topcenter",
        at = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), legend_width = unit(10, "cm"),
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"))


core.heatmap <- Heatmap(core_taxa_wide, cluster_columns = TRUE, cluster_rows = TRUE, border = TRUE, col=col_fun_2,
       row_names_gp = gpar(fontface = "italic"), 
       ##right_annotation = rowAnn_2, 
       bottom_annotation = core.colAnn,
       show_column_names = FALSE, ##column_split = split, 
       row_names_side = "right",
       row_split = core_ann_2$Domain,
       row_gap = unit(c(2), "mm"),
        row_title_gp = gpar(fontsize = 18),
       heatmap_legend_param = lgd_list_2, row_title = NULL, left_annotation = rowAnn_2)

core.heatmap2<- draw(core.heatmap, heatmap_legend_side = "top", annotation_legend_side = "right", legend_grouping = "original")
                     
tiff("core.tax.2.tiff", width = 10, height = 5, units = "in", res = 900) ##increase height
core.heatmap2
dev.off()
```

```{r}
##16S
##Unique taxa
##https://schuyler-smith.github.io/phylosmith/graphics.html#taxa_core_graph 
#devtools::install_github('schuyler-smith/phylosmith')
#library(phylosmith)

##Treatment
##For treatment, we should use the phyloseq object excluding the baseline measurement
prok.unique.trt <- unique_taxa(pruned_norm_real_mas, treatment = "Treatment")

prok.unique.trt.1 <-data.frame(lapply(prok.unique.trt, "length<-", max(lengths(prok.unique.trt))))

prok.unique.trt.long <- gather(prok.unique.trt.1, Treatment, otu, 1:2, factor_key=TRUE)

prok.unique.trt.long.1 <- prok.unique.trt.long[complete.cases(prok.unique.trt.long$otu), ]

prok.unique.trt.long.2 <- merge(prok.unique.trt.long.1, comm.prok.maas, by = c("otu"))

##Now, a taxon may correspond to multple ASVs. We must use the unique ASV dataframe to find unique species
unique_species.treatment <- prok.unique.trt.long.2 %>%
  group_by(species) %>%
  summarise(has_biostimulant = any(Treatment == "Biostimulant"),
            has_control = any(Treatment == "Control")) %>%
  filter(has_biostimulant != has_control) %>%
  mutate(unique_treatment = ifelse(has_biostimulant, "Biostimulant", "Control")) %>%
  select(species, unique_treatment)

unique_species.treatment <- unique_species.treatment %>%
  rename(metric = unique_treatment) %>%
  mutate(taxtype = "Prokaryote")

##Cultivar
##For Cultivar and Growth Stage, we will use the full dataset
prok.unique.cult <- unique_taxa(pruned_prok_decontam_mas, treatment = "Cultivar")

prok.unique.cult.1 <-data.frame(lapply(prok.unique.cult, "length<-", max(lengths(prok.unique.cult))))

prok.unique.cult.long <- gather(prok.unique.cult.1, Cultivar, otu, 1:2, factor_key=TRUE)

prok.unique.cult.long.1 <- prok.unique.cult.long[complete.cases(prok.unique.cult.long$otu), ]

prok.unique.cult.long.2 <- merge(prok.unique.cult.long.1, comm.prok.maas, by = c("otu"))

unique_species.cult <- prok.unique.cult.long.2 %>%
  group_by(species) %>%
  summarise(has_CZ4979X = any(Cultivar == "CZ4979X"),
            has_CZ4810X = any(Cultivar == "CZ4810X")) %>%
  filter(has_CZ4979X != has_CZ4810X) %>%
  mutate(unique_cultivar = ifelse(has_CZ4979X, "CZ4979X", "CZ4810X")) %>%
  select(species, unique_cultivar)

unique_species.cult <- unique_species.cult %>%
  rename(metric = unique_cultivar) %>%
  mutate(taxtype = "Prokaryote")

##Developmental_Stage
prok.unique.ds <- unique_taxa(pruned_prok_decontam_mas, treatment = "Developmental_Stage")

prok.unique.ds.1 <-data.frame(lapply(prok.unique.ds, "length<-", max(lengths(prok.unique.ds))))

prok.unique.ds.long <- gather(prok.unique.ds.1, Developmental_Stage, otu, 1:4, factor_key=TRUE)

prok.unique.ds.long.1 <- prok.unique.ds.long[complete.cases(prok.unique.ds.long$otu), ]

prok.unique.ds.long.2 <- merge(prok.unique.ds.long.1, comm.prok.maas, by = c("otu"))

unique_species.ds <- prok.unique.ds.long.2 %>%
  group_by(species) %>%
  summarise(has_V1 = any(Developmental_Stage == "V1"),
            has_V6 = any(Developmental_Stage == "V6"),
            has_R3 = any(Developmental_Stage == "R3"),
            has_R6 = any(Developmental_Stage == "R6")) %>%
  filter((has_V1 & !has_V6 & !has_R3 & !has_R6) |
           (!has_V1 & has_V6 & !has_R3 & !has_R6) |
           (!has_V1 & !has_V6 & has_R3 & !has_R6) |
           (!has_V1 & !has_V6 & !has_R3 & has_R6)) %>%
  mutate(unique_stage = case_when(
    has_V1 & !has_V6 & !has_R3 & !has_R6 ~ "V1",
    !has_V1 & has_V6 & !has_R3 & !has_R6 ~ "V6",
    !has_V1 & !has_V6 & has_R3 & !has_R6 ~ "R3",
    !has_V1 & !has_V6 & !has_R3 & has_R6 ~ "R6"
  )) %>%
  select(species, unique_stage)

unique_species.ds <- unique_species.ds %>%
  rename(metric = unique_stage) %>%
  mutate(taxtype = "Prokaryote")

##combine all
unique.prok <- rbind(unique_species.treatment, unique_species.ds, unique_species.cult)
```

```{r}
##Unique taxa
##https://schuyler-smith.github.io/phylosmith/graphics.html#taxa_core_graph 
#devtools::install_github('schuyler-smith/phylosmith')
#library(phylosmith)

##Treatment
##For treatment, we should use the phyloseq object excluding the baseline measurement
fun.unique.trt <- unique_taxa(pruned_norm_real_mas.fun, treatment = "Treatment")

fun.unique.trt.1 <-data.frame(lapply(fun.unique.trt, "length<-", max(lengths(fun.unique.trt))))

fun.unique.trt.long <- gather(fun.unique.trt.1, Treatment, otu, 1:2, factor_key=TRUE)

fun.unique.trt.long.1 <- fun.unique.trt.long[complete.cases(fun.unique.trt.long$otu), ]

fun.unique.trt.long.2 <- merge(fun.unique.trt.long.1, fun.taxmat1, by = c("otu"))

##Now, a taxon may correspond to multple ASVs. We must use the unique ASV dataframe to find unique species
fun.unique_species.treatment <- fun.unique.trt.long.2 %>%
  group_by(genus) %>%
  summarise(has_biostimulant = any(Treatment == "Biostimulant"),
            has_control = any(Treatment == "Control")) %>%
  filter(has_biostimulant != has_control) %>%
  mutate(unique_treatment = ifelse(has_biostimulant, "Biostimulant", "Control")) %>%
  select(genus, unique_treatment)

fun.unique_species.treatment <- fun.unique_species.treatment %>%
  rename(metric = unique_treatment) %>%
  mutate(taxtype = "Eukaryote")

##Cultivar
##For Cultivar and Growth Stage, we will use the full dataset
fun.unique.cult <- unique_taxa(fun_decontam_norm_glom, treatment = "Cultivar")

fun.unique.cult.1 <-data.frame(lapply(fun.unique.cult, "length<-", max(lengths(fun.unique.cult))))

fun.unique.cult.long <- gather(fun.unique.cult.1, Cultivar, otu, 1:2, factor_key=TRUE)

fun.unique.cult.long.1 <- fun.unique.cult.long[complete.cases(fun.unique.cult.long$otu), ]

fun.unique.cult.long.2 <- merge(fun.unique.cult.long.1, fun.taxmat1, by = c("otu"))

fun.unique_species.cult <- fun.unique.cult.long.2 %>%
  group_by(genus) %>%
  summarise(has_CZ4979X = any(Cultivar == "CZ4979X"),
            has_CZ4810X = any(Cultivar == "CZ4810X")) %>%
  filter(has_CZ4979X != has_CZ4810X) %>%
  mutate(unique_cultivar = ifelse(has_CZ4979X, "CZ4979X", "CZ4810X")) %>%
  select(genus, unique_cultivar)

fun.unique_species.cult <- fun.unique_species.cult %>%
  rename(metric = unique_cultivar) %>%
  mutate(taxtype = "Eukaryote")

##Developmental_Stage
fun.unique.ds <- unique_taxa(fun_decontam_norm_glom, treatment = "Developmental_Stage")

fun.unique.ds.1 <-data.frame(lapply(fun.unique.ds, "length<-", max(lengths(fun.unique.ds))))

fun.unique.ds.long <- gather(fun.unique.ds.1, Developmental_Stage, otu, 1:4, factor_key=TRUE)

fun.unique.ds.long.1 <- fun.unique.ds.long[complete.cases(fun.unique.ds.long$otu), ]

fun.unique.ds.long.2 <- merge(fun.unique.ds.long.1, fun.taxmat1, by = c("otu"))

fun.unique_species.ds <- fun.unique.ds.long.2 %>%
  group_by(genus) %>%
  summarise(has_V1 = any(Developmental_Stage == "V1"),
            has_V6 = any(Developmental_Stage == "V6"),
            has_R3 = any(Developmental_Stage == "R3"),
            has_R6 = any(Developmental_Stage == "R6")) %>%
  filter((has_V1 & !has_V6 & !has_R3 & !has_R6) |
           (!has_V1 & has_V6 & !has_R3 & !has_R6) |
           (!has_V1 & !has_V6 & has_R3 & !has_R6) |
           (!has_V1 & !has_V6 & !has_R3 & has_R6)) %>%
  mutate(unique_stage = case_when(
    has_V1 & !has_V6 & !has_R3 & !has_R6 ~ "V1",
    !has_V1 & has_V6 & !has_R3 & !has_R6 ~ "V6",
    !has_V1 & !has_V6 & has_R3 & !has_R6 ~ "R3",
    !has_V1 & !has_V6 & !has_R3 & has_R6 ~ "R6"
  )) %>%
  select(genus, unique_stage)

fun.unique_species.ds <- fun.unique_species.ds %>%
  rename(metric = unique_stage) %>%
  mutate(taxtype = "Eukaryote")

##combine all
unique.fun <- rbind(fun.unique_species.treatment, fun.unique_species.ds, fun.unique_species.cult)

##Rename genus to species to facilitate rbind
unique.fun <- unique.fun %>%
  rename(species = genus)

unique_all <- rbind(unique.prok, unique.fun)

unique_all.2 <- unique_all
unique_all.2$num_unique <- table(unique_all.2$metric)[unique_all$metric]
unique_all.2$num_unique <- as.numeric(unique_all.2$num_unique)
```

```{r}
##Visualize unique taxa
unique_taxa_hm <- subset(unique_all, select = -c(3))
unique_taxa_hm$presence <- "1"

unique_taxa_wide <- spread(unique_taxa_hm, metric, presence)

##reorder columns
unique_taxa_wide.2 <- unique_taxa_wide[, c(1,8,9,6,7,5,4,3,2)] %>% column_to_rownames(var="species")

unique_taxa_wide.2[is.na(unique_taxa_wide.2)] <- 0

##transverse
unique.t <- t(unique_taxa_wide.2)
rownames(unique.t)[rownames(unique.t) == "R3"] <- "R2"

##Colors
binary_colors <- c("antiquewhite", "lightblue3")
binary_colors <- c("presence" = "lightblue3", "absence" = "antiquewhite")

##Column metadata
unique_all_sorted <- unique_all[order(unique_all$species), ]
unique_species <- unique_all_sorted[!duplicated(unique_all_sorted$species), ]
unique_species$metric <- NULL
unique_species$taxtype <- NULL

##Combine all functional info
unique_species.2 <- unique_species %>%
  inner_join(taxon.function.2, by = c("species" = "genus"))

unique_species.2 <- unique_species.2 %>%
  select(species, Domain, `Lifestyle/Pathogenicity`, Environment, Growth, Structure)

unique_species.3 <- as.data.frame(unique_species.2)
rownames(unique_species.3) <- unique_species.3$species
unique_species.3$species <- NULL
sapply(unique_species.3, unique)

##simplify lifestyle and structure for effective visualization
unique_species.3 <- unique_species.3 %>%
  mutate(
    `Lifestyle/Pathogenicity` = case_when(
      grepl("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", `Lifestyle/Pathogenicity`) & grepl("pathogen", `Lifestyle/Pathogenicity`) ~ gsub("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", "saprotroph", `Lifestyle/Pathogenicity`),
      grepl("(soil saprotroph|litter saprotroph|nectar/tap saprotroph|unspecified saprotroph|dung saprotroph|wood saprotroph|soil saprotroph/rock-inhabiting|pollen saprotroph)", `Lifestyle/Pathogenicity`) ~ "saprotroph",
      TRUE ~ `Lifestyle/Pathogenicity`
    ),
    `Lifestyle/Pathogenicity` = ifelse(
      grepl("saprotroph/.+pathogen|.+pathogen/saprotroph", `Lifestyle/Pathogenicity`),
      sapply(strsplit(`Lifestyle/Pathogenicity`, "/"), function(x) paste(sort(x), collapse="/")),
      `Lifestyle/Pathogenicity`
    ),
    Structure = ifelse(Structure %in% c("None", "other"), "unreported", Structure),
    Environment = case_when(
      Environment == "facultative anaerobe" ~ "anaerobe",
      Environment == "aerobe/anaerobe/facultative aerobe" ~ "aerobe/anaerobe",
      Environment == "facultative aerobe" ~ "aerobe",
      Environment == "aerobe/obligate aerobe" ~ "aerobe",
      Environment == "aerobe/facultative anaerobe" ~ "aerobe/anaerobe",
      Environment == "aerobe/facultative aerobe" ~ "aerobe",
      Environment == "anaerobe/obligate anaerobe" ~ "anaerobe",
      Environment == "obligate aerobe" ~ "aerobe",
      Environment == "freshwater" ~ "aquatic",
      TRUE ~ Environment
    )
  )


##Assign colors
colours <- list(
  'Structure' = c(
    setNames(natparks.pals("Yellowstone", 15), 
             c('gram-negative', 'unreported', 'agaricoid', 'perithecium', 'gram-positive',
               'apothecium', 'gram-negative/gram-positive', 'gasteroid', 'polyporoid',
               'cleistothecium', 'smut', 'corticioid', 'zoosporangium', 'phalloid', 'clathroid'))
  ),
  'Growth' = c(
    setNames(natparks.pals("BryceCanyon", 10), 
             c('mesophilic', 'filamentous mycelium', 'unreported', 'thallus photosynthetic',
               'mesophilic/thermophilic', 'mesophilic/psychrophilic', 
               'mesophilic/psychrophilic/thermophilic', 'psychrophilic/thermophilic',
               'psychrophilic', 'dimorphic yeast'))
  ),
  'Environment' = c(
    setNames(natparks.pals("Acadia", 7), 
             c('aerobe', 'unreported', 'partly aquatic', 'non-aquatic', 
               'aerobe/anaerobe', 'anaerobe', 'aquatic'))
  ),
  'Lifestyle/Pathogenicity' = c(
    setNames(natparks.pals("Olympic", 14), 
             c('human/animal pathogen', 'unreported', 'plant pathogen', 'saprotroph',
               'plant pathogen/saprotroph', 'animal pathogen/saprotroph', 'lichenized', 'plant pathogen/foliar endophyte', 
               'animal pathogen/animal decomposer', 'mycoparasite/fungal decomposer', 'mycoparasite/foliar endophyte',
               'arbuscular mycorrhizal/root-associated', 'animal pathogen/plant pathogen', 'algal pathogen/saprotroph'))
  ),
  'Domain' = c(
    'Prokaryote' = "#a8c0a8",
    'Eukaryote' = "#a890a8"
  )
)

unique_species.3 <- unique_species.3[, rev(colnames(unique_species.3))]

col.unique <- HeatmapAnnotation(df = unique_species.3,
  which = 'column',
  col = colours,
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"), show_legend = FALSE) ##will change for diff heatmaps

##Row metadata
row_meta <- unique_all.2[!duplicated(unique_all.2$metric), ]
row_meta <- select(row_meta, -1)
row_meta$metric <- gsub("R3", "R2", row_meta$metric)
row_meta <- row_meta %>%
  mutate(type = case_when(
    metric %in% c('Biostimulant', 'Control') ~ 'Treatment',
    metric %in% c('CZ4810X', 'CZ4979X') ~ 'Cultivar',
    metric %in% c('V1', 'V6', 'R2', 'R6') ~ 'Developmental_Stage',
    TRUE ~ NA_character_)) 

row_order <- factor(row_meta$metric, levels = c("V1", "V6", "R2", "R6", "CZ4979X", "CZ4810X", "Control", "Biostimulant"))

# Reorder the rows based on the custom factor variable
row_meta_ordered <- row_meta[order(row_order), ] %>% column_to_rownames(var="metric")

##Row metadata.2
unique_all.3 <- unique_all.2 %>%
  group_by(metric, taxtype) %>%
  summarize(count = n()) %>%
  pivot_wider(names_from = taxtype, values_from = count, values_fill = 0) %>%
  ungroup()

unique_all.3$metric <- gsub("R3", "R2", unique_all.3$metric)

unique_all.order <- factor(unique_all.3$metric, levels = c("V1", "V6", "R2", "R6", "CZ4979X", "CZ4810X", "Control", "Biostimulant"))

unique_all.4 <- unique_all.3[order(unique_all.order), ] %>% column_to_rownames(var="metric")

row.unique <- HeatmapAnnotation(df = row_meta_ordered,
  which = 'row',
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE)

unique_heat <- Heatmap(unique_taxa_wide.2, col=binary_colors, border = TRUE,
        row_split = unique_species$Domain, column_gap = unit(c(1), "mm"),
        column_split = row_meta_ordered$type,
        column_names_rot = 45,
        show_heatmap_legend = FALSE, 
        row_names_gp = grid::gpar(fontsize = 5, fontface = "italic"),
        row_title = NULL, column_title = NULL)

unique_heat2<- draw(unique_heat, legend_grouping = "original",  padding = unit(c(0, 8, 10, 0), "mm"))

tiff("unique.heat.2.tiff", width = 6, height = 25, units = "in", res = 900) 
unique_heat2
dev.off()

##barplot
column_ha3 = HeatmapAnnotation("# unique obs"= anno_barplot(row_meta_ordered$num_unique, gp = gpar(fill = "#2b465570")), 
                                which = c("row"))

##stacked barplot
column_ha4 = HeatmapAnnotation("# unique obs"= anno_barplot(unique_all.4, axis_param = list(side = "top"), gp = gpar(fill = c("#a8c0a8", "#a890a8"))), 
                               which = c("row"), show_annotation_name = FALSE)

##Replace 0 and 1 with absence and presence, respectively
unique.t.2 <- unique.t

# Replace 0 with "absence" and 1 with "presence"
unique.t.2[unique.t == 0] <- "absence"
unique.t.2[unique.t == 1] <- "presence"

lgd_list.unique <- list(
        border = "black", lwd = 1,
         legend_width = unit(10, "cm"),
        direction = "horizontal",
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"),
        nrow = 1,
        title = "")
        
##opposite orientation
unique_heat.1 <- Heatmap(unique.t.2, col=binary_colors, border = TRUE,
        column_split = unique_species.3$Domain, 
        row_gap = unit(c(1), "mm"),
        rect_gp = gpar(col = "black", lwd = 0.1),
        row_split = row_meta_ordered$type,
       #row_names_rot = 45,
        show_heatmap_legend = TRUE, 
        row_title = NULL, show_column_names = FALSE,
       row_names_side = "left", row_names_gp = gpar(fontsize = 16),
       heatmap_legend_param = lgd_list.unique,
       column_title = NULL, top_annotation = col.unique) 
       

unique_heat3 <- unique_heat.1 + column_ha4

unique_heat4 <- draw(unique_heat3, heatmap_legend_side = "top", 
                     annotation_legend_side = "bottom", 
                     legend_grouping = "original", padding = unit(c(0, 8, 5, 2), "mm"))

tiff("unique.heat.3.tiff", width = 14, height = 3, units = "in", res = 900) 
unique_heat4
dev.off()
```

```{r}
##Let's explore unique trends
##taxon.function.1
trends <- merge(taxon.function.1, unique_all, by.x = "genus", by.y = "species", all.y = TRUE)

write.csv(trends, "/Users/brett/Documents/trends.csv", row.names = FALSE)


# Summarize total number of rows for each value of metric
total_summary <- trends %>%
  group_by(metric) %>%
  summarise(total_count = n())

# Summarize rows where Domain equals "Prokaryote" or "Eukaryote" grouped by metric
domain_summary <- trends %>%
  filter(taxtype %in% c("Prokaryote", "Eukaryote")) %>%
  group_by(metric, taxtype) %>%
  summarise(domain_count = n())

# Merge the two summaries to get a comprehensive view
final_summary <- left_join(total_summary, domain_summary, by = "metric")

domain_summary <- unique_all %>%
  filter(taxtype %in% c("Prokaryote", "Eukaryote")) %>%
  group_by(metric, taxtype) %>%
  summarise(domain_count = n())
```

```{r}
##16S
##Let's find the total number of shared taxa as well to better contextualize the number of unique
shared.trt <- as.data.frame(tax_table(pruned_norm_real_mas))
shared.trt.1 <- shared.trt[!duplicated(shared.trt$species), ]

##Remove taxtype column
unique_species.treatment.2 <- unique_species.treatment[, -which(names(unique_species.treatment) == "taxtype")]

##Spread the values in the 'species' column into separate columns
trt.spread_df <- unique_species.treatment.2 %>%
  pivot_wider(names_from = species, values_from = species, values_fn = length, values_fill = 0)
#trt.spread_df <- subset(trt.spread_df, select = -c(presence))

##retain only shared taxa
##Get species exclusive to shared.trt.1
trt.exclusive_species <- setdiff(shared.trt.1$species, unique_species.treatment.2$species)

##Create shared.trt.2 dataframe with exclusive species
shared.trt.2 <- shared.trt.1[shared.trt.1$species %in% trt.exclusive_species, ]

##Remove specified columns
shared.trt.2 <- subset(shared.trt.2, select = -c(kingdom, phylum, class, order, family, genus))

##Generate all combinations of species and metric
trt.combinations <- expand.grid(species = unique(shared.trt.2$species), metric = c("Biostimulant", "Control"))

##Merge the combinations with the shared.trt.2 dataframe
shared.trt.2 <- merge(shared.trt.2, trt.combinations, by = "species", all = TRUE)

##Sort the dataframe by species
shared.trt.2 <- shared.trt.2[order(shared.trt.2$species), ]

# Spread the species column and fill with '1'
shared.trt.2 <- shared.trt.2 %>%
  pivot_wider(names_from = species, values_from = species, values_fn = length, values_fill = 1)

venn.trt.1 <- merge(shared.trt.2, trt.spread_df, by = c("metric"))

rownames(venn.trt.1) <- venn.trt.1[[1]]

# Remove the first column
venn.trt.1 <- venn.trt.1[, -1]
venn.trt.2 <- t(venn.trt.1)
venn.trt.2 <- as.data.frame(venn.trt.2)

sets <- list(
  Biostimulant = which(venn.trt.2$Biostimulant == 1),
  Control = which(venn.trt.2$Control == 1))

#ggVennDiagram(sets)

bac.trt <- ggvenn(
  sets, columns = c("Biostimulant", "Control"),
  stroke_size = 1, 
  fill_color = c("#c27668", "#c3d6ce"),
  text_size = 6,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = TRUE) +
  theme_minimal()+
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())

euler_plot <- euler(sets)

trt.venn <- plot(euler_plot, 
     fills = list(fill = c("#c27668", "#c3d6ce"), alpha = 0.5), 
     quantities = list(type = c("percent", "counts")),
     gpar(fontsize = 23)) +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())

ggsave("eular.test.tiff", width = 12, height = 8, units = "in", dpi = 900)

###Cultivar###
##Let's find the total number of shared taxa as well to better contextualize the number of unique
shared.cult <- as.data.frame(tax_table(pruned_prok_decontam_mas))
shared.cult.1 <- shared.cult[!duplicated(shared.cult$species), ]

##Remove taxtype column
unique_species.cult.2 <- unique_species.cult[, -which(names(unique_species.cult) == "taxtype")]

##Spread the values in the 'species' column into separate columns
cult.spread_df <- unique_species.cult.2 %>%
  pivot_wider(names_from = species, values_from = species, values_fn = length, values_fill = 0)
#cult.spread_df <- subset(cult.spread_df, select = -c(presence))

##retain only shared taxa
##Get species exclusive to shared.cult.1
cult.exclusive_species <- setdiff(shared.cult.1$species, unique_species.cult.2$species)

##Create shared.cult.2 dataframe with exclusive species
shared.cult.2 <- shared.cult.1[shared.cult.1$species %in% cult.exclusive_species, ]

##Remove specified columns
shared.cult.2 <- subset(shared.cult.2, select = -c(kingdom, phylum, class, order, family, genus))

##Generate all combinations of species and metric
cult.combinations <- expand.grid(species = unique(shared.cult.2$species), metric = c("CZ4979X", "CZ4810X"))

##Merge the combinations with the shared.cult.2 dataframe
shared.cult.2 <- merge(shared.cult.2, cult.combinations, by = "species", all = TRUE)

##Sort the dataframe by species
shared.cult.2 <- shared.cult.2[order(shared.cult.2$species), ]

# Spread the species column and fill with '1'
shared.cult.2 <- shared.cult.2 %>%
  pivot_wider(names_from = species, values_from = species, values_fn = length, values_fill = 1)

venn.cult.1 <- merge(shared.cult.2, cult.spread_df, by = c("metric"))

rownames(venn.cult.1) <- venn.cult.1[[1]]

# Remove the first column
venn.cult.1 <- venn.cult.1[, -1]
venn.cult.2 <- t(venn.cult.1)
venn.cult.2 <- as.data.frame(venn.cult.2)

cult.sets <- list(
  CZ4979X = which(venn.cult.2$CZ4979X == 1),
  CZ4810X = which(venn.cult.2$CZ4810X == 1))

bac.cult <- ggvenn(
  cult.sets, columns = c("CZ4979X", "CZ4810X"),
  stroke_size = 1, 
  fill_color = c("#ba7233", "#ced1af"),
  text_size = 6,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = TRUE) +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())

###ds###
##Let's find the total number of shared taxa as well to better contextualize the number of unique
#shared.ds <- as.data.frame(tax_table(prok_decontam_norm))
shared.ds <- as.data.frame(tax_table(pruned_prok_decontam_mas))
shared.ds.1 <- shared.ds[!duplicated(shared.ds$species), ]

##Remove taxtype column
unique_species.ds.2 <- unique_species.ds[, -which(names(unique_species.ds) == "taxtype")]

##Spread the values in the 'species' column into separate columns
ds.spread_df <- unique_species.ds.2 %>%
  pivot_wider(names_from = species, values_from = species, values_fn = length, values_fill = 0)
#ds.spread_df <- subset(ds.spread_df, select = -c(presence))

##retain only shared taxa
##Get species exclusive to shared.trt.1
ds.exclusive_species <- setdiff(shared.ds.1$species, unique_species.ds.2$species)

##Create shared.ds.2 dataframe with exclusive species
shared.ds.2 <- shared.ds.1[shared.ds.1$species %in% ds.exclusive_species, ]

##Remove specified columns
shared.ds.2 <- subset(shared.ds.2, select = -c(kingdom, phylum, class, order, family, genus))

##Generate all combinations of species and metric
ds.combinations <- expand.grid(species = unique(shared.ds.2$species), metric = c("V1", "V6", "R3", "R6"))

##Merge the combinations with the shared.ds.2 dataframe
shared.ds.2 <- merge(shared.ds.2, ds.combinations, by = "species", all = TRUE)

##Sort the dataframe by species
shared.ds.2 <- shared.ds.2[order(shared.ds.2$species), ]

# Spread the species column and fill with '1'
shared.ds.2 <- shared.ds.2 %>%
  pivot_wider(names_from = species, values_from = species, values_fn = length, values_fill = 1)

venn.ds.1 <- merge(shared.ds.2, ds.spread_df, by = c("metric"))

rownames(venn.ds.1) <- venn.ds.1[[1]]

# Remove the first column
venn.ds.1 <- venn.ds.1[, -1]
venn.ds.2 <- t(venn.ds.1)
venn.ds.2 <- as.data.frame(venn.ds.2)

ds.sets <- list(
  V1 = which(venn.ds.2$V1 == 1),
  V6 = which(venn.ds.2$V6 == 1),
  R2 = which(venn.ds.2$R3 == 1),
  R6 = which(venn.ds.2$R6 == 1))
  
bac.ds <- ggvenn(
  ds.sets, columns = c("V1", "V6", "R2", "R6"),
  stroke_size = 1, 
  fill_color = c("#9b332b", "#697852", "#2b4655", "#a9845b"),
  text_size = 5,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = FALSE) +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())

ds.euler_plot <- euler(ds.sets)

plot(ds.euler_plot, 
     fills = list(fill = c("#9b332b", "#697852", "#2b4655", "#a9845b"), alpha = 0.5), 
     quantities = list(type = c("percent", "counts")),
     gpar(fontsize = 23),
     shape = "ellipse")

ds.venn.euler <- venn(ds.sets)

plot(ds.venn.euler, 
     fills = list(fill = c("#9b332b", "#697852", "#2b4655", "#a9845b"), alpha = 0.5), 
     quantities = list(type = c("percent", "counts")),
     gpar(fontsize = 23),
     shape = "ellipse")
```

```{r}
##18S-ITS
##Let's find the total number of shared taxa as well to better contextualize the number of unique
##Will keep naming species for consistency, although we are investigating genera

fun.shared.trt <- as.data.frame(tax_table(pruned_norm_real_mas.fun))
fun.shared.trt.1 <- fun.shared.trt[!duplicated(fun.shared.trt$genus), ]

##Remove taxtype column
fun.unique_species.treatment.2 <- fun.unique_species.treatment[, -which(names(fun.unique_species.treatment) == "taxtype")]

##Spread the values in the 'genus' column into separate columns
fun.trt.spread_df <- fun.unique_species.treatment.2 %>%
  pivot_wider(names_from = genus, values_from = genus, values_fn = length, values_fill = 0)
#fun.trt.spread_df <- subset(trt.spread_df, select = -c(presence))

##retain only shared taxa
##Get genus exclusive to shared.trt.1
fun.trt.exclusive_species <- setdiff(fun.shared.trt.1$genus, fun.unique_species.treatment.2$genus)

##Create shared.trt.2 dataframe with exclusive genus
fun.shared.trt.2 <- fun.shared.trt.1[fun.shared.trt.1$genus %in% fun.trt.exclusive_species, ]

##Remove specified columns
fun.shared.trt.2 <- subset(fun.shared.trt.2, select = -c(kingdom, phylum, class, order, family, species))

##Generate all combinations of genus and metric
fun.trt.combinations <- expand.grid(genus = unique(fun.shared.trt.2$genus), metric = c("Biostimulant", "Control"))

##Merge the combinations with the shared.trt.2 dataframe
fun.shared.trt.2 <- merge(fun.shared.trt.2, fun.trt.combinations, by = "genus", all = TRUE)

##Sort the dataframe by genus
fun.shared.trt.2 <- fun.shared.trt.2[order(fun.shared.trt.2$genus), ]

# Spread the genus column and fill with '1'
fun.shared.trt.2 <- fun.shared.trt.2 %>%
  pivot_wider(names_from = genus, values_from = genus, values_fn = length, values_fill = 1)

##Further modification is needed since no taxa are unique to the Biostimulant treatment
# Create a new row with 'Biostimulant'
new_row <- c('Biostimulant', rep(0, ncol(fun.trt.spread_df) - 1))
# Bind the new row to the dataframe
fun.trt.spread_df <- rbind(fun.trt.spread_df, new_row)

fun.venn.trt.1 <- merge(fun.shared.trt.2, fun.trt.spread_df, by = c("metric"))

rownames(fun.venn.trt.1) <- fun.venn.trt.1[[1]]

# Remove the first column
fun.venn.trt.1 <- fun.venn.trt.1[, -1]
fun.venn.trt.2 <- t(fun.venn.trt.1)
fun.venn.trt.2 <- as.data.frame(fun.venn.trt.2)

fun.sets <- list(
  Biostimulant = which(fun.venn.trt.2$Biostimulant == 1),
  Control = which(fun.venn.trt.2$Control == 1))

fun.trt <- ggvenn(
  fun.sets, columns = c("Biostimulant", "Control"),
  stroke_size = 1, 
  fill_color = c("#c27668", "#c3d6ce"),
  text_size = 6,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = TRUE) +
  theme_minimal()+
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) +
  scale_x_continuous(expand = expansion(mult = .12))

ggsave("fun.trt.venn.tiff", width = 10, height = 4, units = "in", dpi = 900)

###Cultivar###
##Let's find the total number of shared taxa as well to better contextualize the number of unique
fun.shared.cult <- as.data.frame(tax_table(fun_decontam_norm_glom))
fun.shared.cult.1 <- fun.shared.cult[!duplicated(fun.shared.cult$genus), ]

##Remove taxtype column
fun.unique_species.cult.2 <- fun.unique_species.cult[, -which(names(fun.unique_species.cult) == "taxtype")]

##Spread the values in the 'species' column into separate columns
fun.cult.spread_df <- fun.unique_species.cult.2 %>%
  pivot_wider(names_from = genus, values_from = genus, values_fn = length, values_fill = 0)
#fun.cult.spread_df <- subset(cult.spread_df, select = -c(presence))

##retain only shared taxa
##Get species exclusive to shared.trt.1
fun.cult.exclusive_species <- setdiff(fun.shared.cult.1$genus, fun.unique_species.cult.2$genus)

##Create shared.cult.2 dataframe with exclusive species
fun.shared.cult.2 <- fun.shared.cult.1[fun.shared.cult.1$genus %in% fun.cult.exclusive_species, ]

##Remove specified columns
fun.shared.cult.2 <- subset(fun.shared.cult.2, select = -c(kingdom, phylum, class, order, family, species))

##Generate all combinations of species and metric
fun.cult.combinations <- expand.grid(genus = unique(fun.shared.cult.2$genus), metric = c("CZ4979X", "CZ4810X"))

##Merge the combinations with the shared.cult.2 dataframe
fun.shared.cult.2 <- merge(fun.shared.cult.2, fun.cult.combinations, by = "genus", all = TRUE)

##Sort the dataframe by species
fun.shared.cult.2 <- fun.shared.cult.2[order(fun.shared.cult.2$genus), ]

# Spread the species column and fill with '1'
fun.shared.cult.2 <- fun.shared.cult.2 %>%
  pivot_wider(names_from = genus, values_from = genus, values_fn = length, values_fill = 1)

fun.venn.cult.1 <- merge(fun.shared.cult.2, fun.cult.spread_df, by = c("metric"))

rownames(fun.venn.cult.1) <- fun.venn.cult.1[[1]]

# Remove the first column
fun.venn.cult.1 <- fun.venn.cult.1[, -1]
fun.venn.cult.2 <- t(fun.venn.cult.1)
fun.venn.cult.2 <- as.data.frame(fun.venn.cult.2)

fun.cult.sets <- list(
  CZ4979X = which(fun.venn.cult.2$CZ4979X == 1),
  CZ4810X = which(fun.venn.cult.2$CZ4810X == 1))

fun.cult <- ggvenn(
  fun.cult.sets, columns = c("CZ4979X", "CZ4810X"),
  stroke_size = 1, 
  fill_color = c("#ba7233", "#ced1af"),
  text_size = 5,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = TRUE) +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())

###ds###
##Let's find the total number of shared taxa as well to better contextualize the number of unique
fun.shared.ds <- as.data.frame(tax_table(fun_decontam_norm_glom))
fun.shared.ds.1 <- fun.shared.ds[!duplicated(fun.shared.ds$genus), ]

##Remove taxtype column
fun.unique_species.ds.2 <- fun.unique_species.ds[, -which(names(fun.unique_species.ds) == "taxtype")]

##Spread the values in the 'species' column into separate columns
fun.ds.spread_df <- fun.unique_species.ds.2 %>%
  pivot_wider(names_from = genus, values_from = genus, values_fn = length, values_fill = 0)
#ds.spread_df <- subset(ds.spread_df, select = -c(presence))

##retain only shared taxa
##Get species exclusive to shared.trt.1
fun.ds.exclusive_species <- setdiff(fun.shared.ds.1$genus, fun.unique_species.ds.2$genus)

##Create shared.ds.2 dataframe with exclusive species
fun.shared.ds.2 <- fun.shared.ds.1[fun.shared.ds.1$genus %in% fun.ds.exclusive_species, ]

##Remove specified columns
fun.shared.ds.2 <- subset(fun.shared.ds.2, select = -c(kingdom, phylum, class, order, family, species))

##Generate all combinations of species and metric
fun.ds.combinations <- expand.grid(genus = unique(fun.shared.ds.2$genus), metric = c("V1", "V6", "R3", "R6"))

##Merge the combinations with the shared.ds.2 dataframe
fun.shared.ds.2 <- merge(fun.shared.ds.2, fun.ds.combinations, by = "genus", all = TRUE)

##Sort the dataframe by species
fun.shared.ds.2 <- fun.shared.ds.2[order(fun.shared.ds.2$genus), ]

# Spread the species column and fill with '1'
fun.shared.ds.2 <- fun.shared.ds.2 %>%
  pivot_wider(names_from = genus, values_from = genus, values_fn = length, values_fill = 1)

fun.venn.ds.1 <- merge(fun.shared.ds.2, fun.ds.spread_df, by = c("metric"))

rownames(fun.venn.ds.1) <- fun.venn.ds.1[[1]]

# Remove the first column
fun.venn.ds.1 <- fun.venn.ds.1[, -1]
fun.venn.ds.2 <- t(fun.venn.ds.1)
fun.venn.ds.2 <- as.data.frame(fun.venn.ds.2)

fun.ds.sets <- list(
  V1 = which(fun.venn.ds.2$V1 == 1),
  V6 = which(fun.venn.ds.2$V6 == 1),
  R2 = which(fun.venn.ds.2$R3 == 1),
  R6 = which(fun.venn.ds.2$R6 == 1))
  
fun.ds <-ggvenn(
  fun.ds.sets, columns = c("V1", "V6", "R2", "R6"),
  stroke_size = 1, 
  fill_color = c("#9b332b", "#697852", "#2b4655", "#a9845b"),
  text_size = 5,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = FALSE) +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())
 
venn.attempt <- (bac.trt | bac.cult | bac.ds) /
(fun.trt | fun.cult |fun.ds)

tiff("venn.attempt.tiff", width = 12, height = 7, units = "in", res = 900) 
venn.attempt
dev.off()
```

```{r}
##Co-occurrence network analysis. This will be performed at genus level and span eukaryotes and prokaryotes.
##with phylosmith
##Global network
library(qgraph)
library(igraph)
library(ggClusterNet)
library(phylosmith)

net.col <- list ("#a8c0a8",
        "#c0c0a8",
        "#786060",
        "#a8a890",
        "#c07848",
        "#c0c0c0",
        "#d8c0a8",
        "#a890a8",
        "#c09048")

net.col.2 <- list ("#a8c0a8",
        "#786060",
        "#a8a890",
        "#c07848",
        "#c0c0c0",
        "#d8c0a8",
        "#a890a8")

olsen_seq = c(
        "#701c00", 
        "#8c3800", 
        "#a85400", 
        "#c47000",
        "#c48c1c",
        "#e0a81c", 
        "#e0c48c", 
        "#e0e0e0",
        "#c4c4c4",
        "#8ca8c4", 
        "#708ca8",
        "#1c54a8",
        "#385438",
        "#38381c")

test <- as.data.frame(sampleData(merged_phylo))
##merge phyloseq objects- will use those that have V1 set to control

merged_phylo <- merge_phyloseq(prok_decontam_norm, fun_decontam_filter_norm)
##merged_phylo.1 <- taxa_filter(merged_phylo, frequency = 0.2)

filtered_obj <- conglomerate_taxa(merged_phylo, "genus")
merged_phylo.tax <- as.data.frame(tax_table(merged_phylo))

table.sp <- co_occurrence(filtered_obj, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)

##fdr correction and filtering
table.sp$p <-p.adjust(table.sp$p, "fdr")
table.sp <- filter(table.sp,p <= 0.05)

##network_layout_ps function
layout.sp <- network_layout_ps(filtered_obj,
treatment = NULL, subset = NULL, co_occurrence_table = table.sp,
algorithm = 'sphere')

##network_ps function 
global.net.sp <- network_ps(filtered_obj, treatment = NULL, subset = NULL, co_occurrence_table = table.sp, rho = 0.6)

edge_col <- list("gray22", "tomato3")
edge_col <- adjust_transparency(edge_col, alpha = 0.1)
edge_col.2 <- adjust_transparency(edge_col, alpha = 0.5)
edge_col.3 <- adjust_transparency(edge_col.2, alpha = 0.1)
  
##Network properties- ggclusternet package
net_prop.global.sp <- net_properties(global.net.sp)
node_prop.global.sp <- node_properties(global.net.sp)
hub_score.global.sp <- hub_score(global.net.sp, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.global.sp <- authority_score(global.net.sp, scale = TRUE, weights = NULL, options = arpack_defaults)
eigen_centrality.global.sp <- eigen_centrality(global.net.sp)

##package influential
neighbor.global.sp <- neighborhood.connectivity(global.net.sp, vertices = V(global.net.sp), mode = "all")
neighbor.global.sp <- as.data.frame(neighbor.global.sp)

##check <- as.data.frame(hub_score.global)
##check <- tibble::rownames_to_column(check, "genus") 

##Extract top 10 hubs
check.1 <- check %>% arrange(desc(vector)) %>%
    slice(1:20) 

check.2 <- merge(merged_phylo.tax, check.1, by = c("genus"))
check.2 <- check.2[!duplicated(check.2$genus), ]  

##co-occurence function
##Viz 1
co_occurrence_network(filtered_obj, co_occurrence_table = table.sp,
                      classification = "kingdom",  node_colors = net.col,
                      cluster = FALSE, ##layout = layout.sp,
  negative_positive_colors = c('gray22', 'tomato3')) +
  guides(fill=guide_legend(nrow=2))

  theme(legend.position = 'none')

ggsave("global.net.sp.tiff", width = 10, height = 10, units = "in", dpi = 900)

##Viz 2
global.net <- co_occurrence_network(filtered_obj, co_occurrence_table = table.sp,
                      classification = "kingdom",  node_colors = net.col,
                      cluster = FALSE, layout = layout.sp,
  negative_positive_colors = edge_col) +
  theme(legend.position = 'none')

tiff("global.net.tiff", width = 4, height = 4, units = "in", res = 900)
global.net
dev.off()
  
##https://link.springer.com/protocol/10.1007/978-1-4939-8728-3_16 
# Degrees
deg <- degree(global.net, mode = "all")
# Degree distribution
deg.dist <- degree_distribution(global.net, mode = "all", cumulative = T)

# Plot degree distribution
plot(deg.dist, xlab = "Nodes degree", ylab = "Probability", col = "black", pch = 21, bg="#7f793c", cex=2, 
     main="Degree distribution",  cex.main=2, cex.lab=1.7, cex.sub=1.2)
lines(deg.dist)
text(perfor, policy, canton, pos=1)
grid()

# qgraph method
centralityPlot(global.net) +
  theme(axis.text.y = element_blank())
```

```{r}
##Let's make a stacked barplot to improve visualization of the global network
# Create a new dataframe by stacking Y below X
net.bar <- data.frame(genus = c(table.sp$X, table.sp$Y))

##Create global taxonoomy file
prok.tax <- as.data.frame(tax_table(prok_decontam_norm))
fun.tax <- as.data.frame(tax_table(fun_decontam_filter_norm))
global.tax <- rbind(prok.tax, fun.tax)
global.tax.1 <- distinct(global.tax, genus, .keep_all = TRUE)

##get taxonomy info for network members
net.bar.1 <- merge(net.bar, global.tax.1, by = c("genus"))

##Create plot
ggplot(net.bar.1, aes(x = factor(1), fill = kingdom)) +
  geom_bar(color = "black", width = 0.05) +
  labs(x = NULL, y = "Frequency") +
  scale_fill_manual(values=net.col.2, name = "Kingdom") +
  theme_classic() +
  theme(legend.position = "right", legend.title = element_text(face = "bold")) +
  coord_flip()

net.bar.1_summary <- net.bar.1 %>%
  group_by(kingdom) %>%
  summarise(frequency = n()) %>%
  mutate(percentage = frequency / sum(frequency) * 100,
         label_position = ifelse(kingdom %in% c("Alveolata", "Bacteria", "Heterolobosa", "Stamenopila"), "above", "below"))

# Create the stacked bar plot
ggplot(net.bar.1_summary, aes(x = factor(1), y = percentage, fill = kingdom)) +
  geom_bar(stat = "identity", width = 0.1, color = "black", size = 1) +
  geom_text(data = subset(net.bar.1_summary, label_position == "above"),
            aes(label = paste0(round(percentage, 1), "%")),
            position = position_stack(vjust = 0.8),
            color = "black", fontface = "bold", vjust = -2, size = 5) +
  geom_text(data = subset(net.bar.1_summary, label_position == "below"),
            aes(label = paste0(round(percentage, 1), "%")),
            position = position_stack(vjust = 2),
            color = "black", fontface = "bold", vjust = 3, size = 5) +
  labs(x = NULL, y = "Percentage") +
  scale_fill_manual(values = net.col.2, name = "Kingdom") +
  theme_classic() +
  coord_flip() +
  theme(legend.position = "right", legend.title = element_text(face = "bold")) 
 
##Modify legend
ggplot(net.bar.1_summary, aes(x = factor(1), y = percentage, fill = kingdom)) +
  geom_bar(stat = "identity", width = 0.1, color = "black", size = 1) +
  geom_text(data = subset(net.bar.1_summary, label_position == "above"),
            aes(label = paste0(round(percentage, 1), "%")),
            position = position_stack(vjust = 0.8),
            color = "black", fontface = "bold", vjust = -2, size = 5) +
  geom_text(data = subset(net.bar.1_summary, label_position == "below"),
            aes(label = paste0(round(percentage, 1), "%")),
            position = position_stack(vjust = 2),
            color = "black", fontface = "bold", vjust = 3, size = 5) +
  labs(x = NULL, y = "Percentage") +
  scale_fill_manual(values = net.col.2, name = "Kingdom") +
  theme_classic() +
  coord_flip() +
  theme(legend.position = "bottom", 
        legend.box.just = "center", 
        legend.justification = "top",
        legend.title = element_text(face = "bold", margin = margin(0, 0, 10, 0))) +
  guides(fill = guide_legend(nrow = 1),title.position = "top")

ggsave("net.bar.tiff", width = 7, height = 3, units = "in", dpi = 900)
```

```{r}
##V1
##CZ4810X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V1.10X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.10X.ctrl <- prune_samples(sample_data(V1.10X.ctrl)$Treatment == "Control", V1.10X.ctrl)
V1.10X.ctrl <- prune_samples(sample_data(V1.10X.ctrl)$Cultivar == "CZ4810X", V1.10X.ctrl)

V1.10X.ctrl.table <- co_occurrence(V1.10X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V1.10X.ctrl.table$p <-p.adjust(V1.10X.ctrl.table$p, "fdr")
V1.10X.ctrl.table <- filter(V1.10X.ctrl.table,p <= 0.05)

##network_layout_ps function
V1.10X.ctrl.layout <- network_layout_ps(V1.10X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.10X.ctrl, co_occurrence_table = V1.10X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V1.10X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("V1.10X.ctrl.net.tiff", width = 5, height = 5, units = "in", dpi = 900)

##network_ps function 
V1.10X.ctrl.net <- network_ps(V1.10X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.10X.ctrl <- net_properties(V1.10X.ctrl.net)
node_prop.V1.10X.ctrl <- node_properties(V1.10X.ctrl.net)
hub_score.V1.10X.ctrl <- hub_score(V1.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.10X.ctrl <- authority_score(V1.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.10X.ctrl <- neighborhood.connectivity(V1.10X.ctrl.net, vertices = V(V1.10X.ctrl.net), mode = "all")
neighbor.V1.10X.ctrl <- as.data.frame(neighbor.V1.10X.ctrl)

##Network properties for comparison
net_prop.V1.10X.ctrl <- as.data.frame(net_prop.V1.10X.ctrl)
net_prop.V1.10X.ctrl <- tibble::rownames_to_column(net_prop.V1.10X.ctrl, "metric")
net_prop.V1.10X.ctrl <- net_prop.V1.10X.ctrl %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.10X.ctrl <- net_prop.V1.10X.ctrl %>% mutate(Treatment = 'Control')
net_prop.V1.10X.ctrl <- net_prop.V1.10X.ctrl %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V1.10X.ctrl <- as.data.frame(hub_score.V1.10X.ctrl)
hub_score.V1.10X.ctrl.1 <- filter(hub_score.V1.10X.ctrl,vector >= 0.2)
##9

net_prop.V1.10X.ctrl <- net_prop.V1.10X.ctrl %>% mutate(Hub_Count = '9')

##Add mean neighborhood connectivity
mean(neighbor.V1.10X.ctrl$neighbor.V1.10X.ctrl) ##2
net_prop.V1.10X.ctrl <- net_prop.V1.10X.ctrl %>% mutate(Mean_neighborhood = '2')

net_prop.V1.10X.ctrl.wide <-spread(net_prop.V1.10X.ctrl, metric, value)
net_prop.V1.10X.ctrl.final <- gather(net_prop.V1.10X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
V1.10X.ctrl.node.names <- as.data.frame(V(V1.10X.ctrl.net)$name)
V1.10X.ctrl.node.names <- V1.10X.ctrl.node.names %>% mutate(Developmental_Stage = 'V1')
V1.10X.ctrl.node.names <- V1.10X.ctrl.node.names %>% mutate(Treatment = 'Control')
V1.10X.ctrl.node.names <- V1.10X.ctrl.node.names %>% mutate(Cultivar = 'CZ4810X')
names(V1.10X.ctrl.node.names)[names(V1.10X.ctrl.node.names) == "V(V1.10X.ctrl.net)$name"] <- "genus"
V1.10X.ctrl.node.names <- V1.10X.ctrl.node.names %>% mutate(Condition = 'Condition 1')
hub_score.V1.10X.ctrl$genus <- rownames(hub_score.V1.10X.ctrl)
V1.10X.ctrl.heat <- merge(V1.10X.ctrl.node.names, hub_score.V1.10X.ctrl, by = c("genus"))
V1.10X.ctrl.heat <-subset(V1.10X.ctrl.heat, select = -c(8:27))

##Additional measurements
V1.10X.ctrl.giant_component_size <- max(components(V1.10X.ctrl.net)$csize) ##3
V1.10X.ctrl.giant_component_size <- as.data.frame(V1.10X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
V1.10X.ctrl.community <- cluster_walktrap(V1.10X.ctrl.net)

##Get the community membership vector and calculate modularity
V1.10X.ctrl.membership <- V1.10X.ctrl.community$membership
V1.10X.ctrl.modularity <- modularity(V1.10X.ctrl.community) ##0.6666667
V1.10X.ctrl.modularity <- as.data.frame(V1.10X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.10X.ctrl.degree_dist <- degree_distribution(V1.10X.ctrl.net)
# Step 2: Fit the power-law distribution
V1.10X.ctrl.pl_fit <- fit_power_law(V1.10X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) ##error
V1.10X.ctrl.pl_fit <- as.data.frame(V1.10X.ctrl.pl_fit)
##correct KS pvalue
V1.10X.ctrl.pl_fit$KS.p <- p.adjust(V1.10X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.2290335, ##KS.p = 0.9975114

##Add additional metrics to dataframe
net_prop.V1.10X.ctrl.wide.1 <-spread(net_prop.V1.10X.ctrl.final, metric, value)
net_prop.V1.10X.ctrl.wide.1 <- net_prop.V1.10X.ctrl.wide.1 %>% mutate(giant_component_size = '3')
net_prop.V1.10X.ctrl.wide.1 <- net_prop.V1.10X.ctrl.wide.1 %>% mutate(Modularity = '0.6666667')
##net_prop.V1.10X.ctrl.wide.1 <- net_prop.V1.10X.ctrl.wide.1 %>% mutate(KS.stat = '0.2290335')
##net_prop.V1.10X.ctrl.wide.1 <- net_prop.V1.10X.ctrl.wide.1 %>% mutate(KS.p = '0.9975114')
net_prop.V1.10X.ctrl.final <- gather(net_prop.V1.10X.ctrl.wide.1, metric, value, 4:21)
net_prop.V1.10X.ctrl.final <- net_prop.V1.10X.ctrl.final %>% mutate(Condition = 'Condition 1')
```

```{r}
##V6
##CZ4810X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V6.10X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.10X.ctrl <- prune_samples(sample_data(V6.10X.ctrl)$Treatment == "Control", V6.10X.ctrl)
V6.10X.ctrl <- prune_samples(sample_data(V6.10X.ctrl)$Cultivar == "CZ4810X", V6.10X.ctrl)

##Co-occurence table
V6.10X.ctrl.table <- co_occurrence(V6.10X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V6.10X.ctrl.table$p <-p.adjust(V6.10X.ctrl.table$p, "fdr")
V6.10X.ctrl.table <- filter(V6.10X.ctrl.table,p <= 0.05)

##network_layout_ps function
V6.10X.ctrl.layout <- network_layout_ps(V6.10X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.10X.ctrl, co_occurrence_table = V6.10X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V6.10X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V6.10X.ctrl.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V6.10X.ctrl.net <- network_ps(V6.10X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.10X.ctrl <- net_properties(V6.10X.ctrl.net)
node_prop.V6.10X.ctrl <- node_properties(V6.10X.ctrl.net)
hub_score.V6.10X.ctrl <- hub_score(V6.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.10X.ctrl <- authority_score(V6.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.10X.ctrl <- neighborhood.connectivity(V6.10X.ctrl.net, vertices = V(V6.10X.ctrl.net), mode = "all")
neighbor.V6.10X.ctrl <- as.data.frame(neighbor.V6.10X.ctrl)

##Network properties for comparison
net_prop.V6.10X.ctrl <- as.data.frame(net_prop.V6.10X.ctrl)
net_prop.V6.10X.ctrl <- tibble::rownames_to_column(net_prop.V6.10X.ctrl, "metric")
net_prop.V6.10X.ctrl <- net_prop.V6.10X.ctrl %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.10X.ctrl <- net_prop.V6.10X.ctrl %>% mutate(Treatment = 'Control')
net_prop.V6.10X.ctrl <- net_prop.V6.10X.ctrl %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V6.10X.ctrl <- as.data.frame(hub_score.V6.10X.ctrl)
hub_score.V6.10X.ctrl.1 <- filter(hub_score.V6.10X.ctrl,vector >= 0.2)
##4

net_prop.V6.10X.ctrl <- net_prop.V6.10X.ctrl %>% mutate(Hub_Count = '4')

##Add mean neighborhood connectivity
mean(neighbor.V6.10X.ctrl$neighbor.V6.10X.ctrl) ##3
net_prop.V6.10X.ctrl <- net_prop.V6.10X.ctrl %>% mutate(Mean_neighborhood = '3')

net_prop.V6.10X.ctrl.wide <-spread(net_prop.V6.10X.ctrl, metric, value)
net_prop.V6.10X.ctrl.final <- gather(net_prop.V6.10X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
V6.10X.ctrl.node.names <- as.data.frame(V(V6.10X.ctrl.net)$name)
V6.10X.ctrl.node.names <- V6.10X.ctrl.node.names %>% mutate(Developmental_Stage = 'V6')
V6.10X.ctrl.node.names <- V6.10X.ctrl.node.names %>% mutate(Treatment = 'Control')
V6.10X.ctrl.node.names <- V6.10X.ctrl.node.names %>% mutate(Cultivar = 'CZ4810X')
names(V6.10X.ctrl.node.names)[names(V6.10X.ctrl.node.names) == "V(V6.10X.ctrl.net)$name"] <- "genus"
V6.10X.ctrl.node.names <- V6.10X.ctrl.node.names %>% mutate(Condition = 'Condition 2')
hub_score.V6.10X.ctrl$genus <- rownames(hub_score.V6.10X.ctrl)
V6.10X.ctrl.heat <- merge(V6.10X.ctrl.node.names, hub_score.V6.10X.ctrl, by = c("genus"))
V6.10X.ctrl.heat <-subset(V6.10X.ctrl.heat, select = -c(8:27))

##Additional measurements
V6.10X.ctrl.giant_component_size <- max(components(V6.10X.ctrl.net)$csize) ##4
V6.10X.ctrl.giant_component_size <- as.data.frame(V6.10X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
V6.10X.ctrl.community <- cluster_walktrap(V6.10X.ctrl.net)

##Get the community membership vector and calculate modularity
V6.10X.ctrl.membership <- V6.10X.ctrl.community$membership
V6.10X.ctrl.modularity <- modularity(V6.10X.ctrl.community) ##0
V6.10X.ctrl.modularity <- as.data.frame(V6.10X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.10X.ctrl.degree_dist <- degree_distribution(V6.10X.ctrl.net)
# Step 2: Fit the power-law distribution
V6.10X.ctrl.pl_fit <- fit_power_law(V6.10X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) ##error
V6.10X.ctrl.pl_fit <- as.data.frame(V6.10X.ctrl.pl_fit)
##correct KS pvalue
V6.10X.ctrl.pl_fit$KS.p <- p.adjust(V6.10X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.2290335, ##KS.p = 0.9975114

##Add additional metrics to dataframe
net_prop.V6.10X.ctrl.wide.1 <-spread(net_prop.V6.10X.ctrl.final, metric, value)
net_prop.V6.10X.ctrl.wide.1 <- net_prop.V6.10X.ctrl.wide.1 %>% mutate(giant_component_size = '4')
net_prop.V6.10X.ctrl.wide.1 <- net_prop.V6.10X.ctrl.wide.1 %>% mutate(Modularity = '0')
##net_prop.V6.10X.ctrl.wide.1 <- net_prop.V6.10X.ctrl.wide.1 %>% mutate(KS.stat = '0.2290335')
##net_prop.V6.10X.ctrl.wide.1 <- net_prop.V6.10X.ctrl.wide.1 %>% mutate(KS.p = '0.9975114')
net_prop.V6.10X.ctrl.final <- gather(net_prop.V6.10X.ctrl.wide.1, metric, value, 4:21)
net_prop.V6.10X.ctrl.final <- net_prop.V6.10X.ctrl.final %>% mutate(Condition = 'Condition 2')
```

```{r}
##R3
##CZ4810X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R3.10X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.10X.ctrl <- prune_samples(sample_data(R3.10X.ctrl)$Treatment == "Control", R3.10X.ctrl)
R3.10X.ctrl <- prune_samples(sample_data(R3.10X.ctrl)$Cultivar == "CZ4810X", R3.10X.ctrl)

##Co-occurence table
R3.10X.ctrl.table <- co_occurrence(R3.10X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R3.10X.ctrl.table$p <-p.adjust(R3.10X.ctrl.table$p, "fdr")
R3.10X.ctrl.table <- filter(R3.10X.ctrl.table,p <= 0.05)

##network_layout_ps function
R3.10X.ctrl.layout <- network_layout_ps(R3.10X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R3.10X.ctrl, co_occurrence_table = R3.10X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R3.10X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2)+
  theme(legend.position = 'none')

ggsave("R3.10X.ctrl.ctrl.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R3.10X.ctrl.net <- network_ps(R3.10X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.10X.ctrl <- net_properties(R3.10X.ctrl.net)
node_prop.R3.10X.ctrl <- node_properties(R3.10X.ctrl.net)
hub_score.R3.10X.ctrl <- hub_score(R3.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.10X.ctrl <- authority_score(R3.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.10X.ctrl <- neighborhood.connectivity(R3.10X.ctrl.net, vertices = V(R3.10X.ctrl.net), mode = "all")
neighbor.R3.10X.ctrl <- as.data.frame(neighbor.R3.10X.ctrl)

##Network properties for comparison
net_prop.R3.10X.ctrl <- as.data.frame(net_prop.R3.10X.ctrl)
net_prop.R3.10X.ctrl <- tibble::rownames_to_column(net_prop.R3.10X.ctrl, "metric")
net_prop.R3.10X.ctrl <- net_prop.R3.10X.ctrl %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.10X.ctrl <- net_prop.R3.10X.ctrl %>% mutate(Treatment = 'Control')
net_prop.R3.10X.ctrl <- net_prop.R3.10X.ctrl %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R3.10X.ctrl <- as.data.frame(hub_score.R3.10X.ctrl)

hub_score.R3.10X.ctrl.1 <- filter(hub_score.R3.10X.ctrl,vector >= 0.2)
##7

net_prop.R3.10X.ctrl <- net_prop.R3.10X.ctrl %>% mutate(Hub_Count = '7')

##Add mean neighborhood connectivity
mean(neighbor.R3.10X.ctrl$neighbor.R3.10X.ctrl) ##3.923077
net_prop.R3.10X.ctrl <- net_prop.R3.10X.ctrl %>% mutate(Mean_neighborhood = '3.923077')
net_prop.R3.10X.ctrl.wide <-spread(net_prop.R3.10X.ctrl, metric, value)
net_prop.R3.10X.ctrl.final <- gather(net_prop.R3.10X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
R3.10X.ctrl.node.names <- as.data.frame(V(R3.10X.ctrl.net)$name)
R3.10X.ctrl.node.names <- R3.10X.ctrl.node.names %>% mutate(Developmental_Stage = 'R3')
R3.10X.ctrl.node.names <- R3.10X.ctrl.node.names %>% mutate(Treatment = 'Control')
R3.10X.ctrl.node.names <- R3.10X.ctrl.node.names %>% mutate(Cultivar = 'CZ4810X')
names(R3.10X.ctrl.node.names)[names(R3.10X.ctrl.node.names) == "V(R3.10X.ctrl.net)$name"] <- "genus"
R3.10X.ctrl.node.names <- R3.10X.ctrl.node.names %>% mutate(Condition = 'Condition 3')
hub_score.R3.10X.ctrl$genus <- rownames(hub_score.R3.10X.ctrl)
R3.10X.ctrl.heat <- merge(R3.10X.ctrl.node.names, hub_score.R3.10X.ctrl, by = c("genus"))
R3.10X.ctrl.heat <-subset(R3.10X.ctrl.heat, select = -c(8:27))

##Additional measurements
R3.10X.ctrl.giant_component_size <- max(components(R3.10X.ctrl.net)$csize) ##7
R3.10X.ctrl.giant_component_size <- as.data.frame(R3.10X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
R3.10X.ctrl.community <- cluster_walktrap(R3.10X.ctrl.net)

##Get the community membership vector and calculate modularity
R3.10X.ctrl.membership <- R3.10X.ctrl.community$membership
R3.10X.ctrl.modularity <- modularity(R3.10X.ctrl.community) ##0.7197232
R3.10X.ctrl.modularity <- as.data.frame(R3.10X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.10X.ctrl.degree_dist <- degree_distribution(R3.10X.ctrl.net)
# Step 2: Fit the power-law distribution
R3.10X.ctrl.pl_fit <- fit_power_law(R3.10X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) 
R3.10X.ctrl.pl_fit <- as.data.frame(R3.10X.ctrl.pl_fit)
##correct KS pvalue
R3.10X.ctrl.pl_fit$KS.p <- p.adjust(R3.10X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.2290335, ##KS.p = 0.9975114

##Add additional metrics to dataframe
net_prop.R3.10X.ctrl.wide.1 <-spread(net_prop.R3.10X.ctrl.final, metric, value)
net_prop.R3.10X.ctrl.wide.1 <- net_prop.R3.10X.ctrl.wide.1 %>% mutate(giant_component_size = '7')
net_prop.R3.10X.ctrl.wide.1 <- net_prop.R3.10X.ctrl.wide.1 %>% mutate(Modularity = '0.7197232')
net_prop.R3.10X.ctrl.wide.1 <- net_prop.R3.10X.ctrl.wide.1 %>% mutate(KS.stat = '0.2290335')
net_prop.R3.10X.ctrl.wide.1 <- net_prop.R3.10X.ctrl.wide.1 %>% mutate(KS.p = '0.9975114')
net_prop.R3.10X.ctrl.final <- gather(net_prop.R3.10X.ctrl.wide.1, metric, value, 4:23)
net_prop.R3.10X.ctrl.final <- net_prop.R3.10X.ctrl.final %>% mutate(Condition = 'Condition 3')
```

```{r}
##R6
##CZ4810X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R6.10X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.10X.ctrl <- prune_samples(sample_data(R6.10X.ctrl)$Treatment == "Control", R6.10X.ctrl)
R6.10X.ctrl <- prune_samples(sample_data(R6.10X.ctrl)$Cultivar == "CZ4810X", R6.10X.ctrl)

##Co-occurence table
R6.10X.ctrl.table <- co_occurrence(R6.10X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R6.10X.ctrl.table$p <-p.adjust(R6.10X.ctrl.table$p, "fdr")
R6.10X.ctrl.table <- filter(R6.10X.ctrl.table,p <= 0.05)

##network_layout_ps function
R6.10X.ctrl.layout <- network_layout_ps(R6.10X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R6.10X.ctrl, co_occurrence_table = R6.10X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R6.10X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R6.10X.ctrl.ctrl.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R6.10X.ctrl.net <- network_ps(R6.10X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.10X.ctrl <- net_properties(R6.10X.ctrl.net)
node_prop.R6.10X.ctrl <- node_properties(R6.10X.ctrl.net)
hub_score.R6.10X.ctrl <- hub_score(R6.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.10X.ctrl <- authority_score(R6.10X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.10X.ctrl <- neighborhood.connectivity(R6.10X.ctrl.net, vertices = V(R6.10X.ctrl.net), mode = "all")
neighbor.R6.10X.ctrl <- as.data.frame(neighbor.R6.10X.ctrl)

##Network properties for comparison
net_prop.R6.10X.ctrl <- as.data.frame(net_prop.R6.10X.ctrl)
net_prop.R6.10X.ctrl <- tibble::rownames_to_column(net_prop.R6.10X.ctrl, "metric")
net_prop.R6.10X.ctrl <- net_prop.R6.10X.ctrl %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.10X.ctrl <- net_prop.R6.10X.ctrl %>% mutate(Treatment = 'Control')
net_prop.R6.10X.ctrl <- net_prop.R6.10X.ctrl %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R6.10X.ctrl <- as.data.frame(hub_score.R6.10X.ctrl)

hub_score.R6.10X.ctrl.1 <- filter(hub_score.R6.10X.ctrl,vector >= 0.2)
##7

net_prop.R6.10X.ctrl <- net_prop.R6.10X.ctrl %>% mutate(Hub_Count = '7')

##Add mean neighborhood connectivity
mean(neighbor.R6.10X.ctrl$neighbor.R6.10X.ctrl) ##3.909091
net_prop.R6.10X.ctrl <- net_prop.R6.10X.ctrl %>% mutate(Mean_neighborhood = '3.909091')
net_prop.R6.10X.ctrl.wide <-spread(net_prop.R6.10X.ctrl, metric, value)
net_prop.R6.10X.ctrl.final <- gather(net_prop.R6.10X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
R6.10X.ctrl.node.names <- as.data.frame(V(R6.10X.ctrl.net)$name)
R6.10X.ctrl.node.names <- R6.10X.ctrl.node.names %>% mutate(Developmental_Stage = 'R6')
R6.10X.ctrl.node.names <- R6.10X.ctrl.node.names %>% mutate(Treatment = 'Control')
R6.10X.ctrl.node.names <- R6.10X.ctrl.node.names %>% mutate(Cultivar = 'CZ4810X')
names(R6.10X.ctrl.node.names)[names(R6.10X.ctrl.node.names) == "V(R6.10X.ctrl.net)$name"] <- "genus"
R6.10X.ctrl.node.names <- R6.10X.ctrl.node.names %>% mutate(Condition = 'Condition 4')
hub_score.R6.10X.ctrl$genus <- rownames(hub_score.R6.10X.ctrl)
R6.10X.ctrl.heat <- merge(R6.10X.ctrl.node.names, hub_score.R6.10X.ctrl, by = c("genus"))
R6.10X.ctrl.heat <-subset(R6.10X.ctrl.heat, select = -c(8:27))

##Additional measurements
R6.10X.ctrl.giant_component_size <- max(components(R6.10X.ctrl.net)$csize) ##7
R6.10X.ctrl.giant_component_size <- as.data.frame(R6.10X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
R6.10X.ctrl.community <- cluster_walktrap(R6.10X.ctrl.net)

##Get the community membership vector and calculate modularity
R6.10X.ctrl.membership <- R6.10X.ctrl.community$membership
R6.10X.ctrl.modularity <- modularity(R6.10X.ctrl.community) ##0.6782044
R6.10X.ctrl.modularity <- as.data.frame(R6.10X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.10X.ctrl.degree_dist <- degree_distribution(R6.10X.ctrl.net)
# Step 2: Fit the power-law distribution
R6.10X.ctrl.pl_fit <- fit_power_law(R6.10X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) 
R6.10X.ctrl.pl_fit <- as.data.frame(R6.10X.ctrl.pl_fit)
##correct KS pvalue
R6.10X.ctrl.pl_fit$KS.p <- p.adjust(R6.10X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.278192, ##KS.p = 0.9162618

##Add additional metrics to dataframe
net_prop.R6.10X.ctrl.wide.1 <-spread(net_prop.R6.10X.ctrl.final, metric, value)
net_prop.R6.10X.ctrl.wide.1 <- net_prop.R6.10X.ctrl.wide.1 %>% mutate(giant_component_size = '7')
net_prop.R6.10X.ctrl.wide.1 <- net_prop.R6.10X.ctrl.wide.1%>% mutate(Modularity = '0.6782044')
net_prop.R6.10X.ctrl.wide.1 <- net_prop.R6.10X.ctrl.wide.1 %>% mutate(KS.stat = '0.278192')
net_prop.R6.10X.ctrl.wide.1 <- net_prop.R6.10X.ctrl.wide.1 %>% mutate(KS.p = '0.9162618')
net_prop.R6.10X.ctrl.final <- gather(net_prop.R6.10X.ctrl.wide.1, metric, value, 4:23)
net_prop.R6.10X.ctrl.final <- net_prop.R6.10X.ctrl.final %>% mutate(Condition = 'Condition 4')
```

```{r}
##V1
##CZ4979X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V1.79X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.79X.ctrl <- prune_samples(sample_data(V1.79X.ctrl)$Treatment == "Control", V1.79X.ctrl)
V1.79X.ctrl <- prune_samples(sample_data(V1.79X.ctrl)$Cultivar == "CZ4979X", V1.79X.ctrl)

##Co-occurence table
V1.79X.ctrl.table <- co_occurrence(V1.79X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V1.79X.ctrl.table$p <-p.adjust(V1.79X.ctrl.table$p, "fdr")
V1.79X.ctrl.table <- filter(V1.79X.ctrl.table,p <= 0.05)

##network_layout_ps function
V1.79X.ctrl.layout <- network_layout_ps(V1.79X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.79X.ctrl, co_occurrence_table = V1.79X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V1.79X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V1.79X.ctrl.net.tiff", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V1.79X.ctrl.net <- network_ps(V1.79X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.79X.ctrl <- net_properties(V1.79X.ctrl.net)
node_prop.V1.79X.ctrl <- node_properties(V1.79X.ctrl.net)
hub_score.V1.79X.ctrl <- hub_score(V1.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.79X.ctrl <- authority_score(V1.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.79X.ctrl <- neighborhood.connectivity(V1.79X.ctrl.net, vertices = V(V1.79X.ctrl.net), mode = "all")
neighbor.V1.79X.ctrl <- as.data.frame(neighbor.V1.79X.ctrl)

##Network properties for comparison
net_prop.V1.79X.ctrl <- as.data.frame(net_prop.V1.79X.ctrl)
net_prop.V1.79X.ctrl <- tibble::rownames_to_column(net_prop.V1.79X.ctrl, "metric")
net_prop.V1.79X.ctrl <- net_prop.V1.79X.ctrl %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.79X.ctrl <- net_prop.V1.79X.ctrl %>% mutate(Treatment = 'Control')
net_prop.V1.79X.ctrl <- net_prop.V1.79X.ctrl %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V1.79X.ctrl <- as.data.frame(hub_score.V1.79X.ctrl)

hub_score.V1.79X.ctrl.1 <- filter(hub_score.V1.79X.ctrl,vector >= 0.2)
##6

net_prop.V1.79X.ctrl <- net_prop.V1.79X.ctrl %>% mutate(Hub_Count = '6')

##Add mean neighborhood connectivity
mean(neighbor.V1.79X.ctrl$neighbor.V1.79X.ctrl) ##2
net_prop.V1.79X.ctrl <- net_prop.V1.79X.ctrl %>% mutate(Mean_neighborhood = '2')
net_prop.V1.79X.ctrl.wide <-spread(net_prop.V1.79X.ctrl, metric, value)
net_prop.V1.79X.ctrl.final <- gather(net_prop.V1.79X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
V1.79X.ctrl.node.names <- as.data.frame(V(V1.79X.ctrl.net)$name)
V1.79X.ctrl.node.names <- V1.79X.ctrl.node.names %>% mutate(Developmental_Stage = 'V1')
V1.79X.ctrl.node.names <- V1.79X.ctrl.node.names %>% mutate(Treatment = 'Control')
V1.79X.ctrl.node.names <- V1.79X.ctrl.node.names %>% mutate(Cultivar = 'CZ4979X')
names(V1.79X.ctrl.node.names)[names(V1.79X.ctrl.node.names) == "V(V1.79X.ctrl.net)$name"] <- "genus"
V1.79X.ctrl.node.names <- V1.79X.ctrl.node.names %>% mutate(Condition = 'Condition 5')
hub_score.V1.79X.ctrl$genus <- rownames(hub_score.V1.79X.ctrl)
V1.79X.ctrl.heat <- merge(V1.79X.ctrl.node.names, hub_score.V1.79X.ctrl, by = c("genus"))
V1.79X.ctrl.heat <-subset(V1.79X.ctrl.heat, select = -c(8:27))

##Additional measurements
V1.79X.ctrl.giant_component_size <- max(components(V1.79X.ctrl.net)$csize) ##3
V1.79X.ctrl.giant_component_size <- as.data.frame(V1.79X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
V1.79X.ctrl.community <- cluster_walktrap(V1.79X.ctrl.net)

##Get the community membership vector and calculate modularity
V1.79X.ctrl.membership <- V1.79X.ctrl.community$membership
V1.79X.ctrl.modularity <- modularity(V1.79X.ctrl.community) ##0.5
V1.79X.ctrl.modularity <- as.data.frame(V1.79X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.79X.ctrl.degree_dist <- degree_distribution(V1.79X.ctrl.net)
# Step 2: Fit the power-law distribution
V1.79X.ctrl.pl_fit <- fit_power_law(V1.79X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) ##error
V1.79X.ctrl.pl_fit <- as.data.frame(V1.79X.ctrl.pl_fit)
##correct KS pvalue
V1.79X.ctrl.pl_fit$KS.p <- p.adjust(V1.79X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.3646647, ##KS.p = 0.9529957

##Add additional metrics to dataframe
net_prop.V1.79X.ctrl.wide.1 <-spread(net_prop.V1.79X.ctrl.final, metric, value)
net_prop.V1.79X.ctrl.wide.1 <- net_prop.V1.79X.ctrl.wide.1 %>% mutate(giant_component_size = '3')
net_prop.V1.79X.ctrl.wide.1 <- net_prop.V1.79X.ctrl.wide.1%>% mutate(Modularity = '0.5')
##net_prop.V1.79X.ctrl.wide.1 <- net_prop.V1.79X.ctrl.wide.1 %>% mutate(KS.stat = '0.3646647')
##net_prop.V1.79X.ctrl.wide.1 <- net_prop.V1.79X.ctrl.wide.1 %>% mutate(KS.p = '0.9529957')
net_prop.V1.79X.ctrl.final <- gather(net_prop.V1.79X.ctrl.wide.1, metric, value, 4:21)
net_prop.V1.79X.ctrl.final <- net_prop.V1.79X.ctrl.final %>% mutate(Condition = 'Condition 5')
```

```{r}
##V6
##CZ4979X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V6.79X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.79X.ctrl <- prune_samples(sample_data(V6.79X.ctrl)$Treatment == "Control", V6.79X.ctrl)
V6.79X.ctrl <- prune_samples(sample_data(V6.79X.ctrl)$Cultivar == "CZ4979X", V6.79X.ctrl)

##Co-occurence table
V6.79X.ctrl.table <- co_occurrence(V6.79X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V6.79X.ctrl.table$p <-p.adjust(V6.79X.ctrl.table$p, "fdr")
V6.79X.ctrl.table <- filter(V6.79X.ctrl.table,p <= 0.05)

##network_layout_ps function
V6.79X.ctrl.layout <- network_layout_ps(V6.79X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.79X.ctrl, co_occurrence_table = V6.79X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V6.79X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V6.79X.ctrl.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V6.79X.ctrl.net <- network_ps(V6.79X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.79X.ctrl <- net_properties(V6.79X.ctrl.net)
node_prop.V6.79X.ctrl <- node_properties(V6.79X.ctrl.net)
hub_score.V6.79X.ctrl <- hub_score(V6.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.79X.ctrl <- authority_score(V6.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.79X.ctrl <- neighborhood.connectivity(V6.79X.ctrl.net, vertices = V(V6.79X.ctrl.net), mode = "all")
neighbor.V6.79X.ctrl <- as.data.frame(neighbor.V6.79X.ctrl)

##Network properties for comparison
net_prop.V6.79X.ctrl <- as.data.frame(net_prop.V6.79X.ctrl)
net_prop.V6.79X.ctrl <- tibble::rownames_to_column(net_prop.V6.79X.ctrl, "metric")
net_prop.V6.79X.ctrl <- net_prop.V6.79X.ctrl %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.79X.ctrl <- net_prop.V6.79X.ctrl %>% mutate(Treatment = 'Control')
net_prop.V6.79X.ctrl <- net_prop.V6.79X.ctrl %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V6.79X.ctrl <- as.data.frame(hub_score.V6.79X.ctrl)

hub_score.V6.79X.ctrl.1 <- filter(hub_score.V6.79X.ctrl,vector >= 0.2)
##6

net_prop.V6.79X.ctrl <- net_prop.V6.79X.ctrl %>% mutate(Hub_Count = '6')

##Add mean neighborhood connectivity
mean(neighbor.V6.79X.ctrl$neighbor.V6.79X.ctrl) ##2
net_prop.V6.79X.ctrl <- net_prop.V6.79X.ctrl %>% mutate(Mean_neighborhood = '2')
net_prop.V6.79X.ctrl.wide <-spread(net_prop.V6.79X.ctrl, metric, value)
net_prop.V6.79X.ctrl.final <- gather(net_prop.V6.79X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
V6.79X.ctrl.node.names <- as.data.frame(V(V6.79X.ctrl.net)$name)
V6.79X.ctrl.node.names <- V6.79X.ctrl.node.names %>% mutate(Developmental_Stage = 'V6')
V6.79X.ctrl.node.names <- V6.79X.ctrl.node.names %>% mutate(Treatment = 'Control')
V6.79X.ctrl.node.names <- V6.79X.ctrl.node.names %>% mutate(Cultivar = 'CZ4979X')
names(V6.79X.ctrl.node.names)[names(V6.79X.ctrl.node.names) == "V(V6.79X.ctrl.net)$name"] <- "genus"
V6.79X.ctrl.node.names <- V6.79X.ctrl.node.names %>% mutate(Condition = 'Condition 6')
hub_score.V6.79X.ctrl$genus <- rownames(hub_score.V6.79X.ctrl)
V6.79X.ctrl.heat <- merge(V6.79X.ctrl.node.names, hub_score.V6.79X.ctrl, by = c("genus"))
V6.79X.ctrl.heat <-subset(V6.79X.ctrl.heat, select = -c(8:27))

##Additional measurements
V6.79X.ctrl.giant_component_size <- max(components(V6.79X.ctrl.net)$csize) ##3
V6.79X.ctrl.giant_component_size <- as.data.frame(V6.79X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
V6.79X.ctrl.community <- cluster_walktrap(V6.79X.ctrl.net)

##Get the community membership vector and calculate modularity
V6.79X.ctrl.membership <- V6.79X.ctrl.community$membership
V6.79X.ctrl.modularity <- modularity(V6.79X.ctrl.community) ##0.5
V6.79X.ctrl.modularity <- as.data.frame(V6.79X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.79X.ctrl.degree_dist <- degree_distribution(V6.79X.ctrl.net)
# Step 2: Fit the power-law distribution
V6.79X.ctrl.pl_fit <- fit_power_law(V6.79X.ctrl.degree_dist, implementation = "plift", force.continuous = TRUE) ##error
V6.79X.ctrl.pl_fit <- as.data.frame(V6.79X.ctrl.pl_fit)
##correct KS pvalue
V6.79X.ctrl.pl_fit$KS.p <- p.adjust(V6.79X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.3646647, ##KS.p = 0.9529957

##Add additional metrics to dataframe
net_prop.V6.79X.ctrl.wide.1 <-spread(net_prop.V6.79X.ctrl.final, metric, value)
net_prop.V6.79X.ctrl.wide.1 <- net_prop.V6.79X.ctrl.wide.1 %>% mutate(giant_component_size = '3')
net_prop.V6.79X.ctrl.wide.1 <- net_prop.V6.79X.ctrl.wide.1 %>% mutate(Modularity = '0.5')
##net_prop.V6.79X.ctrl.wide.1 <- net_prop.V6.79X.ctrl.wide.1 %>% mutate(KS.stat = '0.3646647')
##net_prop.V6.79X.ctrl.wide.1 <- net_prop.V6.79X.ctrl.wide.1 %>% mutate(KS.p = '0.9529957')
net_prop.V6.79X.ctrl.final <- gather(net_prop.V6.79X.ctrl.wide.1, metric, value, 4:21)
net_prop.V6.79X.ctrl.final <- net_prop.V6.79X.ctrl.final %>% mutate(Condition = 'Condition 6')
```

```{r}
##R3
##CZ4979X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R3.79X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.79X.ctrl <- prune_samples(sample_data(R3.79X.ctrl)$Treatment == "Control", R3.79X.ctrl)
R3.79X.ctrl <- prune_samples(sample_data(R3.79X.ctrl)$Cultivar == "CZ4979X", R3.79X.ctrl)

##Co-occurence table
R3.79X.ctrl.table <- co_occurrence(R3.79X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R3.79X.ctrl.table$p <-p.adjust(R3.79X.ctrl.table$p, "fdr")
R3.79X.ctrl.table <- filter(R3.79X.ctrl.table,p <= 0.05)

##network_layout_ps function
R3.79X.ctrl.layout <- network_layout_ps(R3.79X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R3.79X.ctrl, co_occurrence_table = R3.79X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R3.79X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R3.79X.ctrl.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R3.79X.ctrl.net <- network_ps(R3.79X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.79X.ctrl <- net_properties(R3.79X.ctrl.net)
node_prop.R3.79X.ctrl <- node_properties(R3.79X.ctrl.net)
hub_score.R3.79X.ctrl <- hub_score(R3.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.79X.ctrl <- authority_score(R3.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.79X.ctrl <- neighborhood.connectivity(R3.79X.ctrl.net, vertices = V(R3.79X.ctrl.net), mode = "all")
neighbor.R3.79X.ctrl <- as.data.frame(neighbor.R3.79X.ctrl)

##Network properties for comparison
net_prop.R3.79X.ctrl <- as.data.frame(net_prop.R3.79X.ctrl)
net_prop.R3.79X.ctrl <- tibble::rownames_to_column(net_prop.R3.79X.ctrl, "metric")
net_prop.R3.79X.ctrl <- net_prop.R3.79X.ctrl %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.79X.ctrl <- net_prop.R3.79X.ctrl %>% mutate(Treatment = 'Control')
net_prop.R3.79X.ctrl <- net_prop.R3.79X.ctrl %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R3.79X.ctrl <- as.data.frame(hub_score.R3.79X.ctrl)

hub_score.R3.79X.ctrl.1 <- filter(hub_score.R3.79X.ctrl,vector >= 0.2)
##5

net_prop.R3.79X.ctrl <- net_prop.R3.79X.ctrl %>% mutate(Hub_Count = '5')

##Add mean neighborhood connectivity
mean(neighbor.R3.79X.ctrl$neighbor.R3.79X.ctrl) ##2.818182
net_prop.R3.79X.ctrl <- net_prop.R3.79X.ctrl %>% mutate(Mean_neighborhood = '2.818182')
net_prop.R3.79X.ctrl.wide <-spread(net_prop.R3.79X.ctrl, metric, value)
net_prop.R3.79X.ctrl.final <- gather(net_prop.R3.79X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
R3.79X.ctrl.node.names <- as.data.frame(V(R3.79X.ctrl.net)$name)
R3.79X.ctrl.node.names <- R3.79X.ctrl.node.names %>% mutate(Developmental_Stage = 'R3')
R3.79X.ctrl.node.names <- R3.79X.ctrl.node.names %>% mutate(Treatment = 'Control')
R3.79X.ctrl.node.names <- R3.79X.ctrl.node.names %>% mutate(Cultivar = 'CZ4979X')
names(R3.79X.ctrl.node.names)[names(R3.79X.ctrl.node.names) == "V(R3.79X.ctrl.net)$name"] <- "genus"
R3.79X.ctrl.node.names <- R3.79X.ctrl.node.names %>% mutate(Condition = 'Condition 7')
hub_score.R3.79X.ctrl$genus <- rownames(hub_score.R3.79X.ctrl)
R3.79X.ctrl.heat <- merge(R3.79X.ctrl.node.names, hub_score.R3.79X.ctrl, by = c("genus"))
R3.79X.ctrl.heat <-subset(R3.79X.ctrl.heat, select = -c(8:27))

##Additional measurements
R3.79X.ctrl.giant_component_size <- max(components(R3.79X.ctrl.net)$csize) ##5
R3.79X.ctrl.giant_component_size <- as.data.frame(R3.79X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
R3.79X.ctrl.community <- cluster_walktrap(R3.79X.ctrl.net)

##Get the community membership vector and calculate modularity
R3.79X.ctrl.membership <- R3.79X.ctrl.community$membership
R3.79X.ctrl.modularity <- modularity(R3.79X.ctrl.community) ##0.792924
R3.79X.ctrl.modularity <- as.data.frame(R3.79X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.79X.ctrl.degree_dist <- degree_distribution(R3.79X.ctrl.net)
# Step 2: Fit the power-law distribution
R3.79X.ctrl.pl_fit <- fit_power_law(R3.79X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) 
R3.79X.ctrl.pl_fit <- as.data.frame(R3.79X.ctrl.pl_fit)
##correct KS pvalue
R3.79X.ctrl.pl_fit$KS.p <- p.adjust(R3.79X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.3646647, ##KS.p = 0.9529957

##Add additional metrics to dataframe
net_prop.R3.79X.ctrl.wide.1 <-spread(net_prop.R3.79X.ctrl.final, metric, value)
net_prop.R3.79X.ctrl.wide.1 <- net_prop.R3.79X.ctrl.wide.1 %>% mutate(giant_component_size = '5')
net_prop.R3.79X.ctrl.wide.1 <- net_prop.R3.79X.ctrl.wide.1 %>% mutate(Modularity = '0.792924')
net_prop.R3.79X.ctrl.wide.1 <- net_prop.R3.79X.ctrl.wide.1 %>% mutate(KS.stat = '0.3646647')
net_prop.R3.79X.ctrl.wide.1 <- net_prop.R3.79X.ctrl.wide.1 %>% mutate(KS.p = '0.9529957')
net_prop.R3.79X.ctrl.final <- gather(net_prop.R3.79X.ctrl.wide.1, metric, value, 4:23)
net_prop.R3.79X.ctrl.final <- net_prop.R3.79X.ctrl.final %>% mutate(Condition = 'Condition 7')
```

```{r}
##R6
##CZ4979X
##Ctrol
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R6.79X.ctrl <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.79X.ctrl <- prune_samples(sample_data(R6.79X.ctrl)$Treatment == "Control", R6.79X.ctrl)
R6.79X.ctrl <- prune_samples(sample_data(R6.79X.ctrl)$Cultivar == "CZ4979X", R6.79X.ctrl)

##Co-occurence table
R6.79X.ctrl.table <- co_occurrence(R6.79X.ctrl, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R6.79X.ctrl.table$p <-p.adjust(R6.79X.ctrl.table$p, "fdr")
R6.79X.ctrl.table <- filter(R6.79X.ctrl.table,p <= 0.05)

##network_layout_ps function
R6.79X.ctrl.layout <- network_layout_ps(R6.79X.ctrl, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.ctrl.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R6.79X.ctrl, co_occurrence_table = R6.79X.ctrl.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R6.79X.ctrl.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R6.79X.ctrl.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R6.79X.ctrl.net <- network_ps(R6.79X.ctrl, treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.ctrl.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.79X.ctrl <- net_properties(R6.79X.ctrl.net)
node_prop.R6.79X.ctrl <- node_properties(R6.79X.ctrl.net)
hub_score.R6.79X.ctrl <- hub_score(R6.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.79X.ctrl <- authority_score(R6.79X.ctrl.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.79X.ctrl <- neighborhood.connectivity(R6.79X.ctrl.net, vertices = V(R6.79X.ctrl.net), mode = "all")
neighbor.R6.79X.ctrl <- as.data.frame(neighbor.R6.79X.ctrl)

##Network properties for comparison
net_prop.R6.79X.ctrl <- as.data.frame(net_prop.R6.79X.ctrl)
net_prop.R6.79X.ctrl <- tibble::rownames_to_column(net_prop.R6.79X.ctrl, "metric")
net_prop.R6.79X.ctrl <- net_prop.R6.79X.ctrl %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.79X.ctrl <- net_prop.R6.79X.ctrl %>% mutate(Treatment = 'Control')
net_prop.R6.79X.ctrl <- net_prop.R6.79X.ctrl %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R6.79X.ctrl <- as.data.frame(hub_score.R6.79X.ctrl)

hub_score.R6.79X.ctrl.1 <- filter(hub_score.R6.79X.ctrl,vector >= 0.2)
##7

net_prop.R6.79X.ctrl <- net_prop.R6.79X.ctrl %>% mutate(Hub_Count = '7')

##Add mean neighborhood connectivity
mean(neighbor.R6.79X.ctrl$neighbor.R6.79X.ctrl) ##3.272727
net_prop.R6.79X.ctrl <- net_prop.R6.79X.ctrl %>% mutate(Mean_neighborhood = '3.272727')
net_prop.R6.79X.ctrl.wide <-spread(net_prop.R6.79X.ctrl, metric, value)
net_prop.R6.79X.ctrl.final <- gather(net_prop.R6.79X.ctrl.wide, metric, value, 4:19)

##Node table for heatmap
R6.79X.ctrl.node.names <- as.data.frame(V(R6.79X.ctrl.net)$name)
R6.79X.ctrl.node.names <- R6.79X.ctrl.node.names %>% mutate(Developmental_Stage = 'R6')
R6.79X.ctrl.node.names <- R6.79X.ctrl.node.names %>% mutate(Treatment = 'Control')
R6.79X.ctrl.node.names <- R6.79X.ctrl.node.names %>% mutate(Cultivar = 'CZ4979X')
names(R6.79X.ctrl.node.names)[names(R6.79X.ctrl.node.names) == "V(R6.79X.ctrl.net)$name"] <- "genus"
R6.79X.ctrl.node.names <- R6.79X.ctrl.node.names %>% mutate(Condition = 'Condition 8')
hub_score.R6.79X.ctrl$genus <- rownames(hub_score.R6.79X.ctrl)
R6.79X.ctrl.heat <- merge(R6.79X.ctrl.node.names, hub_score.R6.79X.ctrl, by = c("genus"))
R6.79X.ctrl.heat <-subset(R6.79X.ctrl.heat, select = -c(8:27))

##Additional measurements
R6.79X.ctrl.giant_component_size <- max(components(R6.79X.ctrl.net)$csize) ##7
R6.79X.ctrl.giant_component_size <- as.data.frame(R6.79X.ctrl.giant_component_size)

##community detection
# Perform community detection using walktrap method
R6.79X.ctrl.community <- cluster_walktrap(R6.79X.ctrl.net)

##Get the community membership vector and calculate modularity
R6.79X.ctrl.membership <- R6.79X.ctrl.community$membership
R6.79X.ctrl.modularity <- modularity(R6.79X.ctrl.community) ##0.625
R6.79X.ctrl.modularity <- as.data.frame(R6.79X.ctrl.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.79X.ctrl.degree_dist <- degree_distribution(R6.79X.ctrl.net)
# Step 2: Fit the power-law distribution
R6.79X.ctrl.pl_fit <- fit_power_law(R6.79X.ctrl.degree_dist, implementation = "plfit", force.continuous = TRUE) 
R6.79X.ctrl.pl_fit <- as.data.frame(R6.79X.ctrl.pl_fit)
##correct KS pvalue
R6.79X.ctrl.pl_fit$KS.p <- p.adjust(R6.79X.ctrl.pl_fit$KS.p, "fdr") ##KS.stat = 0.3646647, ##KS.p = 0.9529957

##Add additional metrics to dataframe
net_prop.R6.79X.ctrl.wide.1 <-spread(net_prop.R6.79X.ctrl.final, metric, value)
net_prop.R6.79X.ctrl.wide.1 <- net_prop.R6.79X.ctrl.wide.1 %>% mutate(giant_component_size = '7')
net_prop.R6.79X.ctrl.wide.1 <- net_prop.R6.79X.ctrl.wide.1 %>% mutate(Modularity = '0.625')
net_prop.R6.79X.ctrl.wide.1 <- net_prop.R6.79X.ctrl.wide.1 %>% mutate(KS.stat = '0.3646647')
net_prop.R6.79X.ctrl.wide.1 <- net_prop.R6.79X.ctrl.wide.1 %>% mutate(KS.p = '0.9529957')
net_prop.R6.79X.ctrl.final <- gather(net_prop.R6.79X.ctrl.wide.1, metric, value, 4:23)
net_prop.R6.79X.ctrl.final <- net_prop.R6.79X.ctrl.final %>% mutate(Condition = 'Condition 8')
```

```{r}
##V1
##CZ4810X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V1.10X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.10X.trt <- prune_samples(sample_data(V1.10X.trt)$Treatment == "Biostimulant", V1.10X.trt)
V1.10X.trt <- prune_samples(sample_data(V1.10X.trt)$Cultivar == "CZ4810X", V1.10X.trt)

##Co-occurence table
V1.10X.trt.table <- co_occurrence(V1.10X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V1.10X.trt.table$p <-p.adjust(V1.10X.trt.table$p, "fdr")
V1.10X.trt.table <- filter(V1.10X.trt.table,p <= 0.05)

##network_layout_ps function
V1.10X.trt.layout <- network_layout_ps(V1.10X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.10X.trt, co_occurrence_table = V1.10X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V1.10X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V1.10X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V1.10X.trt.net <- network_ps(V1.10X.trt, treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.10X.trt <- net_properties(V1.10X.trt.net)
node_prop.V1.10X.trt <- node_properties(V1.10X.trt.net)
hub_score.V1.10X.trt <- hub_score(V1.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.10X.trt <- authority_score(V1.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.10X.trt <- neighborhood.connectivity(V1.10X.trt.net, vertices = V(V1.10X.trt.net), mode = "all")
neighbor.V1.10X.trt <- as.data.frame(neighbor.V1.10X.trt)

##Network properties for comparison
net_prop.V1.10X.trt <- as.data.frame(net_prop.V1.10X.trt)
net_prop.V1.10X.trt <- tibble::rownames_to_column(net_prop.V1.10X.trt, "metric")
net_prop.V1.10X.trt <- net_prop.V1.10X.trt %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.10X.trt <- net_prop.V1.10X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.V1.10X.trt <- net_prop.V1.10X.trt %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V1.10X.trt <- as.data.frame(hub_score.V1.10X.trt)

hub_score.V1.10X.trt.1 <- filter(hub_score.V1.10X.trt,vector >= 0.2)
##6

net_prop.V1.10X.trt <- net_prop.V1.10X.trt %>% mutate(Hub_Count = '6')

##Add mean neighborhood connectivity
mean(neighbor.V1.10X.trt$neighbor.V1.10X.trt) ##2
net_prop.V1.10X.trt <- net_prop.V1.10X.trt %>% mutate(Mean_neighborhood = '2')
net_prop.V1.10X.trt.wide <-spread(net_prop.V1.10X.trt, metric, value)
net_prop.V1.10X.trt.final <- gather(net_prop.V1.10X.trt.wide, metric, value, 4:19)

##Node table for heatmap
V1.10X.trt.node.names <- as.data.frame(V(V1.10X.trt.net)$name)
V1.10X.trt.node.names <- V1.10X.trt.node.names %>% mutate(Developmental_Stage = 'V1')
V1.10X.trt.node.names <- V1.10X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
V1.10X.trt.node.names <- V1.10X.trt.node.names %>% mutate(Cultivar = 'CZ4810X')
names(V1.10X.trt.node.names)[names(V1.10X.trt.node.names) == "V(V1.10X.trt.net)$name"] <- "genus"
V1.10X.trt.node.names <- V1.10X.trt.node.names %>% mutate(Condition = 'Condition 9')
hub_score.V1.10X.trt$genus <- rownames(hub_score.V1.10X.trt)
V1.10X.trt.heat <- merge(V1.10X.trt.node.names, hub_score.V1.10X.trt, by = c("genus"))
V1.10X.trt.heat <-subset(V1.10X.trt.heat, select = -c(8:27))

##Additional measurements
V1.10X.trt.giant_component_size <- max(components(V1.10X.trt.net)$csize) ##3
V1.10X.trt.giant_component_size <- as.data.frame(V1.10X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
V1.10X.trt.community <- cluster_walktrap(V1.10X.trt.net)

##Get the community membership vector and calculate modularity
V1.10X.trt.membership <- V1.10X.trt.community$membership
V1.10X.trt.modularity <- modularity(V1.10X.trt.community) ##0.5
V1.10X.trt.modularity <- as.data.frame(V1.10X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.10X.trt.degree_dist <- degree_distribution(V1.10X.trt.net)
# Step 2: Fit the power-law distribution
V1.10X.trt.pl_fit <- fit_power_law(V1.10X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE) ##error
V1.10X.trt.pl_fit <- as.data.frame(V1.10X.trt.pl_fit)
##correct KS pvalue
V1.10X.trt.pl_fit$KS.p <- p.adjust(V1.10X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.2498341, ##KS.p = 0.9920285

##Add additional metrics to dataframe
net_prop.V1.10X.trt.wide.1 <-spread(net_prop.V1.10X.trt.final, metric, value)
net_prop.V1.10X.trt.wide.1 <- net_prop.V1.10X.trt.wide.1 %>% mutate(giant_component_size = '3')
net_prop.V1.10X.trt.wide.1 <- net_prop.V1.10X.trt.wide.1 %>% mutate(Modularity = '0.5')
##net_prop.V6.10X.trt.wide.1 <- net_prop.V6.10X.trt.wide.1 %>% mutate(KS.stat = '0.2498341')
##net_prop.V6.10X.trt.wide.1 <- net_prop.V6.10X.trt.wide.1 %>% mutate(KS.p = '0.9920285')
net_prop.V1.10X.trt.final <- gather(net_prop.V1.10X.trt.wide.1, metric, value, 4:21)
net_prop.V1.10X.trt.final <- net_prop.V1.10X.trt.final %>% mutate(Condition = 'Condition 9')
```

```{r}
##V6
##CZ4810X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V6.10X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.10X.trt <- prune_samples(sample_data(V6.10X.trt)$Treatment == "Biostimulant", V6.10X.trt)
V6.10X.trt <- prune_samples(sample_data(V6.10X.trt)$Cultivar == "CZ4810X", V6.10X.trt)

##Co-occurence table
V6.10X.trt.table <- co_occurrence(V6.10X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V6.10X.trt.table$p <-p.adjust(V6.10X.trt.table$p, "fdr")
V6.10X.trt.table <- filter(V6.10X.trt.table,p <= 0.05)

##network_layout_ps function
V6.10X.trt.layout <- network_layout_ps(V6.10X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.10X.trt, co_occurrence_table = V6.10X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V6.10X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V6.10X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V6.10X.trt.net <- network_ps(V6.10X.trt, treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.10X.trt<- net_properties(V6.10X.trt.net)
node_prop.V6.10X.trt <- node_properties(V6.10X.trt.net)
hub_score.V6.10X.trt <- hub_score(V6.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.10X.trt <- authority_score(V6.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.10X.trt <- neighborhood.connectivity(V6.10X.trt.net, vertices = V(V6.10X.trt.net), mode = "all")
neighbor.V6.10X.trt <- as.data.frame(neighbor.V6.10X.trt)

##Network properties for comparison
net_prop.V6.10X.trt <- as.data.frame(net_prop.V6.10X.trt)
net_prop.V6.10X.trt <- tibble::rownames_to_column(net_prop.V6.10X.trt, "metric")
net_prop.V6.10X.trt <- net_prop.V6.10X.trt %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.10X.trt <- net_prop.V6.10X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.V6.10X.trt <- net_prop.V6.10X.trt %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V6.10X.trt <- as.data.frame(hub_score.V6.10X.trt)

hub_score.V6.10X.trt.1 <- filter(hub_score.V6.10X.trt,vector >= 0.2)
##5

net_prop.V6.10X.trt <- net_prop.V6.10X.trt %>% mutate(Hub_Count = '5')

##Add mean neighborhood connectivity
mean(neighbor.V6.10X.trt$neighbor.V6.10X.trt) ##2.956522
net_prop.V6.10X.trt <- net_prop.V6.10X.trt %>% mutate(Mean_neighborhood = '2.956522')
net_prop.V6.10X.trt.wide <-spread(net_prop.V6.10X.trt, metric, value)
net_prop.V6.10X.trt.final <- gather(net_prop.V6.10X.trt.wide, metric, value, 4:19)

##Node table for heatmap
V6.10X.trt.node.names <- as.data.frame(V(V6.10X.trt.net)$name)
V6.10X.trt.node.names <- V6.10X.trt.node.names %>% mutate(Developmental_Stage = 'V6')
V6.10X.trt.node.names <- V6.10X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
V6.10X.trt.node.names <- V6.10X.trt.node.names %>% mutate(Cultivar = 'CZ4810X')
names(V6.10X.trt.node.names)[names(V6.10X.trt.node.names) == "V(V6.10X.trt.net)$name"] <- "genus"
V6.10X.trt.node.names <- V6.10X.trt.node.names %>% mutate(Condition = 'Condition 10')
hub_score.V6.10X.trt$genus <- rownames(hub_score.V6.10X.trt)
V6.10X.trt.heat <- merge(V6.10X.trt.node.names, hub_score.V6.10X.trt, by = c("genus"))
V6.10X.trt.heat <-subset(V6.10X.trt.heat, select = -c(8:27))

##Additional measurements
V6.10X.trt.giant_component_size <- max(components(V6.10X.trt.net)$csize) ##5
V6.10X.trt.giant_component_size <- as.data.frame(V6.10X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
V6.10X.trt.community <- cluster_walktrap(V6.10X.trt.net)

##Get the community membership vector and calculate modularity
V6.10X.trt.membership <- V6.10X.trt.community$membership
V6.10X.trt.modularity <- modularity(V6.10X.trt.community) ##0.8044983
V6.10X.trt.modularity <- as.data.frame(V6.10X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.10X.trt.degree_dist <- degree_distribution(V6.10X.trt.net)
# Step 2: Fit the power-law distribution
V6.10X.trt.pl_fit <- fit_power_law(V6.10X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
V6.10X.trt.pl_fit <- as.data.frame(V6.10X.trt.pl_fit)
##correct KS pvalue
V6.10X.trt.pl_fit$KS.p <- p.adjust(V6.10X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.2498341, ##KS.p = 0.9920285

##Add additional metrics to dataframe
net_prop.V6.10X.trt.wide.1 <-spread(net_prop.V6.10X.trt.final, metric, value)
net_prop.V6.10X.trt.wide.1 <- net_prop.V6.10X.trt.wide.1 %>% mutate(giant_component_size = '5')
net_prop.V6.10X.trt.wide.1 <- net_prop.V6.10X.trt.wide.1 %>% mutate(Modularity = '0.8044983')
net_prop.V6.10X.trt.wide.1 <- net_prop.V6.10X.trt.wide.1 %>% mutate(KS.stat = '0.2498341')
net_prop.V6.10X.trt.wide.1 <- net_prop.V6.10X.trt.wide.1 %>% mutate(KS.p = '0.9920285')
net_prop.V6.10X.trt.final <- gather(net_prop.V6.10X.trt.wide.1, metric, value, 4:23)
net_prop.V6.10X.trt.final <- net_prop.V6.10X.trt.final %>% mutate(Condition = 'Condition 10')
```

```{r}
##R3
##CZ4810X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R3.10X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.10X.trt <- prune_samples(sample_data(R3.10X.trt)$Treatment == "Biostimulant", R3.10X.trt)
R3.10X.trt <- prune_samples(sample_data(R3.10X.trt)$Cultivar == "CZ4810X", R3.10X.trt)

##Co-occurence table
R3.10X.trt.table <- co_occurrence(R3.10X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R3.10X.trt.table$p <-p.adjust(R3.10X.trt.table$p, "fdr")
R3.10X.trt.table <- filter(R3.10X.trt.table,p <= 0.05)

##network_layout_ps function
R3.10X.trt.layout <- network_layout_ps(R3.10X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R3.10X.trt, co_occurrence_table = R3.10X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R3.10X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R3.10X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R3.10X.trt.net <- network_ps(R3.10X.trt, treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.10X.trt <- net_properties(R3.10X.trt.net)
node_prop.R3.10X.trt <- node_properties(R3.10X.trt.net)
hub_score.R3.10X.trt <- hub_score(R3.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.10X.trt <- authority_score(R3.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.10X.trt <- neighborhood.connectivity(R3.10X.trt.net, vertices = V(R3.10X.trt.net), mode = "all")
neighbor.R3.10X.trt <- as.data.frame(neighbor.R3.10X.trt)

##Network properties for comparison
net_prop.R3.10X.trt <- as.data.frame(net_prop.R3.10X.trt)
net_prop.R3.10X.trt <- tibble::rownames_to_column(net_prop.R3.10X.trt, "metric")
net_prop.R3.10X.trt <- net_prop.R3.10X.trt %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.10X.trt <- net_prop.R3.10X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.R3.10X.trt <- net_prop.R3.10X.trt %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R3.10X.trt <- as.data.frame(hub_score.R3.10X.trt)

hub_score.R3.10X.trt.1 <- filter(hub_score.R3.10X.trt,vector >= 0.2)
##4

net_prop.R3.10X.trt <- net_prop.R3.10X.trt %>% mutate(Hub_Count = '4')

##Add mean neighborhood connectivity
mean(neighbor.R3.10X.trt$neighbor.R3.10X.trt) #2.8
net_prop.R3.10X.trt <- net_prop.R3.10X.trt %>% mutate(Mean_neighborhood = '2.8')
net_prop.R3.10X.trt.wide <-spread(net_prop.R3.10X.trt, metric, value)
net_prop.R3.10X.trt.final <- gather(net_prop.R3.10X.trt.wide, metric, value, 4:19)

##Node table for heatmap
R3.10X.trt.node.names <- as.data.frame(V(R3.10X.trt.net)$name)
R3.10X.trt.node.names <- R3.10X.trt.node.names %>% mutate(Developmental_Stage = 'R3')
R3.10X.trt.node.names <- R3.10X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
R3.10X.trt.node.names <- R3.10X.trt.node.names %>% mutate(Cultivar = 'CZ4810X')
names(R3.10X.trt.node.names)[names(R3.10X.trt.node.names) == "V(R3.10X.trt.net)$name"] <- "genus"
R3.10X.trt.node.names <- R3.10X.trt.node.names %>% mutate(Condition = 'Condition 11')
hub_score.R3.10X.trt$genus <- rownames(hub_score.R3.10X.trt)
R3.10X.trt.heat <- merge(R3.10X.trt.node.names, hub_score.R3.10X.trt, by = c("genus"))
R3.10X.trt.heat <-subset(R3.10X.trt.heat, select = -c(8:27))

##Additional measurements
R3.10X.trt.giant_component_size <- max(components(R3.10X.trt.net)$csize) ##4
R3.10X.trt.giant_component_size <- as.data.frame(R3.10X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
R3.10X.trt.community <- cluster_walktrap(R3.10X.trt.net)

##Get the community membership vector and calculate modularity
R3.10X.trt.membership <- R3.10X.trt.community$membership
R3.10X.trt.modularity <- modularity(R3.10X.trt.community) ##0.7346939
R3.10X.trt.modularity <- as.data.frame(R3.10X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.10X.trt.degree_dist <- degree_distribution(R3.10X.trt.net)
# Step 2: Fit the power-law distribution
R3.10X.trt.pl_fit <- fit_power_law(R3.10X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
R3.10X.trt.pl_fit <- as.data.frame(R3.10X.trt.pl_fit)
##correct KS pvalue
R3.10X.trt.pl_fit$KS.p <- p.adjust(R3.10X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.3646647, ##KS.p = 0.9529957

##Add additional metrics to dataframe
net_prop.R3.10X.trt.wide.1 <-spread(net_prop.R3.10X.trt.final, metric, value)
net_prop.R3.10X.trt.wide.1 <- net_prop.R3.10X.trt.wide.1 %>% mutate(giant_component_size = '4')
net_prop.R3.10X.trt.wide.1 <- net_prop.R3.10X.trt.wide.1 %>% mutate(Modularity = '0.7346939')
net_prop.R3.10X.trt.wide.1 <- net_prop.R3.10X.trt.wide.1 %>% mutate(KS.stat = '0.3646647')
net_prop.R3.10X.trt.wide.1 <- net_prop.R3.10X.trt.wide.1 %>% mutate(KS.p = '0.9529957')
net_prop.R3.10X.trt.final <- gather(net_prop.R3.10X.trt.wide.1, metric, value, 4:23)
net_prop.R3.10X.trt.final <- net_prop.R3.10X.trt.final %>% mutate(Condition = 'Condition 11')
```

```{r}
##R6
##CZ4810X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R6.10X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.10X.trt  <- prune_samples(sample_data(R6.10X.trt)$Treatment == "Biostimulant", R6.10X.trt )
R6.10X.trt  <- prune_samples(sample_data(R6.10X.trt )$Cultivar == "CZ4810X", R6.10X.trt )

##Co-occurence table
R6.10X.trt.table <- co_occurrence(R6.10X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R6.10X.trt.table$p <-p.adjust(R6.10X.trt.table$p, "fdr")
R6.10X.trt.table <- filter(R6.10X.trt.table,p <= 0.05)

##network_layout_ps function
R6.10X.trt.layout <- network_layout_ps(R6.10X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R6.10X.trt, co_occurrence_table = R6.10X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R6.10X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R6.10X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R6.10X.trt.net <- network_ps(R6.10X.trt, treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.10X.trt <- net_properties(R6.10X.trt.net)
node_prop.R6.10X.trt <- node_properties(R6.10X.trt.net)
hub_score.R6.10X.trt <- hub_score(R6.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.10X.trt <- authority_score(R6.10X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.10X.trt <- neighborhood.connectivity(R6.10X.trt.net, vertices = V(R6.10X.trt.net), mode = "all")
neighbor.R6.10X.trt <- as.data.frame(neighbor.R6.10X.trt)

##Network properties for comparison
net_prop.R6.10X.trt <- as.data.frame(net_prop.R6.10X.trt)
net_prop.R6.10X.trt <- tibble::rownames_to_column(net_prop.R6.10X.trt, "metric")
net_prop.R6.10X.trt <- net_prop.R6.10X.trt %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.10X.trt <- net_prop.R6.10X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.R6.10X.trt <- net_prop.R6.10X.trt %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R6.10X.trt <- as.data.frame(hub_score.R6.10X.trt)

hub_score.R6.10X.trt.1 <- filter(hub_score.R6.10X.trt,vector >= 0.2)
##6

net_prop.R6.10X.trt <- net_prop.R6.10X.trt %>% mutate(Hub_Count = '6')

##Add mean neighborhood connectivity
mean(neighbor.R6.10X.trt$neighbor.R6.10X.trt) #3.25
net_prop.R6.10X.trt <- net_prop.R6.10X.trt %>% mutate(Mean_neighborhood = '3.25')
net_prop.R6.10X.trt.wide <-spread(net_prop.R6.10X.trt, metric, value)
net_prop.R6.10X.trt.final <- gather(net_prop.R6.10X.trt.wide, metric, value, 4:19)

##Node table for heatmap
R6.10X.trt.node.names <- as.data.frame(V(R6.10X.trt.net)$name)
R6.10X.trt.node.names <- R6.10X.trt.node.names %>% mutate(Developmental_Stage = 'R6')
R6.10X.trt.node.names <- R6.10X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
R6.10X.trt.node.names <- R6.10X.trt.node.names %>% mutate(Cultivar = 'CZ4810X')
names(R6.10X.trt.node.names)[names(R6.10X.trt.node.names) == "V(R6.10X.trt.net)$name"] <- "genus"
R6.10X.trt.node.names <- R6.10X.trt.node.names %>% mutate(Condition = 'Condition 12')
hub_score.R6.10X.trt$genus <- rownames(hub_score.R6.10X.trt)
R6.10X.trt.heat <- merge(R6.10X.trt.node.names, hub_score.R6.10X.trt, by = c("genus"))
R6.10X.trt.heat <-subset(R6.10X.trt.heat, select = -c(8:27))

##Additional measurements
R6.10X.trt.giant_component_size <- max(components(R6.10X.trt.net)$csize) ##6
R6.10X.trt.giant_component_size <- as.data.frame(R6.10X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
R6.10X.trt.community <- cluster_walktrap(R6.10X.trt.net)

##Get the community membership vector and calculate modularity
R6.10X.trt.membership <- R6.10X.trt.community$membership
R6.10X.trt.modularity <- modularity(R6.10X.trt.community) ##0.7692308
R6.10X.trt.modularity <- as.data.frame(R6.10X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.10X.trt.degree_dist <- degree_distribution(R6.10X.trt.net)
# Step 2: Fit the power-law distribution
R6.10X.trt.pl_fit <- fit_power_law(R6.10X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
R6.10X.trt.pl_fit <- as.data.frame(R6.10X.trt.pl_fit)
##correct KS pvalue
R6.10X.trt.pl_fit$KS.p <- p.adjust(R6.10X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.3333333, ##KS.p = 0.8927783

##Add additional metrics to dataframe
net_prop.R6.10X.trt.wide.1 <-spread(net_prop.R6.10X.trt.final, metric, value)
net_prop.R6.10X.trt.wide.1 <- net_prop.R6.10X.trt.wide.1 %>% mutate(giant_component_size = '6')
net_prop.R6.10X.trt.wide.1 <- net_prop.R6.10X.trt.wide.1 %>% mutate(Modularity = '0.7692308')
net_prop.R6.10X.trt.wide.1 <- net_prop.R6.10X.trt.wide.1 %>% mutate(KS.stat = '0.3333333')
net_prop.R6.10X.trt.wide.1 <- net_prop.R6.10X.trt.wide.1 %>% mutate(KS.p = '0.8927783')
net_prop.R6.10X.trt.final <- gather(net_prop.R6.10X.trt.wide.1, metric, value, 4:23)
net_prop.R6.10X.trt.final <- net_prop.R6.10X.trt.final %>% mutate(Condition = 'Condition 12')
```

```{r}
##V1
##CZ4979X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V1.79X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.79X.trt <- prune_samples(sample_data(V1.79X.trt)$Treatment == "Biostimulant", V1.79X.trt)
V1.79X.trt <- prune_samples(sample_data(V1.79X.trt)$Cultivar == "CZ4979X", V1.79X.trt)

##Co-occurence table
V1.79X.trt.table <- co_occurrence(V1.79X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V1.79X.trt.table$p <-p.adjust(V1.79X.trt.table$p, "fdr")
V1.79X.trt.table <- filter(V1.79X.trt.table,p <= 0.05)

##network_layout_ps function
V1.79X.trt.layout <- network_layout_ps(V1.79X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.79X.trt, co_occurrence_table = V1.79X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V1.79X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V1.79X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V1.79X.trt.net <- network_ps(V1.79X.trt, treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.79X.trt <- net_properties(V1.79X.trt.net)
node_prop.V1.79X.trt <- node_properties(V1.79X.trt.net)
hub_score.V1.79X.trt <- hub_score(V1.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.79X.trt <- authority_score(V1.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.79X.trt <- neighborhood.connectivity(V1.79X.trt.net, vertices = V(V1.79X.trt.net), mode = "all")
neighbor.V1.79X.trt <- as.data.frame(neighbor.V1.79X.trt)

##Network properties for comparison
net_prop.V1.79X.trt <- as.data.frame(net_prop.V1.79X.trt)
net_prop.V1.79X.trt <- tibble::rownames_to_column(net_prop.V1.79X.trt, "metric")
net_prop.V1.79X.trt <- net_prop.V1.79X.trt %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.79X.trt <- net_prop.V1.79X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.V1.79X.trt <- net_prop.V1.79X.trt %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V1.79X.trt <- as.data.frame(hub_score.V1.79X.trt)

hub_score.V1.79X.trt.1 <- filter(hub_score.V1.79X.trt,vector >= 0.2)
##6

net_prop.V1.79X.trt <- net_prop.V1.79X.trt %>% mutate(Hub_Count = '6')

##Add mean neighborhood connectivity
mean(neighbor.V1.79X.trt$neighbor.V1.79X.trt) #3.375
net_prop.V1.79X.trt <- net_prop.V1.79X.trt %>% mutate(Mean_neighborhood = '3.375')
net_prop.V1.79X.trt.wide <-spread(net_prop.V1.79X.trt, metric, value)
net_prop.V1.79X.trt.final <- gather(net_prop.V1.79X.trt.wide, metric, value, 4:19)

##Node table for heatmap
V1.79X.trt.node.names <- as.data.frame(V(V1.79X.trt.net)$name)
V1.79X.trt.node.names <- V1.79X.trt.node.names %>% mutate(Developmental_Stage = 'V1')
V1.79X.trt.node.names <- V1.79X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
V1.79X.trt.node.names <- V1.79X.trt.node.names %>% mutate(Cultivar = 'CZ4979X')
names(V1.79X.trt.node.names)[names(V1.79X.trt.node.names) == "V(V1.79X.trt.net)$name"] <- "genus"
V1.79X.trt.node.names <- V1.79X.trt.node.names %>% mutate(Condition = 'Condition 13')
hub_score.V1.79X.trt$genus <- rownames(hub_score.V1.79X.trt)
V1.79X.trt.heat <- merge(V1.79X.trt.node.names, hub_score.V1.79X.trt, by = c("genus"))
V1.79X.trt.heat <-subset(V1.79X.trt.heat, select = -c(8:27))

##Additional measurements
V1.79X.trt.giant_component_size <- max(components(V1.79X.trt.net)$csize) ##6
V1.79X.trt.giant_component_size <- as.data.frame(V1.79X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
V1.79X.trt.community <- cluster_walktrap(V1.79X.trt.net)

##Get the community membership vector and calculate modularity
V1.79X.trt.membership <- V1.79X.trt.community$membership
V1.79X.trt.modularity <- modularity(V1.79X.trt.community) ##0.617284
V1.79X.trt.modularity <- as.data.frame(V1.79X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.79X.trt.degree_dist <- degree_distribution(V1.79X.trt.net)
# Step 2: Fit the power-law distribution
V1.79X.trt.pl_fit <- fit_power_law(V1.79X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
V1.79X.trt.pl_fit <- as.data.frame(V1.79X.trt.pl_fit)
##correct KS pvalue
V1.79X.trt.pl_fit$KS.p <- p.adjust(V1.79X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.4435365, ##KS.p = 0.5965869

##Add additional metrics to dataframe
net_prop.V1.79X.trt.wide.1 <-spread(net_prop.V1.79X.trt.final, metric, value)
net_prop.V1.79X.trt.wide.1 <- net_prop.V1.79X.trt.wide.1 %>% mutate(giant_component_size = '6')
net_prop.V1.79X.trt.wide.1 <- net_prop.V1.79X.trt.wide.1 %>% mutate(Modularity = '0.617284')
net_prop.V1.79X.trt.wide.1 <- net_prop.V1.79X.trt.wide.1 %>% mutate(KS.stat = '0.4435365')
net_prop.V1.79X.trt.wide.1 <- net_prop.V1.79X.trt.wide.1 %>% mutate(KS.p = '0.5965869')
net_prop.V1.79X.trt.final <- gather(net_prop.V1.79X.trt.wide.1, metric, value, 4:23)
net_prop.V1.79X.trt.final <- net_prop.V1.79X.trt.final %>% mutate(Condition = 'Condition 13')
```

```{r}
##V6
##CZ4979X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
V6.79X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.79X.trt <- prune_samples(sample_data(V6.79X.trt)$Treatment == "Biostimulant", V6.79X.trt)
V6.79X.trt <- prune_samples(sample_data(V6.79X.trt)$Cultivar == "CZ4979X", V6.79X.trt)

##Co-occurence table
V6.79X.trt.table <- co_occurrence(V6.79X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
V6.79X.trt.table$p <-p.adjust(V6.79X.trt.table$p, "fdr")
V6.79X.trt.table <- filter(V6.79X.trt.table,p <= 0.05)

##network_layout_ps function
V6.79X.trt.layout <- network_layout_ps(V6.79X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.79X.trt, co_occurrence_table = V6.79X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = V6.79X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("V6.79X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
V6.79X.trt.net <- network_ps(V6.79X.trt, treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.79X.trt <- net_properties(V6.79X.trt.net)
node_prop.V6.79X.trt <- node_properties(V6.79X.trt.net)
hub_score.V6.79X.trt <- hub_score(V6.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.79X.trt <- authority_score(V6.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.79X.trt <- neighborhood.connectivity(V6.79X.trt.net, vertices = V(V6.79X.trt.net), mode = "all")
neighbor.V6.79X.trt <- as.data.frame(neighbor.V6.79X.trt)

##Network properties for comparison
net_prop.V6.79X.trt <- as.data.frame(net_prop.V6.79X.trt)
net_prop.V6.79X.trt <- tibble::rownames_to_column(net_prop.V6.79X.trt, "metric")
net_prop.V6.79X.trt <- net_prop.V6.79X.trt %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.79X.trt <- net_prop.V6.79X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.V6.79X.trt <- net_prop.V6.79X.trt %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V6.79X.trt <- as.data.frame(hub_score.V6.79X.trt)

hub_score.V6.79X.trt.1 <- filter(hub_score.V6.79X.trt,vector >= 0.2)
##6

net_prop.V6.79X.trt <- net_prop.V6.79X.trt %>% mutate(Hub_Count = '6')

##Add mean neighborhood connectivity
mean(neighbor.V6.79X.trt$neighbor.V6.79X.trt) #3.894737
net_prop.V6.79X.trt <- net_prop.V6.79X.trt %>% mutate(Mean_neighborhood = '3.894737')
net_prop.V6.79X.trt.wide <-spread(net_prop.V6.79X.trt, metric, value)
net_prop.V6.79X.trt.final <- gather(net_prop.V6.79X.trt.wide, metric, value, 4:19)

##Node table for heatmap
V6.79X.trt.node.names <- as.data.frame(V(V6.79X.trt.net)$name)
V6.79X.trt.node.names <- V6.79X.trt.node.names %>% mutate(Developmental_Stage = 'V6')
V6.79X.trt.node.names <- V6.79X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
V6.79X.trt.node.names <- V6.79X.trt.node.names %>% mutate(Cultivar = 'CZ4979X')
names(V6.79X.trt.node.names)[names(V6.79X.trt.node.names) == "V(V6.79X.trt.net)$name"] <- "genus"
V6.79X.trt.node.names <- V6.79X.trt.node.names %>% mutate(Condition = 'Condition 14')
hub_score.V6.79X.trt$genus <- rownames(hub_score.V6.79X.trt)
V6.79X.trt.heat <- merge(V6.79X.trt.node.names, hub_score.V6.79X.trt, by = c("genus"))
V6.79X.trt.heat <-subset(V6.79X.trt.heat, select = -c(8:27))

##Additional measurements
V6.79X.trt.giant_component_size <- max(components(V6.79X.trt.net)$csize) ##6
V6.79X.trt.giant_component_size <- as.data.frame(V6.79X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
V6.79X.trt.community <- cluster_walktrap(V6.79X.trt.net)

##Get the community membership vector and calculate modularity
V6.79X.trt.membership <- V6.79X.trt.community$membership
V6.79X.trt.modularity <- modularity(V6.79X.trt.community) ##0.7100073
V6.79X.trt.modularity <- as.data.frame(V6.79X.trt.modularity)

##V6.79X.trt.q <- modularity_matrix(V6.79X.trt.net, membership = V6.79X.trt.membership)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.79X.trt.degree_dist <- degree_distribution(V6.79X.trt.net)
# Step 2: Fit the power-law distribution
V6.79X.trt.pl_fit <- fit_power_law(V6.79X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
V6.79X.trt.pl_fit <- as.data.frame(V6.79X.trt.pl_fit)
##correct KS pvalue
V6.79X.trt.pl_fit$KS.p <- p.adjust(V6.79X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.2343015, ##KS.p = 0.9965528

##Add additional metrics to dataframe
net_prop.V6.79X.trt.wide.1 <-spread(net_prop.V6.79X.trt.final, metric, value)
net_prop.V6.79X.trt.wide.1 <- net_prop.V6.79X.trt.wide.1 %>% mutate(giant_component_size = '6')
net_prop.V6.79X.trt.wide.1 <- net_prop.V6.79X.trt.wide.1 %>% mutate(Modularity = '0.7100073')
net_prop.V6.79X.trt.wide.1 <- net_prop.V6.79X.trt.wide.1 %>% mutate(KS.stat = '0.2343015')
net_prop.V6.79X.trt.wide.1 <- net_prop.V6.79X.trt.wide.1 %>% mutate(KS.p = '0.9965528')
net_prop.V6.79X.trt.final <- gather(net_prop.V6.79X.trt.wide.1, metric, value, 4:23)
net_prop.V6.79X.trt.final <- net_prop.V6.79X.trt.final %>% mutate(Condition = 'Condition 14')
```

```{r}
##R3
##CZ4979X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R3.79X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.79X.trt <- prune_samples(sample_data(R3.79X.trt)$Treatment == "Biostimulant", R3.79X.trt)
R3.79X.trt <- prune_samples(sample_data(R3.79X.trt)$Cultivar == "CZ4979X", R3.79X.trt)

##Co-occurence table
R3.79X.trt.table <- co_occurrence(R3.79X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R3.79X.trt.table$p <-p.adjust(R3.79X.trt.table$p, "fdr")
R3.79X.trt.table <- filter(R3.79X.trt.table,p <= 0.05)

##network_layout_ps function
R3.79X.trt.layout <- network_layout_ps(R3.79X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R3.79X.trt, co_occurrence_table = R3.79X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R3.79X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R3.79X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R3.79X.trt.net <- network_ps(R3.79X.trt, treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.79X.trt <- net_properties(R3.79X.trt.net)
node_prop.R3.79X.trt <- node_properties(R3.79X.trt.net)
hub_score.R3.79X.trt <- hub_score(R3.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.79X.trt <- authority_score(R3.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.79X.trt <- neighborhood.connectivity(R3.79X.trt.net, vertices = V(R3.79X.trt.net), mode = "all")
neighbor.R3.79X.trt <- as.data.frame(neighbor.R3.79X.trt)

##Network properties for comparison
net_prop.R3.79X.trt <- as.data.frame(net_prop.R3.79X.trt)
net_prop.R3.79X.trt <- tibble::rownames_to_column(net_prop.R3.79X.trt, "metric")
net_prop.R3.79X.trt <- net_prop.R3.79X.trt %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.79X.trt <- net_prop.R3.79X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.R3.79X.trt <- net_prop.R3.79X.trt %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R3.79X.trt <- as.data.frame(hub_score.R3.79X.trt)

hub_score.R3.79X.trt.1 <- filter(hub_score.R3.79X.trt,vector >= 0.2)
##12

net_prop.R3.79X.trt <- net_prop.R3.79X.trt %>% mutate(Hub_Count = '12')

##Add mean neighborhood connectivity
mean(neighbor.R3.79X.trt$neighbor.R3.79X.trt) #3.666667
net_prop.R3.79X.trt <- net_prop.R3.79X.trt %>% mutate(Mean_neighborhood = '3.666667')
net_prop.R3.79X.trt.wide <-spread(net_prop.R3.79X.trt, metric, value)
net_prop.R3.79X.trt.final <- gather(net_prop.R3.79X.trt.wide, metric, value, 4:19)

##Node table for heatmap
R3.79X.trt.node.names <- as.data.frame(V(R3.79X.trt.net)$name)
R3.79X.trt.node.names <- R3.79X.trt.node.names %>% mutate(Developmental_Stage = 'R3')
R3.79X.trt.node.names <- R3.79X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
R3.79X.trt.node.names <- R3.79X.trt.node.names %>% mutate(Cultivar = 'CZ4979X')
names(R3.79X.trt.node.names)[names(R3.79X.trt.node.names) == "V(R3.79X.trt.net)$name"] <- "genus"
R3.79X.trt.node.names <- R3.79X.trt.node.names %>% mutate(Condition = 'Condition 15')
hub_score.R3.79X.trt$genus <- rownames(hub_score.R3.79X.trt)
R3.79X.trt.heat <- merge(R3.79X.trt.node.names, hub_score.R3.79X.trt, by = c("genus"))
R3.79X.trt.heat <-subset(R3.79X.trt.heat, select = -c(8:27))

##Additional measurements
R3.79X.trt.giant_component_size <- max(components(R3.79X.trt.net)$csize) ##6
R3.79X.trt.giant_component_size <- as.data.frame(R3.79X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
R3.79X.trt.community <- cluster_walktrap(R3.79X.trt.net)

##Get the community membership vector and calculate modularity
R3.79X.trt.membership <- R3.79X.trt.community$membership
R3.79X.trt.modularity <- modularity(R3.79X.trt.community) ##0.7973554
R3.79X.trt.modularity <- as.data.frame(R3.79X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.79X.trt.degree_dist <- degree_distribution(R3.79X.trt.net)
# Step 2: Fit the power-law distribution
R3.79X.trt.pl_fit <- fit_power_law(R3.79X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
R3.79X.trt.pl_fit <- as.data.frame(R3.79X.trt.pl_fit)
##correct KS pvalue
R3.79X.trt.pl_fit$KS.p <- p.adjust(R3.79X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.281497, ##KS.p = 0.9091766

##Add additional metrics to dataframe
net_prop.R3.79X.trt.wide.1 <-spread(net_prop.R3.79X.trt.final, metric, value)
net_prop.R3.79X.trt.wide.1 <- net_prop.R3.79X.trt.wide.1 %>% mutate(giant_component_size = '6')
net_prop.R3.79X.trt.wide.1 <- net_prop.R3.79X.trt.wide.1 %>% mutate(Modularity = '0.7973554')
net_prop.R3.79X.trt.wide.1 <- net_prop.R3.79X.trt.wide.1 %>% mutate(KS.stat = '0.281497')
net_prop.R3.79X.trt.wide.1 <- net_prop.R3.79X.trt.wide.1 %>% mutate(KS.p = '0.9091766')
net_prop.R3.79X.trt.final <- gather(net_prop.R3.79X.trt.wide.1, metric, value, 4:23)
net_prop.R3.79X.trt.final <- net_prop.R3.79X.trt.final %>% mutate(Condition = 'Condition 15')
```

```{r}
##R6
##CZ4979X
##Trt
##Use filtered_obj, which is conglomerated at genus level

##Create phyloseq object
R6.79X.trt <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.79X.trt <- prune_samples(sample_data(R6.79X.trt)$Treatment == "Biostimulant", R6.79X.trt)
R6.79X.trt <- prune_samples(sample_data(R6.79X.trt)$Cultivar == "CZ4979X", R6.79X.trt)

##Co-occurence table
R6.79X.trt.table <- co_occurrence(R6.79X.trt, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
R6.79X.trt.table$p <-p.adjust(R6.79X.trt.table$p, "fdr")
R6.79X.trt.table <- filter(R6.79X.trt.table,p <= 0.05)

##network_layout_ps function
R6.79X.trt.layout <- network_layout_ps(R6.79X.trt, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.trt.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R6.79X.trt, co_occurrence_table = R6.79X.trt.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = R6.79X.trt.layout, ##change here
                      negative_positive_colors = edge_col.2) +
  theme(legend.position = 'none')

ggsave("R6.79X.trt.net.png", width = 10, height = 10, units = "in", dpi = 900)

##network_ps function 
R6.79X.trt.net <- network_ps(R6.79X.trt, treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.trt.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.79X.trt <- net_properties(R6.79X.trt.net)
node_prop.R6.79X.trt <- node_properties(R6.79X.trt.net)
hub_score.R6.79X.trt <- hub_score(R6.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.79X.trt <- authority_score(R6.79X.trt.net, scale = TRUE, weights = NULL, options = arpack_defaults)
connectivity <- components(R6.79X.trt.net)$no

##package influential
neighbor.R6.79X.trt <- neighborhood.connectivity(R6.79X.trt.net, vertices = V(R6.79X.trt.net), mode = "all")
neighbor.R6.79X.trt <- as.data.frame(neighbor.R6.79X.trt)

##Network properties for comparison
net_prop.R6.79X.trt <- as.data.frame(net_prop.R6.79X.trt)
net_prop.R6.79X.trt <- tibble::rownames_to_column(net_prop.R6.79X.trt, "metric")
net_prop.R6.79X.trt <- net_prop.R6.79X.trt %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.79X.trt <- net_prop.R6.79X.trt %>% mutate(Treatment = 'Biostimulant')
net_prop.R6.79X.trt <- net_prop.R6.79X.trt %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R6.79X.trt <- as.data.frame(hub_score.R6.79X.trt)

hub_score.R6.79X.trt.1 <- filter(hub_score.R6.79X.trt,vector >= 0.2)
##4

net_prop.R6.79X.trt <- net_prop.R6.79X.trt %>% mutate(Hub_Count = '4')

##Add mean neighborhood connectivity
mean(neighbor.R6.79X.trt$neighbor.R6.79X.trt) #2.181818
net_prop.R6.79X.trt <- net_prop.R6.79X.trt %>% mutate(Mean_neighborhood = '2.181818')
net_prop.R6.79X.trt.wide <-spread(net_prop.R6.79X.trt, metric, value)
net_prop.R6.79X.trt.final <- gather(net_prop.R6.79X.trt.wide, metric, value, 4:19)

##Node table for heatmap
R6.79X.trt.node.names <- as.data.frame(V(R6.79X.trt.net)$name)
R6.79X.trt.node.names <- R6.79X.trt.node.names %>% mutate(Developmental_Stage = 'R6')
R6.79X.trt.node.names <- R6.79X.trt.node.names %>% mutate(Treatment = 'Biostimulant')
R6.79X.trt.node.names <- R6.79X.trt.node.names %>% mutate(Cultivar = 'CZ4979X')
names(R6.79X.trt.node.names)[names(R6.79X.trt.node.names) == "V(R6.79X.trt.net)$name"] <- "genus"
R6.79X.trt.node.names <- R6.79X.trt.node.names %>% mutate(Condition = 'Condition 16')
hub_score.R6.79X.trt$genus <- rownames(hub_score.R6.79X.trt)
R6.79X.trt.heat <- merge(R6.79X.trt.node.names, hub_score.R6.79X.trt, by = c("genus"))
R6.79X.trt.heat <-subset(R6.79X.trt.heat, select = -c(8:27))

##Additional measurements
R6.79X.trt.giant_component_size <- max(components(R6.79X.trt.net)$csize) ##4
R6.79X.trt.giant_component_size <- as.data.frame(R6.79X.trt.giant_component_size)

##community detection
# Perform community detection using walktrap method
R6.79X.trt.community <- cluster_walktrap(R6.79X.trt.net)

##Get the community membership vector and calculate modularity
R6.79X.trt.membership <- R6.79X.trt.community$membership
R6.79X.trt.modularity <- modularity(R6.79X.trt.community) ##0.84375
R6.79X.trt.modularity <- as.data.frame(R6.79X.trt.modularity)

##Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.79X.trt.degree_dist <- degree_distribution(R6.79X.trt.net)
# Step 2: Fit the power-law distribution
R6.79X.trt.pl_fit <- fit_power_law(R6.79X.trt.degree_dist, implementation = "plfit", force.continuous = TRUE)
R6.79X.trt.pl_fit <- as.data.frame(R6.79X.trt.pl_fit)
##correct KS pvalue
R6.79X.trt.pl_fit$KS.p <- p.adjust(R6.79X.trt.pl_fit$KS.p, "fdr") ##KS.stat = 0.3646647, ##KS.p = 0.9529957

##Add additional metrics to dataframe
net_prop.R6.79X.trt.wide.1 <-spread(net_prop.R6.79X.trt.final, metric, value)
net_prop.R6.79X.trt.wide.1 <- net_prop.R6.79X.trt.wide.1 %>% mutate(giant_component_size = '4')
net_prop.R6.79X.trt.wide.1 <- net_prop.R6.79X.trt.wide.1 %>% mutate(Modularity = '0.84375')
net_prop.R6.79X.trt.wide.1 <- net_prop.R6.79X.trt.wide.1 %>% mutate(KS.stat = '0.3646647')
net_prop.R6.79X.trt.wide.1 <- net_prop.R6.79X.trt.wide.1 %>% mutate(KS.p = '0.9529957')
net_prop.R6.79X.trt.final <- gather(net_prop.R6.79X.trt.wide.1, metric, value, 4:23)
net_prop.R6.79X.trt.final <- net_prop.R6.79X.trt.final %>% mutate(Condition = 'Condition 16')
```

```{r}
##Visualize Network Properties

Network_Properties <- rbind(net_prop.R6.79X.ctrl.final, net_prop.R6.79X.trt.final, net_prop.R6.10X.ctrl.final, net_prop.R6.10X.trt.final, 
                            net_prop.R3.79X.ctrl.final, net_prop.R3.79X.trt.final, net_prop.R3.10X.trt.final, net_prop.R3.10X.ctrl.final,
                            net_prop.V6.79X.trt.final, net_prop.V6.79X.ctrl.final, net_prop.V6.10X.trt.final, net_prop.V6.10X.ctrl.final, 
                            net_prop.V1.79X.ctrl.final, net_prop.V1.79X.trt.final, net_prop.V1.10X.ctrl.final, net_prop.V1.10X.trt.final)

Network_Properties$Developmental_Stage[Network_Properties$Developmental_Stage == "R3"] <- "R2"
Network_Properties$Developmental_Stage <- factor(Network_Properties$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))
Network_Properties$value <- as.numeric(Network_Properties$value)

##Filter out metrics we don't want to plot
Network_Properties <- Network_Properties[Network_Properties$metric != "num.pos.edges" & Network_Properties$metric != "num.neg.edges" & Network_Properties$metric != "centralization.betweenness"
& Network_Properties$metric != "centralization.closeness" & Network_Properties$metric != "clustering.coefficient" & 
  Network_Properties$metric != "average.path.length" & Network_Properties$metric != "diameter" &
  Network_Properties$metric != "edge.connectivity" &
  Network_Properties$metric != "KS.p" &
  Network_Properties$metric != "Mean_neighborhood", ] 

##Rename metrics
Network_Properties$metric [Network_Properties$metric == 'average.degree'] <- 'Mean degree'
Network_Properties$metric [Network_Properties$metric == 'centralization.degree'] <- 'Centralization degree'
Network_Properties$metric [Network_Properties$metric == 'connectance'] <- 'Connectance'
Network_Properties$metric [Network_Properties$metric == 'Hub_Count'] <- 'Hub count'
##Network_Properties$metric [Network_Properties$metric == 'Mean_neighborhood'] <- 'MNC'
Network_Properties$metric [Network_Properties$metric == 'no.clusters'] <- 'Cluster count'
Network_Properties$metric [Network_Properties$metric == 'num.edges'] <- 'Edge count'
Network_Properties$metric [Network_Properties$metric == 'num.vertices'] <- 'Node count'
Network_Properties$metric [Network_Properties$metric == 'KS.stat'] <- 'KS stat'
Network_Properties$metric [Network_Properties$metric == 'giant_component_size'] <- 'Giant component size'

##visualize
ggplot(Network_Properties, aes(x = Developmental_Stage, y = value, fill = interaction(Treatment, Cultivar))) + 
  geom_col(aes(y = 0), alpha = 0, width = 1) +  # Temporary invisible layer
  geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="V6") -0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="V6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
 geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="R6")-0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="R6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
  geom_col(stat = "identity", position=position_dodge(0.9), colour="black", width=0.7) +
  facet_wrap(~metric, scales = "free", nrow = 2) +
  theme_bw() +
  scale_fill_manual(values=c("lightblue3", "antiquewhite", "#8c3800", "#c0c0c0")) +
  theme(axis.title = element_blank(), axis.text.y = element_text(size = 12, color = "black"), axis.ticks = element_blank(), 
        strip.text = element_text(size = 14, face = "bold"), axis.text.x = element_text(size = 12, color = "black", face = "bold"), 
        legend.title = element_text(size = 12, face = "bold"), legend.text = element_text(size = 10), legend.position = "top") +
  labs(fill = "Treatment + Cultivar") +
  guides(fill=guide_legend(nrow=1,byrow=TRUE))

#alpha = 0.8, 
ggsave("prelim_network_stats.2.png", width = 13, height = 5, units = "in", dpi = 900)
```

```{r}
##Let's identify core and unique nodes
network.heatmap <- rbind(R6.79X.trt.heat, R3.79X.trt.heat, V6.79X.trt.heat, V1.79X.trt.heat,
                         R6.10X.trt.heat, R3.10X.trt.heat, V6.10X.trt.heat, V1.10X.trt.heat,
                         R6.79X.ctrl.heat, R3.79X.ctrl.heat, V6.79X.ctrl.heat, V1.79X.ctrl.heat,
                         R6.10X.ctrl.heat, R3.10X.ctrl.heat, V6.10X.ctrl.heat, V1.10X.ctrl.heat)

##Modify for heatmap
network.heatmap.1 <- subset(network.heatmap, select = -c(2:4, 7))
network.heatmap.wide <- spread(network.heatmap.1, Condition, vector)
rownames(network.heatmap.wide) <- network.heatmap.wide$genus
network.heatmap.wide$genus <- NULL
net.col.order <- c("Condition 1", "Condition 2", "Condition 3", "Condition 4", "Condition 5", "Condition 6", "Condition 7", "Condition 8", 
                   "Condition 9", "Condition 10", "Condition 11", "Condition 12", "Condition 13", "Condition 14", "Condition 15", "Condition 16")

network.heatmap.wide.1 <- network.heatmap.wide[, net.col.order]

##Row metadata
network.heatmap$Developmental_Stage <- factor(network.heatmap$Developmental_Stage, levels = c("V1", "V6", "R3", "R6"))

net.colours <- list('Treatment' = c('Control' = "#c3d6ce", 'Biostimulant' = "#c27668"),
  'Cultivar' = c('CZ4979X' = "#ba7233", 'CZ4810X' = "#ced1af"),
  'Growth Stage' =c('V1' = "#9b332b", 'V6' = "#697852", 'R2' = "#2b4655", 'R6' = "#a9845b"))

net.heat.meta <- network.heatmap[!duplicated(network.heatmap$Condition), ]
net.heat.meta <- subset(net.heat.meta, select = -c(1,6:7))
rownames(net.heat.meta) <- net.heat.meta$Condition
net.heat.meta$Condition <- NULL
net.heat.meta <- net.heat.meta[, c("Treatment", "Cultivar", "Developmental_Stage")]
names(net.heat.meta)[3] <- "Developmental Stage"
net.heat.meta <- net.heat.meta[net.col.order, ]
names(net.heat.meta)[names(net.heat.meta) == "Developmental Stage"] <- "Growth Stage"

legend_sizes <- list(
  Treatment = 2,
  Cultivar = 2,
  `Developmental Stage` = 2)

net.colAnn <- rowAnnotation(df = net.heat.meta,
  col = net.colours,
  annotation_width = unit(c(1, 2), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"))

##Column_metadata
tax_info <- as.data.frame(tax_table(merged_phylo))
tax_info <-rownames_to_column(tax_info, var = "RowNames")
tax_info <- subset(tax_info, select = -c(1,3:6, 8))
tax_info$taxtype <- ifelse(tax_info$kingdom == "Bacteria", "Prokaryote", "Eukaryote")
tax_info.1 <- subset(tax_info, select = -c(1))
tax_info.1 <- tax_info.1[!duplicated(tax_info.1$genus), ]
tax_info.2 <- merge(tax_info.1, network.heatmap.1, by = c("genus"))
tax_info.2 <- tax_info.2[!duplicated(tax_info.2$genus), ]
rownames(tax_info.2) <- tax_info.2$genus
tax_info.2$genus <- NULL
tax_info.2$Condition <- NULL
tax_info.2$vector <- NULL

tax_info.3 <- tax_info.2 %>% rename(Domain = taxtype)

tax.colours <- list('Domain' = c('Prokaryote' = "#a8c0a8", 'Eukaryote' = "#a890a8"))

##switched names for row and column
net.rowAnn <- HeatmapAnnotation(df = tax_info.3,   
  which = 'col',
  col = tax.colours,
  annotation_width = unit(c(1, 2), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"), show_legend = FALSE)

##Legend
lgd_list_3 <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12),title = "Hub Score",
        border = "black", lwd = 1,  
        title_position = "topcenter",
        at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0), legend_width = unit(10, "cm"),
        direction = "horizontal",
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"),
        labels = c("","Hub node", "", "", "", ""))
 

##For stacked bar plot-row
stack.bar.row.1 <- merge(tax_info.1, network.heatmap.1, by = c("genus"))
stack.bar.row.2 <- stack.bar.row.1 %>%
  group_by(Condition, taxtype) %>%
  summarize(count = n()) %>%
  pivot_wider(names_from = taxtype, values_from = count, values_fill = 0) %>%
  ungroup()

##Reorder to match the order of clustered rows in heatmap
stack.bar.row.order <- factor(stack.bar.row.2$Condition, levels = c("Condition 1", "Condition 2", "Condition 3", "Condition 4",
                                                                    "Condition 5", "Condition 6", "Condition 7", "Condition 8", 
                                                                    "Condition 9", "Condition 10", "Condition 11", "Condition 12", 
                                                                    "Condition 13", "Condition 14","Condition 15", "Condition 16"))

stack.bar.row.3 <- stack.bar.row.2[order(stack.bar.row.order), ] %>% column_to_rownames(var="Condition")


##Heatmap anno for stacked bar plot- row
row.stack = HeatmapAnnotation("# nodes"= anno_barplot(stack.bar.row.3, axis_param = list(side = "top"), gp = gpar(fill = c("#a890a8","#a8c0a8"))), 
                               which = c("row"), show_annotation_name = FALSE)

##For stacked bar plot- column
stack.bar.col.1 <- spread(network.heatmap.1, Condition, vector)

##Create matrix of hubs and members
stack.bar.col.2 <- stack.bar.col.1 %>%
  mutate(Hub = rowSums(. >= 0.2, na.rm = TRUE),
         Member = rowSums(. < 0.2 & !is.na(.))) %>%
  select(genus, Hub, Member) %>% column_to_rownames(var="genus")

col.stack = HeatmapAnnotation("# net"= anno_barplot(stack.bar.col.2, axis_param = list(side = "left"), 
                      gp = gpar(fill = c("#8c3800", "lightblue3"))), 
                       which = c("column"),   show_annotation_name = FALSE)


my_palette <- colorRamp2(c(0, 0.2, 1), c("lightblue3", "antiquewhite", "#8c3800")) ##, transparency = 0.5) 

##Plot heatmap
Heatmap(network.heatmap.wide.1, cluster_columns = TRUE, cluster_rows = FALSE, border = TRUE, col=my_palette,
       row_names_gp = gpar(fontface = "italic"), rect_gp = gpar(col = "black", lwd = 1),
       bottom_annotation = net.colAnn, 
       show_column_names = FALSE, ##column_split = split, 
       row_split = core_ann_2$taxtype,
       row_gap = unit(c(2), "mm"),
        row_title_gp = gpar(fontsize = 18))
   
##Transpose and plot
transposed_network.heatmap <- t(network.heatmap.wide.1)

net.heatmap.1 <- 
  Heatmap(transposed_network.heatmap, cluster_columns = FALSE, cluster_rows = TRUE, border = TRUE, col=my_palette, na_col = "gray57",
       column_names_gp = gpar(fontface = "italic", fontsize = 10),
       heatmap_legend_param = lgd_list_3,
        rect_gp = gpar(col = "black", lwd = 1),
       right_annotation = net.colAnn,
       bottom_annotation = net.rowAnn,
       column_split = tax_info.2$taxtype,
        #show_row_names = FALSE,
        ##row_split = net.heat.meta$Developmental_Stage,
        column_gap = unit(c(2), "mm"),
        column_names_rot = 45,
       column_title = NULL, top_annotation = col.stack)

net.heatmap.2 <- net.heatmap.1 + row.stack 

##https://jokergoo.github.io/ComplexHeatmap/reference/draw-HeatmapList-method.html
 net.heatmap.3 <- 
   draw(net.heatmap.2, heatmap_legend_side = "bottom", legend_grouping = "original", #annotation_legend_side = "right", 
        padding = unit(c(0, 7, 5, 0), "mm"))
 
 tiff("net.heatmap.2.tiff", width = 15, height = 6, units = "in", res = 900)
net.heatmap.3
dev.off()
```

```{r}
##Look at distribution of hub scores
ggplot(network.heatmap, aes(x=vector)) + 
  annotate(geom = "rect", xmin = 0, xmax = 0.2, ymin = -Inf, ymax = Inf,
           fill = "#d39a2d", alpha = 0.5) +
  annotate(geom = "rect", xmin = 0.2, xmax = 1, ymin = -Inf, ymax = Inf,
           fill = "#2c4b27", alpha = 0.5) +
  geom_density(alpha = 0.5, fill="#591c19") +
  ##geom_histogram(aes(y=..density..), colour="black", fill="#262d42", alpha = 0.5) +
  labs(x = "Hub Score", y = "Node Count", title = "Hub Score Distribution") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, color = "black", size = 18, face = "bold"), 
        axis.text = element_text(size = 12, color = "black"), axis.title = element_text(size = 16, color = "black"))

ggsave("hub_score_distribution.png", width = 8, height = 3, units = "in", dpi = 900)
```


```{r}
##Reconstruct condition-specific networks with Pearson associations
# Calculate the frequency and percentage for each kingdom
V1.10X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.10X.ctrl.pearson <- prune_samples(sample_data(V1.10X.ctrl.pearson)$Treatment == "Control", V1.10X.ctrl.pearson)
V1.10X.ctrl.pearson <- prune_samples(sample_data(V1.10X.ctrl.pearson)$Cultivar == "CZ4810X", V1.10X.ctrl.pearson)

V1.10X.ctrl.pearson.table <- co_occurrence(V1.10X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V1.10X.ctrl.pearson.table$p <- p.adjust(V1.10X.ctrl.pearson.table$p, "fdr")
V1.10X.ctrl.pearson.table <- filter(V1.10X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
V1.10X.ctrl.pearson.layout <- network_layout_ps(V1.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.10X.ctrl.pearson, co_occurrence_table = V1.10X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V1.10X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("V1.10X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V1.10X.ctrl.net.pearson <- network_ps(V1.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.10X.ctrl.pearson <- net_properties(V1.10X.ctrl.net.pearson)
node_prop.V1.10X.ctrl.pearson <- node_properties(V1.10X.ctrl.net.pearson)
hub_score.V1.10X.ctrl.pearson <- hub_score(V1.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.10X.ctrl.pearson <- authority_score(V1.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.10X.ctrl.pearson <- neighborhood.connectivity(V1.10X.ctrl.net.pearson, vertices = V(V1.10X.ctrl.net.pearson), mode = "all")
neighbor.V1.10X.ctrl.pearson <- as.data.frame(neighbor.V1.10X.ctrl.pearson)

##Network properties for comparison
net_prop.V1.10X.ctrl.pearson <- as.data.frame(net_prop.V1.10X.ctrl.pearson)
net_prop.V1.10X.ctrl.pearson <- tibble::rownames_to_column(net_prop.V1.10X.ctrl.pearson, "metric")
net_prop.V1.10X.ctrl.pearson <- net_prop.V1.10X.ctrl.pearson %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.10X.ctrl.pearson <- net_prop.V1.10X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.V1.10X.ctrl.pearson <- net_prop.V1.10X.ctrl.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V1.10X.ctrl.pearson <- as.data.frame(hub_score.V1.10X.ctrl.pearson)
hub_score.V1.10X.ctrl.pearson.1 <- filter(hub_score.V1.10X.ctrl.pearson, vector >= 0.2)

##25

net_prop.V1.10X.ctrl.pearson <- net_prop.V1.10X.ctrl.pearson %>% mutate(Hub_Count = '25')

##Add mean neighborhood connectivity
mean(neighbor.V1.10X.ctrl.pearson$neighbor.V1.10X.ctrl.pearson) ##14.41374
net_prop.V1.10X.ctrl.pearson <- net_prop.V1.10X.ctrl.pearson %>% mutate(Mean_neighborhood = '14.41374')

net_prop.V1.10X.ctrl.wide.pearson <- spread(net_prop.V1.10X.ctrl.pearson, metric, value)
net_prop.V1.10X.ctrl.final.pearson <- gather(net_prop.V1.10X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V1.10X.ctrl.node.names.pearson <- as.data.frame(V(V1.10X.ctrl.net.pearson)$name)
V1.10X.ctrl.node.names.pearson <- V1.10X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'V1')
V1.10X.ctrl.node.names.pearson <- V1.10X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
V1.10X.ctrl.node.names.pearson <- V1.10X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(V1.10X.ctrl.node.names.pearson)[names(V1.10X.ctrl.node.names.pearson) == "V(V1.10X.ctrl.net.pearson)$name"] <- "genus"
V1.10X.ctrl.node.names.pearson <- V1.10X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 1')
hub_score.V1.10X.ctrl.pearson$genus <- rownames(hub_score.V1.10X.ctrl.pearson)
V1.10X.ctrl.heat.pearson <- merge(V1.10X.ctrl.node.names.pearson, hub_score.V1.10X.ctrl.pearson, by = c("genus"))
V1.10X.ctrl.heat.pearson <- subset(V1.10X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
V1.10X.ctrl.giant_component_size.pearson <- max(components(V1.10X.ctrl.net.pearson)$csize) ##29
V1.10X.ctrl.giant_component_size.pearson <- as.data.frame(V1.10X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V1.10X.ctrl.community.pearson <- cluster_walktrap(V1.10X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
V1.10X.ctrl.membership.pearson <- V1.10X.ctrl.community.pearson$membership
V1.10X.ctrl.modularity.pearson <- modularity(V1.10X.ctrl.community.pearson) 
V1.10X.ctrl.modularity.pearson <- as.data.frame(V1.10X.ctrl.modularity.pearson) ##0.7421354

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.10X.ctrl.degree_dist.pearson <- degree_distribution(V1.10X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
V1.10X.ctrl.pl_fit.pearson <- fit_power_law(V1.10X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V1.10X.ctrl.pl_fit.pearson <- as.data.frame(V1.10X.ctrl.pl_fit.pearson)
## Correct KS p-value
V1.10X.ctrl.pl_fit.pearson$KS.p <- p.adjust(V1.10X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1818182, ##KS.p = 0.8602552

## Add additional metrics to the dataframe
net_prop.V1.10X.ctrl.wide.1.pearson <- spread(net_prop.V1.10X.ctrl.final.pearson, metric, value)
net_prop.V1.10X.ctrl.wide.1.pearson <- net_prop.V1.10X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '29')
net_prop.V1.10X.ctrl.wide.1.pearson <- net_prop.V1.10X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.7421354')
net_prop.V1.10X.ctrl.wide.1.pearson <- net_prop.V1.10X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.1818182')
net_prop.V1.10X.ctrl.wide.1.pearson <- net_prop.V1.10X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.8602552')
net_prop.V1.10X.ctrl.final.pearson <- gather(net_prop.V1.10X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.V1.10X.ctrl.final.pearson <- net_prop.V1.10X.ctrl.final.pearson %>% mutate(Condition = 'Condition 1')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
V6.10X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.10X.ctrl.pearson <- prune_samples(sample_data(V6.10X.ctrl.pearson)$Treatment == "Control", V6.10X.ctrl.pearson)
V6.10X.ctrl.pearson <- prune_samples(sample_data(V6.10X.ctrl.pearson)$Cultivar == "CZ4810X", V6.10X.ctrl.pearson)

V6.10X.ctrl.pearson.table <- co_occurrence(V6.10X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V6.10X.ctrl.pearson.table$p <- p.adjust(V6.10X.ctrl.pearson.table$p, "fdr")
V6.10X.ctrl.pearson.table <- filter(V6.10X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
V6.10X.ctrl.pearson.layout <- network_layout_ps(V6.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.10X.ctrl.pearson, co_occurrence_table = V6.10X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V6.10X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V6.10X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V6.10X.ctrl.net.pearson <- network_ps(V6.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.10X.ctrl.pearson <- net_properties(V6.10X.ctrl.net.pearson)
node_prop.V6.10X.ctrl.pearson <- node_properties(V6.10X.ctrl.net.pearson)
hub_score.V6.10X.ctrl.pearson <- hub_score(V6.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.10X.ctrl.pearson <- authority_score(V6.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.10X.ctrl.pearson <- neighborhood.connectivity(V6.10X.ctrl.net.pearson, vertices = V(V6.10X.ctrl.net.pearson), mode = "all")
neighbor.V6.10X.ctrl.pearson <- as.data.frame(neighbor.V6.10X.ctrl.pearson)

##Network properties for comparison
net_prop.V6.10X.ctrl.pearson <- as.data.frame(net_prop.V6.10X.ctrl.pearson)
net_prop.V6.10X.ctrl.pearson <- tibble::rownames_to_column(net_prop.V6.10X.ctrl.pearson, "metric")
net_prop.V6.10X.ctrl.pearson <- net_prop.V6.10X.ctrl.pearson %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.10X.ctrl.pearson <- net_prop.V6.10X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.V6.10X.ctrl.pearson <- net_prop.V6.10X.ctrl.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V6.10X.ctrl.pearson <- as.data.frame(hub_score.V6.10X.ctrl.pearson)
hub_score.V6.10X.ctrl.pearson.1 <- filter(hub_score.V6.10X.ctrl.pearson, vector >= 0.2)

##23

net_prop.V6.10X.ctrl.pearson <- net_prop.V6.10X.ctrl.pearson %>% mutate(Hub_Count = '23')

##Add mean neighborhood connectivity
mean(neighbor.V6.10X.ctrl.pearson$neighbor.V6.10X.ctrl.pearson) ##13.72064
net_prop.V6.10X.ctrl.pearson <- net_prop.V6.10X.ctrl.pearson %>% mutate(Mean_neighborhood = '13.72064')

net_prop.V6.10X.ctrl.wide.pearson <- spread(net_prop.V6.10X.ctrl.pearson, metric, value)
net_prop.V6.10X.ctrl.final.pearson <- gather(net_prop.V6.10X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V6.10X.ctrl.node.names.pearson <- as.data.frame(V(V6.10X.ctrl.net.pearson)$name)
V6.10X.ctrl.node.names.pearson <- V6.10X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'V6')
V6.10X.ctrl.node.names.pearson <- V6.10X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
V6.10X.ctrl.node.names.pearson <- V6.10X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(V6.10X.ctrl.node.names.pearson)[names(V6.10X.ctrl.node.names.pearson) == "V(V6.10X.ctrl.net.pearson)$name"] <- "genus"
V6.10X.ctrl.node.names.pearson <- V6.10X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 2')
hub_score.V6.10X.ctrl.pearson$genus <- rownames(hub_score.V6.10X.ctrl.pearson)
V6.10X.ctrl.heat.pearson <- merge(V6.10X.ctrl.node.names.pearson, hub_score.V6.10X.ctrl.pearson, by = c("genus"))
V6.10X.ctrl.heat.pearson <- subset(V6.10X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
V6.10X.ctrl.giant_component_size.pearson <- max(components(V6.10X.ctrl.net.pearson)$csize) ##24
V6.10X.ctrl.giant_component_size.pearson <- as.data.frame(V6.10X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V6.10X.ctrl.community.pearson <- cluster_walktrap(V6.10X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
V6.10X.ctrl.membership.pearson <- V6.10X.ctrl.community.pearson$membership
V6.10X.ctrl.modularity.pearson <- modularity(V6.10X.ctrl.community.pearson) 
V6.10X.ctrl.modularity.pearson <- as.data.frame(V6.10X.ctrl.modularity.pearson) ##0.6155333

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.10X.ctrl.degree_dist.pearson <- degree_distribution(V6.10X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
V6.10X.ctrl.pl_fit.pearson <- fit_power_law(V6.10X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V6.10X.ctrl.pl_fit.pearson <- as.data.frame(V6.10X.ctrl.pl_fit.pearson)
## Correct KS p-value
V6.10X.ctrl.pl_fit.pearson$KS.p <- p.adjust(V6.10X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1750169, ##KS.p = 0.9928938

## Add additional metrics to the dataframe
net_prop.V6.10X.ctrl.wide.1.pearson <- spread(net_prop.V6.10X.ctrl.final.pearson, metric, value)
net_prop.V6.10X.ctrl.wide.1.pearson <- net_prop.V6.10X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '56')
net_prop.V6.10X.ctrl.wide.1.pearson <- net_prop.V6.10X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.6155333')
net_prop.V6.10X.ctrl.wide.1.pearson <- net_prop.V6.10X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.1750169')
net_prop.V6.10X.ctrl.wide.1.pearson <- net_prop.V6.10X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.9928938')
net_prop.V6.10X.ctrl.final.pearson <- gather(net_prop.V6.10X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.V6.10X.ctrl.final.pearson <- net_prop.V6.10X.ctrl.final.pearson %>% mutate(Condition = 'Condition 2')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
R3.10X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.10X.ctrl.pearson <- prune_samples(sample_data(R3.10X.ctrl.pearson)$Treatment == "Control", R3.10X.ctrl.pearson)
R3.10X.ctrl.pearson <- prune_samples(sample_data(R3.10X.ctrl.pearson)$Cultivar == "CZ4810X", R3.10X.ctrl.pearson)

R3.10X.ctrl.pearson.table <- co_occurrence(R3.10X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R3.10X.ctrl.pearson.table$p <- p.adjust(R3.10X.ctrl.pearson.table$p, "fdr")
R3.10X.ctrl.pearson.table <- filter(R3.10X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
R3.10X.ctrl.pearson.layout <- network_layout_ps(R3.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R3.10X.ctrl.pearson, co_occurrence_table = R3.10X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R3.10X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R3.10X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R3.10X.ctrl.net.pearson <- network_ps(R3.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.10X.ctrl.pearson <- net_properties(R3.10X.ctrl.net.pearson)
node_prop.R3.10X.ctrl.pearson <- node_properties(R3.10X.ctrl.net.pearson)
hub_score.R3.10X.ctrl.pearson <- hub_score(R3.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.10X.ctrl.pearson <- authority_score(R3.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.10X.ctrl.pearson <- neighborhood.connectivity(R3.10X.ctrl.net.pearson, vertices = V(R3.10X.ctrl.net.pearson), mode = "all")
neighbor.R3.10X.ctrl.pearson <- as.data.frame(neighbor.R3.10X.ctrl.pearson)

##Network properties for comparison
net_prop.R3.10X.ctrl.pearson <- as.data.frame(net_prop.R3.10X.ctrl.pearson)
net_prop.R3.10X.ctrl.pearson <- tibble::rownames_to_column(net_prop.R3.10X.ctrl.pearson, "metric")
net_prop.R3.10X.ctrl.pearson <- net_prop.R3.10X.ctrl.pearson %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.10X.ctrl.pearson <- net_prop.R3.10X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.R3.10X.ctrl.pearson <- net_prop.R3.10X.ctrl.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R3.10X.ctrl.pearson <- as.data.frame(hub_score.R3.10X.ctrl.pearson)
hub_score.R3.10X.ctrl.pearson.1 <- filter(hub_score.R3.10X.ctrl.pearson, vector >= 0.2)

##31

net_prop.R3.10X.ctrl.pearson <- net_prop.R3.10X.ctrl.pearson %>% mutate(Hub_Count = '31')

##Add mean neighborhood connectivity
mean(neighbor.R3.10X.ctrl.pearson$neighbor.R3.10X.ctrl.pearson) ##15.03921
net_prop.R3.10X.ctrl.pearson <- net_prop.R3.10X.ctrl.pearson %>% mutate(Mean_neighborhood = '15.03921')

net_prop.R3.10X.ctrl.wide.pearson <- spread(net_prop.R3.10X.ctrl.pearson, metric, value)
net_prop.R3.10X.ctrl.final.pearson <- gather(net_prop.R3.10X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R3.10X.ctrl.node.names.pearson <- as.data.frame(V(R3.10X.ctrl.net.pearson)$name)
R3.10X.ctrl.node.names.pearson <- R3.10X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'R3')
R3.10X.ctrl.node.names.pearson <- R3.10X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
R3.10X.ctrl.node.names.pearson <- R3.10X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(R3.10X.ctrl.node.names.pearson)[names(R3.10X.ctrl.node.names.pearson) == "V(R3.10X.ctrl.net.pearson)$name"] <- "genus"
R3.10X.ctrl.node.names.pearson <- R3.10X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 3')
hub_score.R3.10X.ctrl.pearson$genus <- rownames(hub_score.R3.10X.ctrl.pearson)
R3.10X.ctrl.heat.pearson <- merge(R3.10X.ctrl.node.names.pearson, hub_score.R3.10X.ctrl.pearson, by = c("genus"))
R3.10X.ctrl.heat.pearson <- subset(R3.10X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
R3.10X.ctrl.giant_component_size.pearson <- max(components(R3.10X.ctrl.net.pearson)$csize) ##56
R3.10X.ctrl.giant_component_size.pearson <- as.data.frame(R3.10X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R3.10X.ctrl.community.pearson <- cluster_walktrap(R3.10X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
R3.10X.ctrl.membership.pearson <- R3.10X.ctrl.community.pearson$membership
R3.10X.ctrl.modularity.pearson <- modularity(R3.10X.ctrl.community.pearson) 
R3.10X.ctrl.modularity.pearson <- as.data.frame(R3.10X.ctrl.modularity.pearson) ##0.7346939

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.10X.ctrl.degree_dist.pearson <- degree_distribution(R3.10X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
R3.10X.ctrl.pl_fit.pearson <- fit_power_law(R3.10X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R3.10X.ctrl.pl_fit.pearson <- as.data.frame(R3.10X.ctrl.pl_fit.pearson)
## Correct KS p-value
R3.10X.ctrl.pl_fit.pearson$KS.p <- p.adjust(R3.10X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.181378, ##KS.p = 0.7858419

## Add additional metrics to the dataframe
net_prop.R3.10X.ctrl.wide.1.pearson <- spread(net_prop.R3.10X.ctrl.final.pearson, metric, value)
net_prop.R3.10X.ctrl.wide.1.pearson <- net_prop.R3.10X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '56')
net_prop.R3.10X.ctrl.wide.1.pearson <- net_prop.R3.10X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.7346939')
net_prop.R3.10X.ctrl.wide.1.pearson <- net_prop.R3.10X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.181378')
net_prop.R3.10X.ctrl.wide.1.pearson <- net_prop.R3.10X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.7858419')
net_prop.R3.10X.ctrl.final.pearson <- gather(net_prop.R3.10X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.R3.10X.ctrl.final.pearson <- net_prop.R3.10X.ctrl.final.pearson %>% mutate(Condition = 'Condition 3')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
R6.10X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.10X.ctrl.pearson <- prune_samples(sample_data(R6.10X.ctrl.pearson)$Treatment == "Control", R6.10X.ctrl.pearson)
R6.10X.ctrl.pearson <- prune_samples(sample_data(R6.10X.ctrl.pearson)$Cultivar == "CZ4810X", R6.10X.ctrl.pearson)

R6.10X.ctrl.pearson.table <- co_occurrence(R6.10X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R6.10X.ctrl.pearson.table$p <- p.adjust(R6.10X.ctrl.pearson.table$p, "fdr")
R6.10X.ctrl.pearson.table <- filter(R6.10X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
R6.10X.ctrl.pearson.layout <- network_layout_ps(R6.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R6.10X.ctrl.pearson, co_occurrence_table = R6.10X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R6.10X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R6.10X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R6.10X.ctrl.net.pearson <- network_ps(R6.10X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.10X.ctrl.pearson <- net_properties(R6.10X.ctrl.net.pearson)
node_prop.R6.10X.ctrl.pearson <- node_properties(R6.10X.ctrl.net.pearson)
hub_score.R6.10X.ctrl.pearson <- hub_score(R6.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.10X.ctrl.pearson <- authority_score(R6.10X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.10X.ctrl.pearson <- neighborhood.connectivity(R6.10X.ctrl.net.pearson, vertices = V(R6.10X.ctrl.net.pearson), mode = "all")
neighbor.R6.10X.ctrl.pearson <- as.data.frame(neighbor.R6.10X.ctrl.pearson)

##Network properties for comparison
net_prop.R6.10X.ctrl.pearson <- as.data.frame(net_prop.R6.10X.ctrl.pearson)
net_prop.R6.10X.ctrl.pearson <- tibble::rownames_to_column(net_prop.R6.10X.ctrl.pearson, "metric")
net_prop.R6.10X.ctrl.pearson <- net_prop.R6.10X.ctrl.pearson %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.10X.ctrl.pearson <- net_prop.R6.10X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.R6.10X.ctrl.pearson <- net_prop.R6.10X.ctrl.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R6.10X.ctrl.pearson <- as.data.frame(hub_score.R6.10X.ctrl.pearson)
hub_score.R6.10X.ctrl.pearson.1 <- filter(hub_score.R6.10X.ctrl.pearson, vector >= 0.2)

##30

net_prop.R6.10X.ctrl.pearson <- net_prop.R6.10X.ctrl.pearson %>% mutate(Hub_Count = '30')

##Add mean neighborhood connectivity
mean(neighbor.R6.10X.ctrl.pearson$neighbor.R6.10X.ctrl.pearson) ##14.17449
net_prop.R6.10X.ctrl.pearson <- net_prop.R6.10X.ctrl.pearson %>% mutate(Mean_neighborhood = '14.17449')

net_prop.R6.10X.ctrl.wide.pearson <- spread(net_prop.R6.10X.ctrl.pearson, metric, value)
net_prop.R6.10X.ctrl.final.pearson <- gather(net_prop.R6.10X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R6.10X.ctrl.node.names.pearson <- as.data.frame(V(R6.10X.ctrl.net.pearson)$name)
R6.10X.ctrl.node.names.pearson <- R6.10X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'R6')
R6.10X.ctrl.node.names.pearson <- R6.10X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
R6.10X.ctrl.node.names.pearson <- R6.10X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(R6.10X.ctrl.node.names.pearson)[names(R6.10X.ctrl.node.names.pearson) == "V(R6.10X.ctrl.net.pearson)$name"] <- "genus"
R6.10X.ctrl.node.names.pearson <- R6.10X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 4')
hub_score.R6.10X.ctrl.pearson$genus <- rownames(hub_score.R6.10X.ctrl.pearson)
R6.10X.ctrl.heat.pearson <- merge(R6.10X.ctrl.node.names.pearson, hub_score.R6.10X.ctrl.pearson, by = c("genus"))
R6.10X.ctrl.heat.pearson <- subset(R6.10X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
R6.10X.ctrl.giant_component_size.pearson <- max(components(R6.10X.ctrl.net.pearson)$csize) ##53
R6.10X.ctrl.giant_component_size.pearson <- as.data.frame(R6.10X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R6.10X.ctrl.community.pearson <- cluster_walktrap(R6.10X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
R6.10X.ctrl.membership.pearson <- R6.10X.ctrl.community.pearson$membership
R6.10X.ctrl.modularity.pearson <- modularity(R6.10X.ctrl.community.pearson) 
R6.10X.ctrl.modularity.pearson <- as.data.frame(R6.10X.ctrl.modularity.pearson) ##0.6764191

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.10X.ctrl.degree_dist.pearson <- degree_distribution(R6.10X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
R6.10X.ctrl.pl_fit.pearson <- fit_power_law(R6.10X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R6.10X.ctrl.pl_fit.pearson <- as.data.frame(R6.10X.ctrl.pl_fit.pearson)
## Correct KS p-value
R6.10X.ctrl.pl_fit.pearson$KS.p <- p.adjust(R6.10X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1666667, ##KS.p = 0.9962552

## Add additional metrics to the dataframe
net_prop.R6.10X.ctrl.wide.1.pearson <- spread(net_prop.R6.10X.ctrl.final.pearson, metric, value)
net_prop.R6.10X.ctrl.wide.1.pearson <- net_prop.R6.10X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '53')
net_prop.R6.10X.ctrl.wide.1.pearson <- net_prop.R6.10X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.6764191')
net_prop.R6.10X.ctrl.wide.1.pearson <- net_prop.R6.10X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.1666667')
net_prop.R6.10X.ctrl.wide.1.pearson <- net_prop.R6.10X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.9962552')
net_prop.R6.10X.ctrl.final.pearson <- gather(net_prop.R6.10X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.R6.10X.ctrl.final.pearson <- net_prop.R6.10X.ctrl.final.pearson %>% mutate(Condition = 'Condition 4')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
V1.79X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.79X.ctrl.pearson <- prune_samples(sample_data(V1.79X.ctrl.pearson)$Treatment == "Control", V1.79X.ctrl.pearson)
V1.79X.ctrl.pearson <- prune_samples(sample_data(V1.79X.ctrl.pearson)$Cultivar == "CZ4979X", V1.79X.ctrl.pearson)

V1.79X.ctrl.pearson.table <- co_occurrence(V1.79X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V1.79X.ctrl.pearson.table$p <- p.adjust(V1.79X.ctrl.pearson.table$p, "fdr")
V1.79X.ctrl.pearson.table <- filter(V1.79X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
V1.79X.ctrl.pearson.layout <- network_layout_ps(V1.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.79X.ctrl.pearson, co_occurrence_table = V1.79X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V1.79X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V1.79X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)


##network_ps function 
V1.79X.ctrl.net.pearson <- network_ps(V1.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.79X.ctrl.pearson <- net_properties(V1.79X.ctrl.net.pearson)
node_prop.V1.79X.ctrl.pearson <- node_properties(V1.79X.ctrl.net.pearson)
hub_score.V1.79X.ctrl.pearson <- hub_score(V1.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.79X.ctrl.pearson <- authority_score(V1.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.79X.ctrl.pearson <- neighborhood.connectivity(V1.79X.ctrl.net.pearson, vertices = V(V1.79X.ctrl.net.pearson), mode = "all")
neighbor.V1.79X.ctrl.pearson <- as.data.frame(neighbor.V1.79X.ctrl.pearson)

##Network properties for comparison
net_prop.V1.79X.ctrl.pearson <- as.data.frame(net_prop.V1.79X.ctrl.pearson)
net_prop.V1.79X.ctrl.pearson <- tibble::rownames_to_column(net_prop.V1.79X.ctrl.pearson, "metric")
net_prop.V1.79X.ctrl.pearson <- net_prop.V1.79X.ctrl.pearson %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.79X.ctrl.pearson <- net_prop.V1.79X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.V1.79X.ctrl.pearson <- net_prop.V1.79X.ctrl.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V1.79X.ctrl.pearson <- as.data.frame(hub_score.V1.79X.ctrl.pearson)
hub_score.V1.79X.ctrl.pearson.1 <- filter(hub_score.V1.79X.ctrl.pearson, vector >= 0.2)

##27

net_prop.V1.79X.ctrl.pearson <- net_prop.V1.79X.ctrl.pearson %>% mutate(Hub_Count = '27')

##Add mean neighborhood connectivity
mean(neighbor.V1.79X.ctrl.pearson$neighbor.V1.79X.ctrl.pearson) ##17.1245
net_prop.V1.79X.ctrl.pearson <- net_prop.V1.79X.ctrl.pearson %>% mutate(Mean_neighborhood = '17.1245')

net_prop.V1.79X.ctrl.wide.pearson <- spread(net_prop.V1.79X.ctrl.pearson, metric, value)
net_prop.V1.79X.ctrl.final.pearson <- gather(net_prop.V1.79X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V1.79X.ctrl.node.names.pearson <- as.data.frame(V(V1.79X.ctrl.net.pearson)$name)
V1.79X.ctrl.node.names.pearson <- V1.79X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'V1')
V1.79X.ctrl.node.names.pearson <- V1.79X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
V1.79X.ctrl.node.names.pearson <- V1.79X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(V1.79X.ctrl.node.names.pearson)[names(V1.79X.ctrl.node.names.pearson) == "V(V1.79X.ctrl.net.pearson)$name"] <- "genus"
V1.79X.ctrl.node.names.pearson <- V1.79X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 5')
hub_score.V1.79X.ctrl.pearson$genus <- rownames(hub_score.V1.79X.ctrl.pearson)
V1.79X.ctrl.heat.pearson <- merge(V1.79X.ctrl.node.names.pearson, hub_score.V1.79X.ctrl.pearson, by = c("genus"))
V1.79X.ctrl.heat.pearson <- subset(V1.79X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
V1.79X.ctrl.giant_component_size.pearson <- max(components(V1.79X.ctrl.net.pearson)$csize) ##30
V1.79X.ctrl.giant_component_size.pearson <- as.data.frame(V1.79X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V1.79X.ctrl.community.pearson <- cluster_walktrap(V1.79X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
V1.79X.ctrl.membership.pearson <- V1.79X.ctrl.community.pearson$membership
V1.79X.ctrl.modularity.pearson <- modularity(V1.79X.ctrl.community.pearson) 
V1.79X.ctrl.modularity.pearson <- as.data.frame(V1.79X.ctrl.modularity.pearson) ##0.6140898

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.79X.ctrl.degree_dist.pearson <- degree_distribution(V1.79X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
V1.79X.ctrl.pl_fit.pearson <- fit_power_law(V1.79X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V1.79X.ctrl.pl_fit.pearson <- as.data.frame(V1.79X.ctrl.pl_fit.pearson)
## Correct KS p-value
V1.79X.ctrl.pl_fit.pearson$KS.p <- p.adjust(V1.79X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1671708, ##KS.p = 0.9960963

## Add additional metrics to the dataframe
net_prop.V1.79X.ctrl.wide.1.pearson <- spread(net_prop.V1.79X.ctrl.final.pearson, metric, value)
net_prop.V1.79X.ctrl.wide.1.pearson <- net_prop.V1.79X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '30')
net_prop.V1.79X.ctrl.wide.1.pearson <- net_prop.V1.79X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.6140898')
net_prop.V1.79X.ctrl.wide.1.pearson <- net_prop.V1.79X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.1671708')
net_prop.V1.79X.ctrl.wide.1.pearson <- net_prop.V1.79X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.9960963')
net_prop.V1.79X.ctrl.final.pearson <- gather(net_prop.V1.79X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.V1.79X.ctrl.final.pearson <- net_prop.V1.79X.ctrl.final.pearson %>% mutate(Condition = 'Condition 5')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
V6.79X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.79X.ctrl.pearson <- prune_samples(sample_data(V6.79X.ctrl.pearson)$Treatment == "Control", V6.79X.ctrl.pearson)
V6.79X.ctrl.pearson <- prune_samples(sample_data(V6.79X.ctrl.pearson)$Cultivar == "CZ4979X", V6.79X.ctrl.pearson)

V6.79X.ctrl.pearson.table <- co_occurrence(V6.79X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V6.79X.ctrl.pearson.table$p <- p.adjust(V6.79X.ctrl.pearson.table$p, "fdr")
V6.79X.ctrl.pearson.table <- filter(V6.79X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
V6.79X.ctrl.pearson.layout <- network_layout_ps(V6.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.79X.ctrl.pearson, co_occurrence_table = V6.79X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V6.79X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V6.79X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V6.79X.ctrl.net.pearson <- network_ps(V6.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.79X.ctrl.pearson <- net_properties(V6.79X.ctrl.net.pearson)
node_prop.V6.79X.ctrl.pearson <- node_properties(V6.79X.ctrl.net.pearson)
hub_score.V6.79X.ctrl.pearson <- hub_score(V6.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.79X.ctrl.pearson <- authority_score(V6.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.79X.ctrl.pearson <- neighborhood.connectivity(V6.79X.ctrl.net.pearson, vertices = V(V6.79X.ctrl.net.pearson), mode = "all")
neighbor.V6.79X.ctrl.pearson <- as.data.frame(neighbor.V6.79X.ctrl.pearson)

##Network properties for comparison
net_prop.V6.79X.ctrl.pearson <- as.data.frame(net_prop.V6.79X.ctrl.pearson)
net_prop.V6.79X.ctrl.pearson <- tibble::rownames_to_column(net_prop.V6.79X.ctrl.pearson, "metric")
net_prop.V6.79X.ctrl.pearson <- net_prop.V6.79X.ctrl.pearson %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.79X.ctrl.pearson <- net_prop.V6.79X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.V6.79X.ctrl.pearson <- net_prop.V6.79X.ctrl.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V6.79X.ctrl.pearson <- as.data.frame(hub_score.V6.79X.ctrl.pearson)
hub_score.V6.79X.ctrl.pearson.1 <- filter(hub_score.V6.79X.ctrl.pearson, vector >= 0.2)

##21

net_prop.V6.79X.ctrl.pearson <- net_prop.V6.79X.ctrl.pearson %>% mutate(Hub_Count = '21')

##Add mean neighborhood connectivity
mean(neighbor.V6.79X.ctrl.pearson$neighbor.V6.79X.ctrl.pearson) ##12.48459
net_prop.V6.79X.ctrl.pearson <- net_prop.V6.79X.ctrl.pearson %>% mutate(Mean_neighborhood = '12.48459')

net_prop.V6.79X.ctrl.wide.pearson <- spread(net_prop.V6.79X.ctrl.pearson, metric, value)
net_prop.V6.79X.ctrl.final.pearson <- gather(net_prop.V6.79X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V6.79X.ctrl.node.names.pearson <- as.data.frame(V(V6.79X.ctrl.net.pearson)$name)
V6.79X.ctrl.node.names.pearson <- V6.79X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'V6')
V6.79X.ctrl.node.names.pearson <- V6.79X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
V6.79X.ctrl.node.names.pearson <- V6.79X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(V6.79X.ctrl.node.names.pearson)[names(V6.79X.ctrl.node.names.pearson) == "V(V6.79X.ctrl.net.pearson)$name"] <- "genus"
V6.79X.ctrl.node.names.pearson <- V6.79X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 6')
hub_score.V6.79X.ctrl.pearson$genus <- rownames(hub_score.V6.79X.ctrl.pearson)
V6.79X.ctrl.heat.pearson <- merge(V6.79X.ctrl.node.names.pearson, hub_score.V6.79X.ctrl.pearson, by = c("genus"))
V6.79X.ctrl.heat.pearson <- subset(V6.79X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
V6.79X.ctrl.giant_component_size.pearson <- max(components(V6.79X.ctrl.net.pearson)$csize) ##24
V6.79X.ctrl.giant_component_size.pearson <- as.data.frame(V6.79X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V6.79X.ctrl.community.pearson <- cluster_walktrap(V6.79X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
V6.79X.ctrl.membership.pearson <- V6.79X.ctrl.community.pearson$membership
V6.79X.ctrl.modularity.pearson <- modularity(V6.79X.ctrl.community.pearson) 
V6.79X.ctrl.modularity.pearson <- as.data.frame(V6.79X.ctrl.modularity.pearson) ##0.7259773

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.79X.ctrl.degree_dist.pearson <- degree_distribution(V6.79X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
V6.79X.ctrl.pl_fit.pearson <- fit_power_law(V6.79X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V6.79X.ctrl.pl_fit.pearson <- as.data.frame(V6.79X.ctrl.pl_fit.pearson)
## Correct KS p-value
V6.79X.ctrl.pl_fit.pearson$KS.p <- p.adjust(V6.79X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.166806, ##KS.p = 0.9962118

## Add additional metrics to the dataframe
net_prop.V6.79X.ctrl.wide.1.pearson <- spread(net_prop.V6.79X.ctrl.final.pearson, metric, value)
net_prop.V6.79X.ctrl.wide.1.pearson <- net_prop.V6.79X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '24')
net_prop.V6.79X.ctrl.wide.1.pearson <- net_prop.V6.79X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.7259773')
net_prop.V6.79X.ctrl.wide.1.pearson <- net_prop.V6.79X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.166806')
net_prop.V6.79X.ctrl.wide.1.pearson <- net_prop.V6.79X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.9962118')
net_prop.V6.79X.ctrl.final.pearson <- gather(net_prop.V6.79X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.V6.79X.ctrl.final.pearson <- net_prop.V6.79X.ctrl.final.pearson %>% mutate(Condition = 'Condition 6')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
R3.79X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.79X.ctrl.pearson <- prune_samples(sample_data(R3.79X.ctrl.pearson)$Treatment == "Control", R3.79X.ctrl.pearson)
R3.79X.ctrl.pearson <- prune_samples(sample_data(R3.79X.ctrl.pearson)$Cultivar == "CZ4979X", R3.79X.ctrl.pearson)

R3.79X.ctrl.pearson.table <- co_occurrence(R3.79X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R3.79X.ctrl.pearson.table$p <- p.adjust(R3.79X.ctrl.pearson.table$p, "fdr")
R3.79X.ctrl.pearson.table <- filter(R3.79X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
R3.79X.ctrl.pearson.layout <- network_layout_ps(R3.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R3.79X.ctrl.pearson, co_occurrence_table = R3.79X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R3.79X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R3.79X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R3.79X.ctrl.net.pearson <- network_ps(R3.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.79X.ctrl.pearson <- net_properties(R3.79X.ctrl.net.pearson)
node_prop.R3.79X.ctrl.pearson <- node_properties(R3.79X.ctrl.net.pearson)
hub_score.R3.79X.ctrl.pearson <- hub_score(R3.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.79X.ctrl.pearson <- authority_score(R3.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.79X.ctrl.pearson <- neighborhood.connectivity(R3.79X.ctrl.net.pearson, vertices = V(R3.79X.ctrl.net.pearson), mode = "all")
neighbor.R3.79X.ctrl.pearson <- as.data.frame(neighbor.R3.79X.ctrl.pearson)

##Network properties for comparison
net_prop.R3.79X.ctrl.pearson <- as.data.frame(net_prop.R3.79X.ctrl.pearson)
net_prop.R3.79X.ctrl.pearson <- tibble::rownames_to_column(net_prop.R3.79X.ctrl.pearson, "metric")
net_prop.R3.79X.ctrl.pearson <- net_prop.R3.79X.ctrl.pearson %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.79X.ctrl.pearson <- net_prop.R3.79X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.R3.79X.ctrl.pearson <- net_prop.R3.79X.ctrl.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R3.79X.ctrl.pearson <- as.data.frame(hub_score.R3.79X.ctrl.pearson)
hub_score.R3.79X.ctrl.pearson.1 <- filter(hub_score.R3.79X.ctrl.pearson, vector >= 0.2)

##19

net_prop.R3.79X.ctrl.pearson <- net_prop.R3.79X.ctrl.pearson %>% mutate(Hub_Count = '19')

##Add mean neighborhood connectivity
mean(neighbor.R3.79X.ctrl.pearson$neighbor.R3.79X.ctrl.pearson) ##9.694197
net_prop.R3.79X.ctrl.pearson <- net_prop.R3.79X.ctrl.pearson %>% mutate(Mean_neighborhood = '9.694197')

net_prop.R3.79X.ctrl.wide.pearson <- spread(net_prop.R3.79X.ctrl.pearson, metric, value)
net_prop.R3.79X.ctrl.final.pearson <- gather(net_prop.R3.79X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R3.79X.ctrl.node.names.pearson <- as.data.frame(V(R3.79X.ctrl.net.pearson)$name)
R3.79X.ctrl.node.names.pearson <- R3.79X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'R3')
R3.79X.ctrl.node.names.pearson <- R3.79X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
R3.79X.ctrl.node.names.pearson <- R3.79X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(R3.79X.ctrl.node.names.pearson)[names(R3.79X.ctrl.node.names.pearson) == "V(R3.79X.ctrl.net.pearson)$name"] <- "genus"
R3.79X.ctrl.node.names.pearson <- R3.79X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 7')
hub_score.R3.79X.ctrl.pearson$genus <- rownames(hub_score.R3.79X.ctrl.pearson)
R3.79X.ctrl.heat.pearson <- merge(R3.79X.ctrl.node.names.pearson, hub_score.R3.79X.ctrl.pearson, by = c("genus"))
R3.79X.ctrl.heat.pearson <- subset(R3.79X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
R3.79X.ctrl.giant_component_size.pearson <- max(components(R3.79X.ctrl.net.pearson)$csize) ##38
R3.79X.ctrl.giant_component_size.pearson <- as.data.frame(R3.79X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R3.79X.ctrl.community.pearson <- cluster_walktrap(R3.79X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
R3.79X.ctrl.membership.pearson <- R3.79X.ctrl.community.pearson$membership
R3.79X.ctrl.modularity.pearson <- modularity(R3.79X.ctrl.community.pearson) 
R3.79X.ctrl.modularity.pearson <- as.data.frame(R3.79X.ctrl.modularity.pearson) ##0.7681484

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.79X.ctrl.degree_dist.pearson <- degree_distribution(R3.79X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
R3.79X.ctrl.pl_fit.pearson <- fit_power_law(R3.79X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R3.79X.ctrl.pl_fit.pearson <- as.data.frame(R3.79X.ctrl.pl_fit.pearson)
## Correct KS p-value
R3.79X.ctrl.pl_fit.pearson$KS.p <- p.adjust(R3.79X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1906093, ##KS.p = 0.6062745

## Add additional metrics to the dataframe
net_prop.R3.79X.ctrl.wide.1.pearson <- spread(net_prop.R3.79X.ctrl.final.pearson, metric, value)
net_prop.R3.79X.ctrl.wide.1.pearson <- net_prop.R3.79X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '38')
net_prop.R3.79X.ctrl.wide.1.pearson <- net_prop.R3.79X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.7681484')
net_prop.R3.79X.ctrl.wide.1.pearson <- net_prop.R3.79X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.1906093')
net_prop.R3.79X.ctrl.wide.1.pearson <- net_prop.R3.79X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.6062745')
net_prop.R3.79X.ctrl.final.pearson <- gather(net_prop.R3.79X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.R3.79X.ctrl.final.pearson <- net_prop.R3.79X.ctrl.final.pearson %>% mutate(Condition = 'Condition 7')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
R6.79X.ctrl.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.79X.ctrl.pearson <- prune_samples(sample_data(R6.79X.ctrl.pearson)$Treatment == "Control", R6.79X.ctrl.pearson)
R6.79X.ctrl.pearson <- prune_samples(sample_data(R6.79X.ctrl.pearson)$Cultivar == "CZ4979X", R6.79X.ctrl.pearson)

R6.79X.ctrl.pearson.table <- co_occurrence(R6.79X.ctrl.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R6.79X.ctrl.pearson.table$p <- p.adjust(R6.79X.ctrl.pearson.table$p, "fdr")
R6.79X.ctrl.pearson.table <- filter(R6.79X.ctrl.pearson.table, p <= 0.05)

##network_layout_ps function
R6.79X.ctrl.pearson.layout <- network_layout_ps(R6.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.ctrl.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(R6.79X.ctrl.pearson, co_occurrence_table = R6.79X.ctrl.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R6.79X.ctrl.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R6.79X.ctrl.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R6.79X.ctrl.net.pearson <- network_ps(R6.79X.ctrl.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.ctrl.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.79X.ctrl.pearson <- net_properties(R6.79X.ctrl.net.pearson)
node_prop.R6.79X.ctrl.pearson <- node_properties(R6.79X.ctrl.net.pearson)
hub_score.R6.79X.ctrl.pearson <- hub_score(R6.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.79X.ctrl.pearson <- authority_score(R6.79X.ctrl.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.79X.ctrl.pearson <- neighborhood.connectivity(R6.79X.ctrl.net.pearson, vertices = V(R6.79X.ctrl.net.pearson), mode = "all")
neighbor.R6.79X.ctrl.pearson <- as.data.frame(neighbor.R6.79X.ctrl.pearson)

##Network properties for comparison
net_prop.R6.79X.ctrl.pearson <- as.data.frame(net_prop.R6.79X.ctrl.pearson)
net_prop.R6.79X.ctrl.pearson <- tibble::rownames_to_column(net_prop.R6.79X.ctrl.pearson, "metric")
net_prop.R6.79X.ctrl.pearson <- net_prop.R6.79X.ctrl.pearson %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.79X.ctrl.pearson <- net_prop.R6.79X.ctrl.pearson %>% mutate(Treatment = 'Control')
net_prop.R6.79X.ctrl.pearson <- net_prop.R6.79X.ctrl.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R6.79X.ctrl.pearson <- as.data.frame(hub_score.R6.79X.ctrl.pearson)
hub_score.R6.79X.ctrl.pearson.1 <- filter(hub_score.R6.79X.ctrl.pearson, vector >= 0.2)

##49

net_prop.R6.79X.ctrl.pearson <- net_prop.R6.79X.ctrl.pearson %>% mutate(Hub_Count = '49')

##Add mean neighborhood connectivity
mean(neighbor.R6.79X.ctrl.pearson$neighbor.R6.79X.ctrl.pearson) ##19.57785
net_prop.R6.79X.ctrl.pearson <- net_prop.R6.79X.ctrl.pearson %>% mutate(Mean_neighborhood = '19.57785')

net_prop.R6.79X.ctrl.wide.pearson <- spread(net_prop.R6.79X.ctrl.pearson, metric, value)
net_prop.R6.79X.ctrl.final.pearson <- gather(net_prop.R6.79X.ctrl.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R6.79X.ctrl.node.names.pearson <- as.data.frame(V(R6.79X.ctrl.net.pearson)$name)
R6.79X.ctrl.node.names.pearson <- R6.79X.ctrl.node.names.pearson %>% mutate(Developmental_Stage = 'R6')
R6.79X.ctrl.node.names.pearson <- R6.79X.ctrl.node.names.pearson %>% mutate(Treatment = 'Control')
R6.79X.ctrl.node.names.pearson <- R6.79X.ctrl.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(R6.79X.ctrl.node.names.pearson)[names(R6.79X.ctrl.node.names.pearson) == "V(R6.79X.ctrl.net.pearson)$name"] <- "genus"
R6.79X.ctrl.node.names.pearson <- R6.79X.ctrl.node.names.pearson %>% mutate(Condition = 'Condition 8')
hub_score.R6.79X.ctrl.pearson$genus <- rownames(hub_score.R6.79X.ctrl.pearson)
R6.79X.ctrl.heat.pearson <- merge(R6.79X.ctrl.node.names.pearson, hub_score.R6.79X.ctrl.pearson, by = c("genus"))
R6.79X.ctrl.heat.pearson <- subset(R6.79X.ctrl.heat.pearson, select = -c(8:27))

##Additional measurements
R6.79X.ctrl.giant_component_size.pearson <- max(components(R6.79X.ctrl.net.pearson)$csize) ##61
R6.79X.ctrl.giant_component_size.pearson <- as.data.frame(R6.79X.ctrl.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R6.79X.ctrl.community.pearson <- cluster_walktrap(R6.79X.ctrl.net.pearson)

## Get the community membership vector and calculate modularity
R6.79X.ctrl.membership.pearson <- R6.79X.ctrl.community.pearson$membership
R6.79X.ctrl.modularity.pearson <- modularity(R6.79X.ctrl.community.pearson) 
R6.79X.ctrl.modularity.pearson <- as.data.frame(R6.79X.ctrl.modularity.pearson) ##0.3501306

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.79X.ctrl.degree_dist.pearson <- degree_distribution(R6.79X.ctrl.net.pearson)
# Step 2: Fit the power-law distribution
R6.79X.ctrl.pl_fit.pearson <- fit_power_law(R6.79X.ctrl.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R6.79X.ctrl.pl_fit.pearson <- as.data.frame(R6.79X.ctrl.pl_fit.pearson)
## Correct KS p-value
R6.79X.ctrl.pl_fit.pearson$KS.p <- p.adjust(R6.79X.ctrl.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.2035523, ##KS.p = 0.8498651

## Add additional metrics to the dataframe
net_prop.R6.79X.ctrl.wide.1.pearson <- spread(net_prop.R6.79X.ctrl.final.pearson, metric, value)
net_prop.R6.79X.ctrl.wide.1.pearson <- net_prop.R6.79X.ctrl.wide.1.pearson %>% mutate(giant_component_size = '61')
net_prop.R6.79X.ctrl.wide.1.pearson <- net_prop.R6.79X.ctrl.wide.1.pearson %>% mutate(Modularity = '0.3501306')
net_prop.R6.79X.ctrl.wide.1.pearson <- net_prop.R6.79X.ctrl.wide.1.pearson %>% mutate(KS.stat = '0.2035523')
net_prop.R6.79X.ctrl.wide.1.pearson <- net_prop.R6.79X.ctrl.wide.1.pearson %>% mutate(KS.p = '0.8498651')
net_prop.R6.79X.ctrl.final.pearson <- gather(net_prop.R6.79X.ctrl.wide.1.pearson, metric, value, 4:23)
net_prop.R6.79X.ctrl.final.pearson <- net_prop.R6.79X.ctrl.final.pearson %>% mutate(Condition = 'Condition 8')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
V1.10X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.10X.trt.pearson <- prune_samples(sample_data(V1.10X.trt.pearson)$Treatment == "Biostimulant", V1.10X.trt.pearson)
V1.10X.trt.pearson <- prune_samples(sample_data(V1.10X.trt.pearson)$Cultivar == "CZ4810X", V1.10X.trt.pearson)

V1.10X.trt.pearson.table <- co_occurrence(V1.10X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V1.10X.trt.pearson.table$p <- p.adjust(V1.10X.trt.pearson.table$p, "fdr")
V1.10X.trt.pearson.table <- filter(V1.10X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
V1.10X.trt.pearson.layout <- network_layout_ps(V1.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.trt.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V1.10X.trt.pearson, co_occurrence_table = V1.10X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V1.10X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V1.10X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V1.10X.trt.net.pearson <- network_ps(V1.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.10X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.10X.trt.pearson <- net_properties(V1.10X.trt.net.pearson)
node_prop.V1.10X.trt.pearson <- node_properties(V1.10X.trt.net.pearson)
hub_score.V1.10X.trt.pearson <- hub_score(V1.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.10X.trt.pearson <- authority_score(V1.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.10X.trt.pearson <- neighborhood.connectivity(V1.10X.trt.net.pearson, vertices = V(V1.10X.trt.net.pearson), mode = "all")
neighbor.V1.10X.trt.pearson <- as.data.frame(neighbor.V1.10X.trt.pearson)

##Network properties for comparison
net_prop.V1.10X.trt.pearson <- as.data.frame(net_prop.V1.10X.trt.pearson)
net_prop.V1.10X.trt.pearson <- tibble::rownames_to_column(net_prop.V1.10X.trt.pearson, "metric")
net_prop.V1.10X.trt.pearson <- net_prop.V1.10X.trt.pearson %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.10X.trt.pearson <- net_prop.V1.10X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.V1.10X.trt.pearson <- net_prop.V1.10X.trt.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V1.10X.trt.pearson <- as.data.frame(hub_score.V1.10X.trt.pearson)
hub_score.V1.10X.trt.pearson.1 <- filter(hub_score.V1.10X.trt.pearson, vector >= 0.2)

##29

net_prop.V1.10X.trt.pearson <- net_prop.V1.10X.trt.pearson %>% mutate(Hub_Count = '29')

##Add mean neighborhood connectivity
mean(neighbor.V1.10X.trt.pearson$neighbor.V1.10X.trt.pearson) ##15.62292
net_prop.V1.10X.trt.pearson <- net_prop.V1.10X.trt.pearson %>% mutate(Mean_neighborhood = '15.62292')

net_prop.V1.10X.trt.wide.pearson <- spread(net_prop.V1.10X.trt.pearson, metric, value)
net_prop.V1.10X.trt.final.pearson <- gather(net_prop.V1.10X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V1.10X.trt.node.names.pearson <- as.data.frame(V(V1.10X.trt.net.pearson)$name)
V1.10X.trt.node.names.pearson <- V1.10X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'V1')
V1.10X.trt.node.names.pearson <- V1.10X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
V1.10X.trt.node.names.pearson <- V1.10X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(V1.10X.trt.node.names.pearson)[names(V1.10X.trt.node.names.pearson) == "V(V1.10X.trt.net.pearson)$name"] <- "genus"
V1.10X.trt.node.names.pearson <- V1.10X.trt.node.names.pearson %>% mutate(Condition = 'Condition 9')
hub_score.V1.10X.trt.pearson$genus <- rownames(hub_score.V1.10X.trt.pearson)
V1.10X.trt.heat.pearson <- merge(V1.10X.trt.node.names.pearson, hub_score.V1.10X.trt.pearson, by = c("genus"))
V1.10X.trt.heat.pearson <- subset(V1.10X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
V1.10X.trt.giant_component_size.pearson <- max(components(V1.10X.trt.net.pearson)$csize) ##29
V1.10X.trt.giant_component_size.pearson <- as.data.frame(V1.10X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V1.10X.trt.community.pearson <- cluster_walktrap(V1.10X.trt.net.pearson)

## Get the community membership vector and calculate modularity
V1.10X.trt.membership.pearson <- V1.10X.trt.community.pearson$membership
V1.10X.trt.modularity.pearson <- modularity(V1.10X.trt.community.pearson) 
V1.10X.trt.modularity.pearson <- as.data.frame(V1.10X.trt.modularity.pearson) ##0.6421498

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.10X.trt.degree_dist.pearson <- degree_distribution(V1.10X.trt.net.pearson)
# Step 2: Fit the power-law distribution
V1.10X.trt.pl_fit.pearson <- fit_power_law(V1.10X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V1.10X.trt.pl_fit.pearson <- as.data.frame(V1.10X.trt.pl_fit.pearson)
## Correct KS p-value
V1.10X.trt.pl_fit.pearson$KS.p <- p.adjust(V1.10X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.2, ##KS.p = 0.8186212

## Add additional metrics to the dataframe
net_prop.V1.10X.trt.wide.1.pearson <- spread(net_prop.V1.10X.trt.final.pearson, metric, value)
net_prop.V1.10X.trt.wide.1.pearson <- net_prop.V1.10X.trt.wide.1.pearson %>% mutate(giant_component_size = '29')
net_prop.V1.10X.trt.wide.1.pearson <- net_prop.V1.10X.trt.wide.1.pearson %>% mutate(Modularity = '0.6421498')
net_prop.V1.10X.trt.wide.1.pearson <- net_prop.V1.10X.trt.wide.1.pearson %>% mutate(KS.stat = '0.2')
net_prop.V1.10X.trt.wide.1.pearson <- net_prop.V1.10X.trt.wide.1.pearson %>% mutate(KS.p = '0.8186212')
net_prop.V1.10X.trt.final.pearson <- gather(net_prop.V1.10X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.V1.10X.trt.final.pearson <- net_prop.V1.10X.trt.final.pearson %>% mutate(Condition = 'Condition 9')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
V6.10X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.10X.trt.pearson <- prune_samples(sample_data(V6.10X.trt.pearson)$Treatment == "Biostimulant", V6.10X.trt.pearson)
V6.10X.trt.pearson <- prune_samples(sample_data(V6.10X.trt.pearson)$Cultivar == "CZ4810X", V6.10X.trt.pearson)

V6.10X.trt.pearson.table <- co_occurrence(V6.10X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V6.10X.trt.pearson.table$p <- p.adjust(V6.10X.trt.pearson.table$p, "fdr")
V6.10X.trt.pearson.table <- filter(V6.10X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
V6.10X.trt.pearson.layout <- network_layout_ps(V6.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.trt.pearson.table, algorithm = 'sphere')

##co-occurence function
co_occurrence_network(V6.10X.trt.pearson, co_occurrence_table = V6.10X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V6.10X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V6.10X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V6.10X.trt.net.pearson <- network_ps(V6.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.10X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.10X.trt.pearson <- net_properties(V6.10X.trt.net.pearson)
node_prop.V6.10X.trt.pearson <- node_properties(V6.10X.trt.net.pearson)
hub_score.V6.10X.trt.pearson <- hub_score(V6.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.10X.trt.pearson <- authority_score(V6.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.10X.trt.pearson <- neighborhood.connectivity(V6.10X.trt.net.pearson, vertices = V(V6.10X.trt.net.pearson), mode = "all")
neighbor.V6.10X.trt.pearson <- as.data.frame(neighbor.V6.10X.trt.pearson)

##Network properties for comparison
net_prop.V6.10X.trt.pearson <- as.data.frame(net_prop.V6.10X.trt.pearson)
net_prop.V6.10X.trt.pearson <- tibble::rownames_to_column(net_prop.V6.10X.trt.pearson, "metric")
net_prop.V6.10X.trt.pearson <- net_prop.V6.10X.trt.pearson %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.10X.trt.pearson <- net_prop.V6.10X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.V6.10X.trt.pearson <- net_prop.V6.10X.trt.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.V6.10X.trt.pearson <- as.data.frame(hub_score.V6.10X.trt.pearson)
hub_score.V6.10X.trt.pearson.1 <- filter(hub_score.V6.10X.trt.pearson, vector >= 0.2)

##40

net_prop.V6.10X.trt.pearson <- net_prop.V6.10X.trt.pearson %>% mutate(Hub_Count = '40')

##Add mean neighborhood connectivity
mean(neighbor.V6.10X.trt.pearson$neighbor.V6.10X.trt.pearson) ##23.1763
net_prop.V6.10X.trt.pearson <- net_prop.V6.10X.trt.pearson %>% mutate(Mean_neighborhood = '23.1763')

net_prop.V6.10X.trt.wide.pearson <- spread(net_prop.V6.10X.trt.pearson, metric, value)
net_prop.V6.10X.trt.final.pearson <- gather(net_prop.V6.10X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V6.10X.trt.node.names.pearson <- as.data.frame(V(V6.10X.trt.net.pearson)$name)
V6.10X.trt.node.names.pearson <- V6.10X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'V6')
V6.10X.trt.node.names.pearson <- V6.10X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
V6.10X.trt.node.names.pearson <- V6.10X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(V6.10X.trt.node.names.pearson)[names(V6.10X.trt.node.names.pearson) == "V(V6.10X.trt.net.pearson)$name"] <- "genus"
V6.10X.trt.node.names.pearson <- V6.10X.trt.node.names.pearson %>% mutate(Condition = 'Condition 10')
hub_score.V6.10X.trt.pearson$genus <- rownames(hub_score.V6.10X.trt.pearson)
V6.10X.trt.heat.pearson <- merge(V6.10X.trt.node.names.pearson, hub_score.V6.10X.trt.pearson, by = c("genus"))
V6.10X.trt.heat.pearson <- subset(V6.10X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
V6.10X.trt.giant_component_size.pearson <- max(components(V6.10X.trt.net.pearson)$csize) ##43
V6.10X.trt.giant_component_size.pearson <- as.data.frame(V6.10X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V6.10X.trt.community.pearson <- cluster_walktrap(V6.10X.trt.net.pearson)

## Get the community membership vector and calculate modularity
V6.10X.trt.membership.pearson <- V6.10X.trt.community.pearson$membership
V6.10X.trt.modularity.pearson <- modularity(V6.10X.trt.community.pearson) 
V6.10X.trt.modularity.pearson <- as.data.frame(V6.10X.trt.modularity.pearson) ##0.6068323

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.10X.trt.degree_dist.pearson <- degree_distribution(V6.10X.trt.net.pearson)
# Step 2: Fit the power-law distribution
V6.10X.trt.pl_fit.pearson <- fit_power_law(V6.10X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V6.10X.trt.pl_fit.pearson <- as.data.frame(V6.10X.trt.pl_fit.pearson)
## Correct KS p-value
V6.10X.trt.pl_fit.pearson$KS.p <- p.adjust(V6.10X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1428571, ##KS.p = 0.9988221

## Add additional metrics to the dataframe
net_prop.V6.10X.trt.wide.1.pearson <- spread(net_prop.V6.10X.trt.final.pearson, metric, value)
net_prop.V6.10X.trt.wide.1.pearson <- net_prop.V6.10X.trt.wide.1.pearson %>% mutate(giant_component_size = '43')
net_prop.V6.10X.trt.wide.1.pearson <- net_prop.V6.10X.trt.wide.1.pearson %>% mutate(Modularity = '0.6068323')
net_prop.V6.10X.trt.wide.1.pearson <- net_prop.V6.10X.trt.wide.1.pearson %>% mutate(KS.stat = '0.1428571')
net_prop.V6.10X.trt.wide.1.pearson <- net_prop.V6.10X.trt.wide.1.pearson %>% mutate(KS.p = '0.9988221')
net_prop.V6.10X.trt.final.pearson <- gather(net_prop.V6.10X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.V6.10X.trt.final.pearson <- net_prop.V6.10X.trt.final.pearson %>% mutate(Condition = 'Condition 10')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
# Calculate the frequency and percentage for each kingdom
R3.10X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.10X.trt.pearson <- prune_samples(sample_data(R3.10X.trt.pearson)$Treatment == "Biostimulant", R3.10X.trt.pearson)
R3.10X.trt.pearson <- prune_samples(sample_data(R3.10X.trt.pearson)$Cultivar == "CZ4810X", R3.10X.trt.pearson)

R3.10X.trt.pearson.table <- co_occurrence(R3.10X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R3.10X.trt.pearson.table$p <- p.adjust(R3.10X.trt.pearson.table$p, "fdr")
R3.10X.trt.pearson.table <- filter(R3.10X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
R3.10X.trt.pearson.layout <- network_layout_ps(R3.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.trt.pearson.table, algorithm = 'sphere')

co_occurrence_network(R3.10X.trt.pearson, co_occurrence_table = R3.10X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R3.10X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R3.10X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R3.10X.trt.net.pearson <- network_ps(R3.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.10X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.10X.trt.pearson <- net_properties(R3.10X.trt.net.pearson)
node_prop.R3.10X.trt.pearson <- node_properties(R3.10X.trt.net.pearson)
hub_score.R3.10X.trt.pearson <- hub_score(R3.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.10X.trt.pearson <- authority_score(R3.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.10X.trt.pearson <- neighborhood.connectivity(R3.10X.trt.net.pearson, vertices = V(R3.10X.trt.net.pearson), mode = "all")
neighbor.R3.10X.trt.pearson <- as.data.frame(neighbor.R3.10X.trt.pearson)

##Network properties for comparison
net_prop.R3.10X.trt.pearson <- as.data.frame(net_prop.R3.10X.trt.pearson)
net_prop.R3.10X.trt.pearson <- tibble::rownames_to_column(net_prop.R3.10X.trt.pearson, "metric")
net_prop.R3.10X.trt.pearson <- net_prop.R3.10X.trt.pearson %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.10X.trt.pearson <- net_prop.R3.10X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.R3.10X.trt.pearson <- net_prop.R3.10X.trt.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R3.10X.trt.pearson <- as.data.frame(hub_score.R3.10X.trt.pearson)
hub_score.R3.10X.trt.pearson.1 <- filter(hub_score.R3.10X.trt.pearson, vector >= 0.2)

##61

net_prop.R3.10X.trt.pearson <- net_prop.R3.10X.trt.pearson %>% mutate(Hub_Count = '61')

##Add mean neighborhood connectivity
mean(neighbor.R3.10X.trt.pearson$neighbor.R3.10X.trt.pearson) ##26.39183
net_prop.R3.10X.trt.pearson <- net_prop.R3.10X.trt.pearson %>% mutate(Mean_neighborhood = '26.39183')

net_prop.R3.10X.trt.wide.pearson <- spread(net_prop.R3.10X.trt.pearson, metric, value)
net_prop.R3.10X.trt.final.pearson <- gather(net_prop.R3.10X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R3.10X.trt.node.names.pearson <- as.data.frame(V(R3.10X.trt.net.pearson)$name)
R3.10X.trt.node.names.pearson <- R3.10X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'R3')
R3.10X.trt.node.names.pearson <- R3.10X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
R3.10X.trt.node.names.pearson <- R3.10X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(R3.10X.trt.node.names.pearson)[names(R3.10X.trt.node.names.pearson) == "V(R3.10X.trt.net.pearson)$name"] <- "genus"
R3.10X.trt.node.names.pearson <- R3.10X.trt.node.names.pearson %>% mutate(Condition = 'Condition 11')
hub_score.R3.10X.trt.pearson$genus <- rownames(hub_score.R3.10X.trt.pearson)
R3.10X.trt.heat.pearson <- merge(R3.10X.trt.node.names.pearson, hub_score.R3.10X.trt.pearson, by = c("genus"))
R3.10X.trt.heat.pearson <- subset(R3.10X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
R3.10X.trt.giant_component_size.pearson <- max(components(R3.10X.trt.net.pearson)$csize) ##88
R3.10X.trt.giant_component_size.pearson <- as.data.frame(R3.10X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R3.10X.trt.community.pearson <- cluster_walktrap(R3.10X.trt.net.pearson)

## Get the community membership vector and calculate modularity
R3.10X.trt.membership.pearson <- R3.10X.trt.community.pearson$membership
R3.10X.trt.modularity.pearson <- modularity(R3.10X.trt.community.pearson) 
R3.10X.trt.modularity.pearson <- as.data.frame(R3.10X.trt.modularity.pearson) ##0.2540816

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.10X.trt.degree_dist.pearson <- degree_distribution(R3.10X.trt.net.pearson)
# Step 2: Fit the power-law distribution
R3.10X.trt.pl_fit.pearson <- fit_power_law(R3.10X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R3.10X.trt.pl_fit.pearson <- as.data.frame(R3.10X.trt.pl_fit.pearson)
## Correct KS p-value
R3.10X.trt.pl_fit.pearson$KS.p <- p.adjust(R3.10X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.2502502, ##KS.p = 0.2096083

## Add additional metrics to the dataframe
net_prop.R3.10X.trt.wide.1.pearson <- spread(net_prop.R3.10X.trt.final.pearson, metric, value)
net_prop.R3.10X.trt.wide.1.pearson <- net_prop.R3.10X.trt.wide.1.pearson %>% mutate(giant_component_size = '88')
net_prop.R3.10X.trt.wide.1.pearson <- net_prop.R3.10X.trt.wide.1.pearson %>% mutate(Modularity = '0.2540816')
net_prop.R3.10X.trt.wide.1.pearson <- net_prop.R3.10X.trt.wide.1.pearson %>% mutate(KS.stat = '0.2502502')
net_prop.R3.10X.trt.wide.1.pearson <- net_prop.R3.10X.trt.wide.1.pearson %>% mutate(KS.p = '0.2096083')
net_prop.R3.10X.trt.final.pearson <- gather(net_prop.R3.10X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.R3.10X.trt.final.pearson <- net_prop.R3.10X.trt.final.pearson %>% mutate(Condition = 'Condition 11')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
R6.10X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.10X.trt.pearson <- prune_samples(sample_data(R6.10X.trt.pearson)$Treatment == "Biostimulant", R6.10X.trt.pearson)
R6.10X.trt.pearson <- prune_samples(sample_data(R6.10X.trt.pearson)$Cultivar == "CZ4810X", R6.10X.trt.pearson)

R6.10X.trt.pearson.table <- co_occurrence(R6.10X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R6.10X.trt.pearson.table$p <- p.adjust(R6.10X.trt.pearson.table$p, "fdr")
R6.10X.trt.pearson.table <- filter(R6.10X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
R6.10X.trt.pearson.layout <- network_layout_ps(R6.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.trt.pearson.table, algorithm = 'sphere')

co_occurrence_network(R6.10X.trt.pearson, co_occurrence_table = R6.10X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R6.10X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R6.10X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R6.10X.trt.net.pearson <- network_ps(R6.10X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.10X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.10X.trt.pearson <- net_properties(R6.10X.trt.net.pearson)
node_prop.R6.10X.trt.pearson <- node_properties(R6.10X.trt.net.pearson)
hub_score.R6.10X.trt.pearson <- hub_score(R6.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.10X.trt.pearson <- authority_score(R6.10X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.10X.trt.pearson <- neighborhood.connectivity(R6.10X.trt.net.pearson, vertices = V(R6.10X.trt.net.pearson), mode = "all")
neighbor.R6.10X.trt.pearson <- as.data.frame(neighbor.R6.10X.trt.pearson)

##Network properties for comparison
net_prop.R6.10X.trt.pearson <- as.data.frame(net_prop.R6.10X.trt.pearson)
net_prop.R6.10X.trt.pearson <- tibble::rownames_to_column(net_prop.R6.10X.trt.pearson, "metric")
net_prop.R6.10X.trt.pearson <- net_prop.R6.10X.trt.pearson %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.10X.trt.pearson <- net_prop.R6.10X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.R6.10X.trt.pearson <- net_prop.R6.10X.trt.pearson %>% mutate(Cultivar = 'CZ4810X')

##Add hub info
hub_score.R6.10X.trt.pearson <- as.data.frame(hub_score.R6.10X.trt.pearson)
hub_score.R6.10X.trt.pearson.1 <- filter(hub_score.R6.10X.trt.pearson, vector >= 0.2)

##48

net_prop.R6.10X.trt.pearson <- net_prop.R6.10X.trt.pearson %>% mutate(Hub_Count = '48')

##Add mean neighborhood connectivity
mean(neighbor.R6.10X.trt.pearson$neighbor.R6.10X.trt.pearson) ##23.60696
net_prop.R6.10X.trt.pearson <- net_prop.R6.10X.trt.pearson %>% mutate(Mean_neighborhood = '23.60696')

net_prop.R6.10X.trt.wide.pearson <- spread(net_prop.R6.10X.trt.pearson, metric, value)
net_prop.R6.10X.trt.final.pearson <- gather(net_prop.R6.10X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R6.10X.trt.node.names.pearson <- as.data.frame(V(R6.10X.trt.net.pearson)$name)
R6.10X.trt.node.names.pearson <- R6.10X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'R6')
R6.10X.trt.node.names.pearson <- R6.10X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
R6.10X.trt.node.names.pearson <- R6.10X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4810X')
names(R6.10X.trt.node.names.pearson)[names(R6.10X.trt.node.names.pearson) == "V(R6.10X.trt.net.pearson)$name"] <- "genus"
R6.10X.trt.node.names.pearson <- R6.10X.trt.node.names.pearson %>% mutate(Condition = 'Condition 12')
hub_score.R6.10X.trt.pearson$genus <- rownames(hub_score.R6.10X.trt.pearson)
R6.10X.trt.heat.pearson <- merge(R6.10X.trt.node.names.pearson, hub_score.R6.10X.trt.pearson, by = c("genus"))
R6.10X.trt.heat.pearson <- subset(R6.10X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
R6.10X.trt.giant_component_size.pearson <- max(components(R6.10X.trt.net.pearson)$csize) ##49
R6.10X.trt.giant_component_size.pearson <- as.data.frame(R6.10X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R6.10X.trt.community.pearson <- cluster_walktrap(R6.10X.trt.net.pearson)

## Get the community membership vector and calculate modularity
R6.10X.trt.membership.pearson <- R6.10X.trt.community.pearson$membership
R6.10X.trt.modularity.pearson <- modularity(R6.10X.trt.community.pearson) 
R6.10X.trt.modularity.pearson <- as.data.frame(R6.10X.trt.modularity.pearson) ##0.5683578

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.10X.trt.degree_dist.pearson <- degree_distribution(R6.10X.trt.net.pearson)
# Step 2: Fit the power-law distribution
R6.10X.trt.pl_fit.pearson <- fit_power_law(R6.10X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R6.10X.trt.pl_fit.pearson <- as.data.frame(R6.10X.trt.pl_fit.pearson)
## Correct KS p-value
R6.10X.trt.pl_fit.pearson$KS.p <- p.adjust(R6.10X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1823493, ##KS.p = 0.8197748

## Add additional metrics to the dataframe
net_prop.R6.10X.trt.wide.1.pearson <- spread(net_prop.R6.10X.trt.final.pearson, metric, value)
net_prop.R6.10X.trt.wide.1.pearson <- net_prop.R6.10X.trt.wide.1.pearson %>% mutate(giant_component_size = '49')
net_prop.R6.10X.trt.wide.1.pearson <- net_prop.R6.10X.trt.wide.1.pearson %>% mutate(Modularity = '0.5683578')
net_prop.R6.10X.trt.wide.1.pearson <- net_prop.R6.10X.trt.wide.1.pearson %>% mutate(KS.stat = '0.1823493')
net_prop.R6.10X.trt.wide.1.pearson <- net_prop.R6.10X.trt.wide.1.pearson %>% mutate(KS.p = '0.8197748')
net_prop.R6.10X.trt.final.pearson <- gather(net_prop.R6.10X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.R6.10X.trt.final.pearson <- net_prop.R6.10X.trt.final.pearson %>% mutate(Condition = 'Condition 12')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
V1.79X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V1", filtered_obj)
V1.79X.trt.pearson <- prune_samples(sample_data(V1.79X.trt.pearson)$Treatment == "Biostimulant", V1.79X.trt.pearson)
V1.79X.trt.pearson <- prune_samples(sample_data(V1.79X.trt.pearson)$Cultivar == "CZ4979X", V1.79X.trt.pearson)

V1.79X.trt.pearson.table <- co_occurrence(V1.79X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V1.79X.trt.pearson.table$p <- p.adjust(V1.79X.trt.pearson.table$p, "fdr")
V1.79X.trt.pearson.table <- filter(V1.79X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
V1.79X.trt.pearson.layout <- network_layout_ps(V1.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.trt.pearson.table, algorithm = 'sphere')

co_occurrence_network(V1.79X.trt.pearson, co_occurrence_table = V1.79X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V1.79X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V1.79X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V1.79X.trt.net.pearson <- network_ps(V1.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V1.79X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V1.79X.trt.pearson <- net_properties(V1.79X.trt.net.pearson)
node_prop.V1.79X.trt.pearson <- node_properties(V1.79X.trt.net.pearson)
hub_score.V1.79X.trt.pearson <- hub_score(V1.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V1.79X.trt.pearson <- authority_score(V1.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V1.79X.trt.pearson <- neighborhood.connectivity(V1.79X.trt.net.pearson, vertices = V(V1.79X.trt.net.pearson), mode = "all")
neighbor.V1.79X.trt.pearson <- as.data.frame(neighbor.V1.79X.trt.pearson)

##Network properties for comparison
net_prop.V1.79X.trt.pearson <- as.data.frame(net_prop.V1.79X.trt.pearson)
net_prop.V1.79X.trt.pearson <- tibble::rownames_to_column(net_prop.V1.79X.trt.pearson, "metric")
net_prop.V1.79X.trt.pearson <- net_prop.V1.79X.trt.pearson %>% mutate(Developmental_Stage = 'V1')
net_prop.V1.79X.trt.pearson <- net_prop.V1.79X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.V1.79X.trt.pearson <- net_prop.V1.79X.trt.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V1.79X.trt.pearson <- as.data.frame(hub_score.V1.79X.trt.pearson)
hub_score.V1.79X.trt.pearson.1 <- filter(hub_score.V1.79X.trt.pearson, vector >= 0.2)

##29

net_prop.V1.79X.trt.pearson <- net_prop.V1.79X.trt.pearson %>% mutate(Hub_Count = '29')

##Add mean neighborhood connectivity
mean(neighbor.V1.79X.trt.pearson$neighbor.V1.79X.trt.pearson) ##13.68148
net_prop.V1.79X.trt.pearson <- net_prop.V1.79X.trt.pearson %>% mutate(Mean_neighborhood = '13.68148')

net_prop.V1.79X.trt.wide.pearson <- spread(net_prop.V1.79X.trt.pearson, metric, value)
net_prop.V1.79X.trt.final.pearson <- gather(net_prop.V1.79X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V1.79X.trt.node.names.pearson <- as.data.frame(V(V1.79X.trt.net.pearson)$name)
V1.79X.trt.node.names.pearson <- V1.79X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'V1')
V1.79X.trt.node.names.pearson <- V1.79X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
V1.79X.trt.node.names.pearson <- V1.79X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(V1.79X.trt.node.names.pearson)[names(V1.79X.trt.node.names.pearson) == "V(V1.79X.trt.net.pearson)$name"] <- "genus"
V1.79X.trt.node.names.pearson <- V1.79X.trt.node.names.pearson %>% mutate(Condition = 'Condition 13')
hub_score.V1.79X.trt.pearson$genus <- rownames(hub_score.V1.79X.trt.pearson)
V1.79X.trt.heat.pearson <- merge(V1.79X.trt.node.names.pearson, hub_score.V1.79X.trt.pearson, by = c("genus"))
V1.79X.trt.heat.pearson <- subset(V1.79X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
V1.79X.trt.giant_component_size.pearson <- max(components(V1.79X.trt.net.pearson)$csize) ##31
V1.79X.trt.giant_component_size.pearson <- as.data.frame(V1.79X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V1.79X.trt.community.pearson <- cluster_walktrap(V1.79X.trt.net.pearson)

## Get the community membership vector and calculate modularity
V1.79X.trt.membership.pearson <- V1.79X.trt.community.pearson$membership
V1.79X.trt.modularity.pearson <- modularity(V1.79X.trt.community.pearson) 
V1.79X.trt.modularity.pearson <- as.data.frame(V1.79X.trt.modularity.pearson) ##0.6355573

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V1.79X.trt.degree_dist.pearson <- degree_distribution(V1.79X.trt.net.pearson)
# Step 2: Fit the power-law distribution
V1.79X.trt.pl_fit.pearson <- fit_power_law(V1.79X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V1.79X.trt.pl_fit.pearson <- as.data.frame(V1.79X.trt.pl_fit.pearson)
## Correct KS p-value
V1.79X.trt.pl_fit.pearson$KS.p <- p.adjust(V1.79X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.2096244, ##KS.p = 0.6173925

## Add additional metrics to the dataframe
net_prop.V1.79X.trt.wide.1.pearson <- spread(net_prop.V1.79X.trt.final.pearson, metric, value)
net_prop.V1.79X.trt.wide.1.pearson <- net_prop.V1.79X.trt.wide.1.pearson %>% mutate(giant_component_size = '31')
net_prop.V1.79X.trt.wide.1.pearson <- net_prop.V1.79X.trt.wide.1.pearson %>% mutate(Modularity = '0.6355573')
net_prop.V1.79X.trt.wide.1.pearson <- net_prop.V1.79X.trt.wide.1.pearson %>% mutate(KS.stat = '0.2096244')
net_prop.V1.79X.trt.wide.1.pearson <- net_prop.V1.79X.trt.wide.1.pearson %>% mutate(KS.p = '0.6173925')
net_prop.V1.79X.trt.final.pearson <- gather(net_prop.V1.79X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.V1.79X.trt.final.pearson <- net_prop.V1.79X.trt.final.pearson %>% mutate(Condition = 'Condition 13')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
V6.79X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "V6", filtered_obj)
V6.79X.trt.pearson <- prune_samples(sample_data(V6.79X.trt.pearson)$Treatment == "Biostimulant", V6.79X.trt.pearson)
V6.79X.trt.pearson <- prune_samples(sample_data(V6.79X.trt.pearson)$Cultivar == "CZ4979X", V6.79X.trt.pearson)

V6.79X.trt.pearson.table <- co_occurrence(V6.79X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
V6.79X.trt.pearson.table$p <- p.adjust(V6.79X.trt.pearson.table$p, "fdr")
V6.79X.trt.pearson.table <- filter(V6.79X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
V6.79X.trt.pearson.layout <- network_layout_ps(V6.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.trt.pearson.table, algorithm = 'sphere')

co_occurrence_network(V6.79X.trt.pearson, co_occurrence_table = V6.79X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = V6.79X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("V6.79X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
V6.79X.trt.net.pearson <- network_ps(V6.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = V6.79X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.V6.79X.trt.pearson <- net_properties(V6.79X.trt.net.pearson)
node_prop.V6.79X.trt.pearson <- node_properties(V6.79X.trt.net.pearson)
hub_score.V6.79X.trt.pearson <- hub_score(V6.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.V6.79X.trt.pearson <- authority_score(V6.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.V6.79X.trt.pearson <- neighborhood.connectivity(V6.79X.trt.net.pearson, vertices = V(V6.79X.trt.net.pearson), mode = "all")
neighbor.V6.79X.trt.pearson <- as.data.frame(neighbor.V6.79X.trt.pearson)

##Network properties for comparison
net_prop.V6.79X.trt.pearson <- as.data.frame(net_prop.V6.79X.trt.pearson)
net_prop.V6.79X.trt.pearson <- tibble::rownames_to_column(net_prop.V6.79X.trt.pearson, "metric")
net_prop.V6.79X.trt.pearson <- net_prop.V6.79X.trt.pearson %>% mutate(Developmental_Stage = 'V6')
net_prop.V6.79X.trt.pearson <- net_prop.V6.79X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.V6.79X.trt.pearson <- net_prop.V6.79X.trt.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.V6.79X.trt.pearson <- as.data.frame(hub_score.V6.79X.trt.pearson)
hub_score.V6.79X.trt.pearson.1 <- filter(hub_score.V6.79X.trt.pearson, vector >= 0.2)

##21

net_prop.V6.79X.trt.pearson <- net_prop.V6.79X.trt.pearson %>% mutate(Hub_Count = '21')

##Add mean neighborhood connectivity
mean(neighbor.V6.79X.trt.pearson$neighbor.V6.79X.trt.pearson) ##11.47115
net_prop.V6.79X.trt.pearson <- net_prop.V6.79X.trt.pearson %>% mutate(Mean_neighborhood = '11.47115')

net_prop.V6.79X.trt.wide.pearson <- spread(net_prop.V6.79X.trt.pearson, metric, value)
net_prop.V6.79X.trt.final.pearson <- gather(net_prop.V6.79X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
V6.79X.trt.node.names.pearson <- as.data.frame(V(V6.79X.trt.net.pearson)$name)
V6.79X.trt.node.names.pearson <- V6.79X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'V6')
V6.79X.trt.node.names.pearson <- V6.79X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
V6.79X.trt.node.names.pearson <- V6.79X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(V6.79X.trt.node.names.pearson)[names(V6.79X.trt.node.names.pearson) == "V(V6.79X.trt.net.pearson)$name"] <- "genus"
V6.79X.trt.node.names.pearson <- V6.79X.trt.node.names.pearson %>% mutate(Condition = 'Condition 14')
hub_score.V6.79X.trt.pearson$genus <- rownames(hub_score.V6.79X.trt.pearson)
V6.79X.trt.heat.pearson <- merge(V6.79X.trt.node.names.pearson, hub_score.V6.79X.trt.pearson, by = c("genus"))
V6.79X.trt.heat.pearson <- subset(V6.79X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
V6.79X.trt.giant_component_size.pearson <- max(components(V6.79X.trt.net.pearson)$csize) ##36
V6.79X.trt.giant_component_size.pearson <- as.data.frame(V6.79X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
V6.79X.trt.community.pearson <- cluster_walktrap(V6.79X.trt.net.pearson)

## Get the community membership vector and calculate modularity
V6.79X.trt.membership.pearson <- V6.79X.trt.community.pearson$membership
V6.79X.trt.modularity.pearson <- modularity(V6.79X.trt.community.pearson) 
V6.79X.trt.modularity.pearson <- as.data.frame(V6.79X.trt.modularity.pearson) ##0.7163988

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
V6.79X.trt.degree_dist.pearson <- degree_distribution(V6.79X.trt.net.pearson)
# Step 2: Fit the power-law distribution
V6.79X.trt.pl_fit.pearson <- fit_power_law(V6.79X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
V6.79X.trt.pl_fit.pearson <- as.data.frame(V6.79X.trt.pl_fit.pearson)
## Correct KS p-value
V6.79X.trt.pl_fit.pearson$KS.p <- p.adjust(V6.79X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.2433654, ##KS.p = 0.5943143

## Add additional metrics to the dataframe
net_prop.V6.79X.trt.wide.1.pearson <- spread(net_prop.V6.79X.trt.final.pearson, metric, value)
net_prop.V6.79X.trt.wide.1.pearson <- net_prop.V6.79X.trt.wide.1.pearson %>% mutate(giant_component_size = '36')
net_prop.V6.79X.trt.wide.1.pearson <- net_prop.V6.79X.trt.wide.1.pearson %>% mutate(Modularity = '0.7163988')
net_prop.V6.79X.trt.wide.1.pearson <- net_prop.V6.79X.trt.wide.1.pearson %>% mutate(KS.stat = '0.2433654')
net_prop.V6.79X.trt.wide.1.pearson <- net_prop.V6.79X.trt.wide.1.pearson %>% mutate(KS.p = '0.5943143')
net_prop.V6.79X.trt.final.pearson <- gather(net_prop.V6.79X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.V6.79X.trt.final.pearson <- net_prop.V6.79X.trt.final.pearson %>% mutate(Condition = 'Condition 14')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
R3.79X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R3", filtered_obj)
R3.79X.trt.pearson <- prune_samples(sample_data(R3.79X.trt.pearson)$Treatment == "Biostimulant", R3.79X.trt.pearson)
R3.79X.trt.pearson <- prune_samples(sample_data(R3.79X.trt.pearson)$Cultivar == "CZ4979X", R3.79X.trt.pearson)

R3.79X.trt.pearson.table <- co_occurrence(R3.79X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R3.79X.trt.pearson.table$p <- p.adjust(R3.79X.trt.pearson.table$p, "fdr")
R3.79X.trt.pearson.table <- filter(R3.79X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
R3.79X.trt.pearson.layout <- network_layout_ps(R3.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.trt.pearson.table, algorithm = 'sphere')

co_occurrence_network(R3.79X.trt.pearson, co_occurrence_table = R3.79X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R3.79X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

#ggsave("R3.79X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R3.79X.trt.net.pearson <- network_ps(R3.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R3.79X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R3.79X.trt.pearson <- net_properties(R3.79X.trt.net.pearson)
node_prop.R3.79X.trt.pearson <- node_properties(R3.79X.trt.net.pearson)
hub_score.R3.79X.trt.pearson <- hub_score(R3.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R3.79X.trt.pearson <- authority_score(R3.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R3.79X.trt.pearson <- neighborhood.connectivity(R3.79X.trt.net.pearson, vertices = V(R3.79X.trt.net.pearson), mode = "all")
neighbor.R3.79X.trt.pearson <- as.data.frame(neighbor.R3.79X.trt.pearson)

##Network properties for comparison
net_prop.R3.79X.trt.pearson <- as.data.frame(net_prop.R3.79X.trt.pearson)
net_prop.R3.79X.trt.pearson <- tibble::rownames_to_column(net_prop.R3.79X.trt.pearson, "metric")
net_prop.R3.79X.trt.pearson <- net_prop.R3.79X.trt.pearson %>% mutate(Developmental_Stage = 'R3')
net_prop.R3.79X.trt.pearson <- net_prop.R3.79X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.R3.79X.trt.pearson <- net_prop.R3.79X.trt.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R3.79X.trt.pearson <- as.data.frame(hub_score.R3.79X.trt.pearson)
hub_score.R3.79X.trt.pearson.1 <- filter(hub_score.R3.79X.trt.pearson, vector >= 0.2)

##19

net_prop.R3.79X.trt.pearson <- net_prop.R3.79X.trt.pearson %>% mutate(Hub_Count = '19')

##Add mean neighborhood connectivity
mean(neighbor.R3.79X.trt.pearson$neighbor.R3.79X.trt.pearson) ##10.13823
net_prop.R3.79X.trt.pearson <- net_prop.R3.79X.trt.pearson %>% mutate(Mean_neighborhood = '10.13823')

net_prop.R3.79X.trt.wide.pearson <- spread(net_prop.R3.79X.trt.pearson, metric, value)
net_prop.R3.79X.trt.final.pearson <- gather(net_prop.R3.79X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R3.79X.trt.node.names.pearson <- as.data.frame(V(R3.79X.trt.net.pearson)$name)
R3.79X.trt.node.names.pearson <- R3.79X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'R3')
R3.79X.trt.node.names.pearson <- R3.79X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
R3.79X.trt.node.names.pearson <- R3.79X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(R3.79X.trt.node.names.pearson)[names(R3.79X.trt.node.names.pearson) == "V(R3.79X.trt.net.pearson)$name"] <- "genus"
R3.79X.trt.node.names.pearson <- R3.79X.trt.node.names.pearson %>% mutate(Condition = 'Condition 15')
hub_score.R3.79X.trt.pearson$genus <- rownames(hub_score.R3.79X.trt.pearson)
R3.79X.trt.heat.pearson <- merge(R3.79X.trt.node.names.pearson, hub_score.R3.79X.trt.pearson, by = c("genus"))
R3.79X.trt.heat.pearson <- subset(R3.79X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
R3.79X.trt.giant_component_size.pearson <- max(components(R3.79X.trt.net.pearson)$csize) ##27
R3.79X.trt.giant_component_size.pearson <- as.data.frame(R3.79X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R3.79X.trt.community.pearson <- cluster_walktrap(R3.79X.trt.net.pearson)

## Get the community membership vector and calculate modularity
R3.79X.trt.membership.pearson <- R3.79X.trt.community.pearson$membership
R3.79X.trt.modularity.pearson <- modularity(R3.79X.trt.community.pearson) 
R3.79X.trt.modularity.pearson <- as.data.frame(R3.79X.trt.modularity.pearson) ##0.8107593

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R3.79X.trt.degree_dist.pearson <- degree_distribution(R3.79X.trt.net.pearson)
# Step 2: Fit the power-law distribution
R3.79X.trt.pl_fit.pearson <- fit_power_law(R3.79X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R3.79X.trt.pl_fit.pearson <- as.data.frame(R3.79X.trt.pl_fit.pearson)
## Correct KS p-value
R3.79X.trt.pl_fit.pearson$KS.p <- p.adjust(R3.79X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1335853, ##KS.p = 0.9999917

## Add additional metrics to the dataframe
net_prop.R3.79X.trt.wide.1.pearson <- spread(net_prop.R3.79X.trt.final.pearson, metric, value)
net_prop.R3.79X.trt.wide.1.pearson <- net_prop.R3.79X.trt.wide.1.pearson %>% mutate(giant_component_size = '27')
net_prop.R3.79X.trt.wide.1.pearson <- net_prop.R3.79X.trt.wide.1.pearson %>% mutate(Modularity = '0.8107593')
net_prop.R3.79X.trt.wide.1.pearson <- net_prop.R3.79X.trt.wide.1.pearson %>% mutate(KS.stat = '0.1335853')
net_prop.R3.79X.trt.wide.1.pearson <- net_prop.R3.79X.trt.wide.1.pearson %>% mutate(KS.p = '0.9999917')
net_prop.R3.79X.trt.final.pearson <- gather(net_prop.R3.79X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.R3.79X.trt.final.pearson <- net_prop.R3.79X.trt.final.pearson %>% mutate(Condition = 'Condition 15')
```

```{r}
#Reconstruct condition-specific networks with Pearson correlations
R6.79X.trt.pearson <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
R6.79X.trt.pearson <- prune_samples(sample_data(R6.79X.trt.pearson)$Treatment == "Biostimulant", R6.79X.trt.pearson)
R6.79X.trt.pearson <- prune_samples(sample_data(R6.79X.trt.pearson)$Cultivar == "CZ4979X", R6.79X.trt.pearson)

R6.79X.trt.pearson.table <- co_occurrence(R6.79X.trt.pearson, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = 'pearson')
##fdr correction and filtering
R6.79X.trt.pearson.table$p <- p.adjust(R6.79X.trt.pearson.table$p, "fdr")
R6.79X.trt.pearson.table <- filter(R6.79X.trt.pearson.table, p <= 0.05)

##network_layout_ps function
R6.79X.trt.pearson.layout <- network_layout_ps(R6.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.trt.pearson.table, algorithm = 'sphere')

co_occurrence_network(R6.79X.trt.pearson, co_occurrence_table = R6.79X.trt.pearson.table, classification = "kingdom", node_colors = net.col, cluster = FALSE, cluster_colors = 'default', layout = R6.79X.trt.pearson.layout, negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none') 
#ggsave("R6.79X.trt.net.png", width = 3, height = 3, units = "in", dpi = 900)

##network_ps function 
R6.79X.trt.net.pearson <- network_ps(R6.79X.trt.pearson, treatment = NULL, subset = NULL, co_occurrence_table = R6.79X.trt.pearson.table, rho = 0.6)

##Network properties- ggclusternet package
net_prop.R6.79X.trt.pearson <- net_properties(R6.79X.trt.net.pearson)
node_prop.R6.79X.trt.pearson <- node_properties(R6.79X.trt.net.pearson)
hub_score.R6.79X.trt.pearson <- hub_score(R6.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)
authority_score.R6.79X.trt.pearson <- authority_score(R6.79X.trt.net.pearson, scale = TRUE, weights = NULL, options = arpack_defaults)

##package influential
neighbor.R6.79X.trt.pearson <- neighborhood.connectivity(R6.79X.trt.net.pearson, vertices = V(R6.79X.trt.net.pearson), mode = "all")
neighbor.R6.79X.trt.pearson <- as.data.frame(neighbor.R6.79X.trt.pearson)

##Network properties for comparison
net_prop.R6.79X.trt.pearson <- as.data.frame(net_prop.R6.79X.trt.pearson)
net_prop.R6.79X.trt.pearson <- tibble::rownames_to_column(net_prop.R6.79X.trt.pearson, "metric")
net_prop.R6.79X.trt.pearson <- net_prop.R6.79X.trt.pearson %>% mutate(Developmental_Stage = 'R6')
net_prop.R6.79X.trt.pearson <- net_prop.R6.79X.trt.pearson %>% mutate(Treatment = 'Biostimulant')
net_prop.R6.79X.trt.pearson <- net_prop.R6.79X.trt.pearson %>% mutate(Cultivar = 'CZ4979X')

##Add hub info
hub_score.R6.79X.trt.pearson <- as.data.frame(hub_score.R6.79X.trt.pearson)
hub_score.R6.79X.trt.pearson.1 <- filter(hub_score.R6.79X.trt.pearson, vector >= 0.2)

##31

net_prop.R6.79X.trt.pearson <- net_prop.R6.79X.trt.pearson %>% mutate(Hub_Count = '31')

##Add mean neighborhood connectivity
mean(neighbor.R6.79X.trt.pearson$neighbor.R6.79X.trt.pearson) ##14.59178
net_prop.R6.79X.trt.pearson <- net_prop.R6.79X.trt.pearson %>% mutate(Mean_neighborhood = '14.59178')

net_prop.R6.79X.trt.wide.pearson <- spread(net_prop.R6.79X.trt.pearson, metric, value)
net_prop.R6.79X.trt.final.pearson <- gather(net_prop.R6.79X.trt.wide.pearson, metric, value, 4:19)

##Node table for heatmap
R6.79X.trt.node.names.pearson <- as.data.frame(V(R6.79X.trt.net.pearson)$name)
R6.79X.trt.node.names.pearson <- R6.79X.trt.node.names.pearson %>% mutate(Developmental_Stage = 'R6')
R6.79X.trt.node.names.pearson <- R6.79X.trt.node.names.pearson %>% mutate(Treatment = 'Biostimulant')
R6.79X.trt.node.names.pearson <- R6.79X.trt.node.names.pearson %>% mutate(Cultivar = 'CZ4979X')
names(R6.79X.trt.node.names.pearson)[names(R6.79X.trt.node.names.pearson) == "V(R6.79X.trt.net.pearson)$name"] <- "genus"
R6.79X.trt.node.names.pearson <- R6.79X.trt.node.names.pearson %>% mutate(Condition = 'Condition 16')
hub_score.R6.79X.trt.pearson$genus <- rownames(hub_score.R6.79X.trt.pearson)
R6.79X.trt.heat.pearson <- merge(R6.79X.trt.node.names.pearson, hub_score.R6.79X.trt.pearson, by = c("genus"))
R6.79X.trt.heat.pearson <- subset(R6.79X.trt.heat.pearson, select = -c(8:27))

##Additional measurements
R6.79X.trt.giant_component_size.pearson <- max(components(R6.79X.trt.net.pearson)$csize) ##32
R6.79X.trt.giant_component_size.pearson <- as.data.frame(R6.79X.trt.giant_component_size.pearson)

## Community detection
# Perform community detection using walktrap method
R6.79X.trt.community.pearson <- cluster_walktrap(R6.79X.trt.net.pearson)

## Get the community membership vector and calculate modularity
R6.79X.trt.membership.pearson <- R6.79X.trt.community.pearson$membership
R6.79X.trt.modularity.pearson <- modularity(R6.79X.trt.community.pearson) 
R6.79X.trt.modularity.pearson <- as.data.frame(R6.79X.trt.modularity.pearson) ##0.6732123

## Estimate the parameters of the power-law distribution
# Step 1: Convert igraph object to degree distribution vector
R6.79X.trt.degree_dist.pearson <- degree_distribution(R6.79X.trt.net.pearson)
# Step 2: Fit the power-law distribution
R6.79X.trt.pl_fit.pearson <- fit_power_law(R6.79X.trt.degree_dist.pearson, implementation = "plfit", force.continuous = TRUE) 
R6.79X.trt.pl_fit.pearson <- as.data.frame(R6.79X.trt.pl_fit.pearson)
## Correct KS p-value
R6.79X.trt.pl_fit.pearson$KS.p <- p.adjust(R6.79X.trt.pl_fit.pearson$KS.p, "fdr") ##KS.stat = 0.1666667, ##KS.p = 0.8927783

## Add additional metrics to the dataframe
net_prop.R6.79X.trt.wide.1.pearson <- spread(net_prop.R6.79X.trt.final.pearson, metric, value)
net_prop.R6.79X.trt.wide.1.pearson <- net_prop.R6.79X.trt.wide.1.pearson %>% mutate(giant_component_size = '32')
net_prop.R6.79X.trt.wide.1.pearson <- net_prop.R6.79X.trt.wide.1.pearson %>% mutate(Modularity = '0.6732123')
net_prop.R6.79X.trt.wide.1.pearson <- net_prop.R6.79X.trt.wide.1.pearson %>% mutate(KS.stat = '0.1666667')
net_prop.R6.79X.trt.wide.1.pearson <- net_prop.R6.79X.trt.wide.1.pearson %>% mutate(KS.p = '0.8927783')
net_prop.R6.79X.trt.final.pearson <- gather(net_prop.R6.79X.trt.wide.1.pearson, metric, value, 4:23)
net_prop.R6.79X.trt.final.pearson <- net_prop.R6.79X.trt.final.pearson %>% mutate(Condition = 'Condition 16')
```

```{r}
##Let's compare Pearson and Spearman Correlations

##First, we need to get the network properties derived from Spearman Correlation and specify correlation method
Network_Properties <- Network_Properties %>% mutate(method = "Spearman")

##Now, let's consolidate network properties for Pearson networks
Network_Properties.pearson <- rbind(net_prop.V1.10X.ctrl.final.pearson, net_prop.V6.10X.ctrl.final.pearson,net_prop.R3.10X.ctrl.final.pearson, net_prop.R6.10X.ctrl.final.pearson, net_prop.V1.79X.ctrl.final.pearson, net_prop.V6.79X.ctrl.final.pearson, net_prop.R3.79X.ctrl.final.pearson,
net_prop.R6.79X.ctrl.final.pearson, net_prop.V1.10X.trt.final.pearson, net_prop.V6.10X.trt.final.pearson, net_prop.R3.10X.trt.final.pearson,
net_prop.R6.10X.trt.final.pearson, net_prop.V1.79X.trt.final.pearson, net_prop.V6.79X.trt.final.pearson, net_prop.R3.79X.trt.final.pearson,
net_prop.R6.79X.trt.final.pearson)
                                    
Network_Properties.pearson$Developmental_Stage[Network_Properties.pearson$Developmental_Stage == "R3"] <- "R2"
Network_Properties.pearson$Developmental_Stage <- factor(Network_Properties.pearson$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))
Network_Properties.pearson$value <- as.numeric(Network_Properties.pearson$value)

## Filter out metrics we don't want to plot
Network_Properties.pearson <- Network_Properties.pearson[Network_Properties.pearson$metric != "num.pos.edges" & Network_Properties.pearson$metric != "num.neg.edges" & Network_Properties.pearson$metric != "centralization.betweenness"
& Network_Properties.pearson$metric != "centralization.closeness" & Network_Properties.pearson$metric != "clustering.coefficient" & 
  Network_Properties.pearson$metric != "average.path.length" & Network_Properties.pearson$metric != "diameter" &
  Network_Properties.pearson$metric != "edge.connectivity" &
  Network_Properties.pearson$metric != "KS.p" &
  Network_Properties.pearson$metric != "Mean_neighborhood", ] 

## Rename metrics
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'average.degree'] <- 'Mean degree'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'centralization.degree'] <- 'Centralization degree'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'connectance'] <- 'Connectance'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'Hub_Count'] <- 'Hub count'
##Network_Properties.pearson$metric [Network_Properties.pearson$metric == 'Mean_neighborhood'] <- 'MNC'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'no.clusters'] <- 'Cluster count'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'num.edges'] <- 'Edge count'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'num.vertices'] <- 'Node count'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'KS.stat'] <- 'KS stat'
Network_Properties.pearson$metric[Network_Properties.pearson$metric == 'giant_component_size'] <- 'Giant component size'

Network_Properties.pearson <- Network_Properties.pearson %>% mutate(method = "Pearson")

##Combine
correlation.comparison <- rbind(Network_Properties, Network_Properties.pearson)

correlation.comparison.1 <- correlation.comparison %>%
  group_by(Condition, metric) %>%
  filter(n_distinct(method) == n_distinct(.$method)) %>%
  ungroup()

##Generate statistics
wilcoxon_pvalues <- correlation.comparison.1 %>%
  group_by(metric) %>%
  summarise(wilcox_pvalue = wilcox.test(value ~ method)$p.value)

##Spread data for Levene Test- having issues with easier methods
spread_data <- spread(correlation.comparison.1, metric, value)

cent.degree <- leveneTest(spread_data$`Centralization degree` ~ method, data = spread_data)
cent.degree <- cent.degree %>% mutate(metric = "Centralization degree")

cluster.count <- leveneTest(spread_data$`Cluster count` ~ method, data = spread_data)
cluster.count <- cluster.count %>% mutate(metric = "Cluster count")

Connectance <- leveneTest(spread_data$Connectance ~ method, data = spread_data)
Connectance <- Connectance %>% mutate(metric = "Connectance")

Edge <- leveneTest(spread_data$`Edge count` ~ method, data = spread_data)
Edge <- Edge %>% mutate(metric = "Edge count")

Giant <- leveneTest(spread_data$`Giant component size` ~ method, data = spread_data)
Giant <- Giant %>% mutate(metric = "Giant component size")

Hub <- leveneTest(spread_data$`Hub count` ~ method, data = spread_data)
Hub <- Hub %>% mutate(metric = "Hub count")

KS  <- leveneTest(spread_data$`KS stat` ~ method, data = spread_data)
KS  <- KS %>% mutate(metric = "KS stat")

degree  <- leveneTest(spread_data$`Mean degree` ~ method, data = spread_data)
degree  <- degree %>% mutate(metric = "Mean degree")

Modularity <- leveneTest(spread_data$Modularity ~ method, data = spread_data)
Modularity  <- Modularity %>% mutate(metric = "Modularity")

Node <- leveneTest(spread_data$`Node count` ~ method, data = spread_data)
Node  <- Node %>% mutate(metric = "Node count")


Levene <- rbind(cent.degree, cluster.count, Connectance, Edge, Giant, Hub, KS, degree, Modularity, Node)

Levene <- Levene[complete.cases(Levene$`F value`), ]

correlation.comparison.2 <- merge(correlation.comparison.1, Levene, by = c("metric"))

##Plot
correlation.comparison.2 <- correlation.comparison.2 %>%
  mutate(facet_label = glue::glue("Levene p-val = {round(`Pr(>F)`, 4)}")) # Add facet labels

ggplot(correlation.comparison.2, aes(x = method, y = value, color = method)) +
  geom_line(aes(group = Condition), size = 0.7, color = "black") +
  geom_point(size = 4) +
  facet_wrap(~ metric, scales = "free", nrow = 2) +
  theme_bw() +
  theme(
    legend.position = "top",
    axis.title = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(color = "black", face = "bold", size = 12),
    strip.text = element_text(size = 12, face = "bold", color = "black"),
    legend.text = element_text(size = 10),
    legend.title = element_blank(),
    axis.ticks.x = element_blank()) +
  scale_color_manual(values = c("#8c3800", "lightblue3")) #+
 # geom_text(
   # aes(label = facet_label),
   # x = Inf,
   # y = -Inf,
    #hjust = 1.3,
    #vjust = -1,
    #color = "black",
    #size = 4)

ggsave("correlation.comparison.2.png", width = 12, height = 6, units = "in", dpi = 900)

##visualize Pearson Network Properties
ggplot(Network_Properties.pearson, aes(x = Developmental_Stage, y = value, fill = interaction(Treatment, Cultivar))) + 
  geom_col(aes(y = 0), alpha = 0, width = 1) +  # Temporary invisible layer
  geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="V6") -0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="V6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
 geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="R6")-0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="R6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
  geom_col( stat = "identity", position=position_dodge(0.9), colour="black", width=0.7) +
  facet_wrap(~metric, scales = "free", nrow = 2) +
  theme_bw() +
  scale_fill_manual(values=c("lightblue3", "antiquewhite", "#8c3800", "#c0c0c0")) +
  theme(axis.title = element_blank(), axis.text.y = element_text(size = 12, color = "black"), axis.ticks = element_blank(), 
        strip.text = element_text(size = 14, face = "bold"), axis.text.x = element_text(size = 12, color = "black", face = "bold"), 
        legend.title = element_text(size = 12, face = "bold"), legend.text = element_text(size = 10), legend.position = "top") +
  labs(fill = "Treatment + Cultivar") +
  guides(fill=guide_legend(nrow=1,byrow=TRUE))

ggsave("pearson_network_stats.2.png", width = 13, height = 5, units = "in", dpi = 900)

##Summary trends for paper
# Filter data based on the given conditions and summarize
# Create a new variable to group R2 and R6 together, and V1 and V6 together
Network_Properties$Stage_Group <- case_when(
  Network_Properties$Developmental_Stage %in% c('R2', 'R6') ~ 'R',
  Network_Properties$Developmental_Stage %in% c('V1', 'V6') ~ 'V',
  TRUE ~ as.character(Network_Properties$Developmental_Stage)
)

# Summarize data based on metric and Stage_Group
temp.net.summary.sp <- Network_Properties %>%
  filter(Stage_Group %in% c('R', 'V')) %>%
  group_by(metric, Stage_Group) %>%
  summarise(
    mean_value = mean(value, na.rm = TRUE),
    SEM_value = sd(value, na.rm = TRUE) / sqrt(n())
  )

##Now for V6 comparison between treatments
# Summarize data based on metric, Treatment, and Developmental_Stage
temp.net.summary.sp.trt <- Network_Properties %>%
  filter(Developmental_Stage == 'V6') %>%
  group_by(metric, Treatment) %>%
  summarise(
    mean_value = mean(value, na.rm = TRUE),
    SEM_value = sd(value, na.rm = TRUE) / sqrt(n())
  )

##Now for Pearson trends
options(scipen=999)
temp.net.pearson.summary <- Network_Properties.pearson %>%
  filter(Developmental_Stage %in% c('V6', 'R2', 'R6')) %>%
  group_by(Treatment,Cultivar, metric) %>%
  summarise(
    mean_value = mean(value, na.rm = TRUE),
    SEM_value = sd(value, na.rm = TRUE) / sqrt(n())
  )

##Overall across network types
pearson.sum <- Network_Properties.pearson %>%
  group_by(metric) %>%
  summarise(
    mean_value = round(mean(value, na.rm = TRUE), digits=5), # Rounding to 5 decimal places, modify as needed
    SEM_value = round(sd(value, na.rm = TRUE) / sqrt(n()), digits=5) # Rounding to 5 decimal places
  )


spearman.sum <- Network_Properties %>%
  group_by(metric) %>%
  summarise(
    mean_value = round(mean(value, na.rm = TRUE), digits=5), # Rounding to 5 decimal places, modify as needed
    SEM_value = round(sd(value, na.rm = TRUE) / sqrt(n()), digits=5) # Rounding to 5 decimal places
  )
```

```{r}
##Let's visualize at the effect of location on co-occurrence network structure

##North
##Create phyloseq object
spearman.north <- prune_samples(sample_data(filtered_obj)$Location == "North", filtered_obj)

spearman.north.table <- co_occurrence(spearman.north, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
spearman.north.table$p <-p.adjust(spearman.north.table$p, "fdr")
spearman.north.table <- filter(spearman.north.table,p <= 0.05)

##network_layout_ps function
spearman.north.layout <- network_layout_ps(spearman.north, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = spearman.north.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(spearman.north, co_occurrence_table = spearman.north.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = spearman.north.layout, ##change here
                      negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("spearman.north.net.tiff", width = 5, height = 5, units = "in", dpi = 900)

##Middle
spearman.middle <- prune_samples(sample_data(filtered_obj)$Location == "Middle", filtered_obj)

spearman.middle.table <- co_occurrence(spearman.middle, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
spearman.middle.table$p <-p.adjust(spearman.middle.table$p, "fdr")
spearman.middle.table <- filter(spearman.middle.table,p <= 0.05)

##network_layout_ps function
spearman.middle.layout <- network_layout_ps(spearman.middle, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = spearman.middle.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(spearman.middle, co_occurrence_table = spearman.middle.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = spearman.middle.layout, ##change here
                      negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("spearman.middle.net.tiff", width = 5, height = 5, units = "in", dpi = 900)

##South
spearman.south <- prune_samples(sample_data(filtered_obj)$Location == "South", filtered_obj)

spearman.south.table <- co_occurrence(spearman.south, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0)
##fdr correction and filtering
spearman.south.table$p <-p.adjust(spearman.south.table$p, "fdr")
spearman.south.table <- filter(spearman.south.table,p <= 0.05)

##network_layout_ps function
spearman.south.layout <- network_layout_ps(spearman.south, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = spearman.south.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(spearman.south, co_occurrence_table = spearman.south.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = spearman.south.layout, ##change here
                      negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("spearman.south.net.tiff", width = 5, height = 5, units = "in", dpi = 900)

##North Pearson- Can use 'spearman.north' phyloseq object
pearson.spearman.north.table <- co_occurrence(spearman.north, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = "pearson")
##fdr correction and filtering
pearson.spearman.north.table$p <-p.adjust(pearson.spearman.north.table$p, "fdr")
pearson.spearman.north.table <- filter(pearson.spearman.north.table,p <= 0.05)

##network_layout_ps function
pearson.spearman.north.layout <- network_layout_ps(spearman.north, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = pearson.spearman.north.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(spearman.north, co_occurrence_table = pearson.spearman.north.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = pearson.spearman.north.layout, ##change here
                      negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("pearson.spearman.north.net.tiff", width = 5, height = 5, units = "in", dpi = 900)

##Middle Pearson
pearson.spearman.middle.table <- co_occurrence(spearman.middle, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = "pearson")
##fdr correction and filtering
pearson.spearman.middle.table$p <-p.adjust(pearson.spearman.middle.table$p, "fdr")
pearson.spearman.middle.table <- filter(pearson.spearman.middle.table,p <= 0.05)

##network_layout_ps function
pearson.spearman.middle.layout <- network_layout_ps(spearman.middle, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = pearson.spearman.middle.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(spearman.middle, co_occurrence_table = pearson.spearman.middle.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = pearson.spearman.middle.layout, ##change here
                      negative_positive_colors = edge_col) + ##Must change edge color as only positive interactions are present
  theme(legend.position = 'none')

ggsave("pearson.spearman.middle.net.tiff", width = 5, height = 5, units = "in", dpi = 900)

##South Pearson
pearson.spearman.south.table <- co_occurrence(spearman.south, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = "pearson")
##fdr correction and filtering
pearson.spearman.south.table$p <-p.adjust(pearson.spearman.south.table$p, "fdr")
pearson.spearman.south.table <- filter(pearson.spearman.south.table,p <= 0.05)

##network_layout_ps function
pearson.spearman.south.layout <- network_layout_ps(spearman.south, ##change here
treatment = NULL, subset = NULL, co_occurrence_table = pearson.spearman.south.table, ##change here
algorithm = 'sphere')

##co-occurence function
co_occurrence_network(spearman.south, co_occurrence_table = pearson.spearman.south.table, ##change here
                      classification = "kingdom", node_colors = net.col,
                      cluster = FALSE, cluster_colors = 'default', layout = pearson.spearman.south.layout, ##change here
                      negative_positive_colors = edge_col.3) +
  theme(legend.position = 'none')

ggsave("pearson.spearman.south.net.tiff", width = 5, height = 5, units = "in", dpi = 900)
```

```{r}
##Now, we need to construct a global pearson network and compare it to the global spearman network
table.pearson <- co_occurrence(filtered_obj, treatment = NULL, subset = NULL, rho = 0.6, p = 0.05, cores = 0, method = "pearson")

##fdr correction and filtering
table.pearson$p <-p.adjust(table.pearson$p, "fdr")
table.pearson <- filter(table.pearson,p <= 0.05)

##network_layout_ps function
layout.pearson <- network_layout_ps(filtered_obj,
treatment = NULL, subset = NULL, co_occurrence_table = table.pearson,
algorithm = 'sphere')

##network_ps function 
global.net.pearson <- network_ps(filtered_obj, treatment = NULL, subset = NULL, co_occurrence_table = table.pearson, rho = 0.6)

##stats
net_prop.global.pearson <- net_properties(global.net.pearson)

##Viz 2
global.net.pearson.viz <- co_occurrence_network(filtered_obj, co_occurrence_table = table.pearson,
                      classification = "kingdom",  node_colors = net.col,
                      cluster = FALSE, layout = layout.pearson,
  negative_positive_colors = edge_col) +
  theme(legend.position = 'none')

tiff("global.net.pearson.tiff", width = 4, height = 4, units = "in", res = 900)
global.net.pearson.viz
dev.off()
```

```{r}
##Compare the global networks
table.pearson.1 <- table.pearson %>%
  mutate(pair = paste(X, Y, sep = "_"),
         method = "Pearson")
view(table.pearson.1)

##Let's ensure all pairs are unqiue in table.pearson.1
##Step 1: Sort the genera in each row so that they are in alphabetical order
table.pearson.1 <- table.pearson.1 %>%
  rowwise() %>%
  mutate(pair_sorted = paste(sort(c(X, Y)), collapse = "_"))

##Step 2: Filter to unique rows based on the sorted genera columns
unique_table.pearson <- table.pearson.1 %>%
  distinct(pair_sorted, .keep_all = TRUE)

##Do the same for spearman
table.sp.1 <- table.sp %>%
  mutate(pair = paste(X, Y, sep = "_"),
         method = "Spearman")
view(table.sp.1)

##Let's ensure all pairs are unqiue in table.sp.1
##Step 1: Sort the genera in each row so that they are in alphabetical order
table.sp.1 <- table.sp.1 %>%
  rowwise() %>%
  mutate(pair_sorted = paste(sort(c(X, Y)), collapse = "_"))

##Step 2: Filter to unique rows based on the sorted genera columns
unique_table.sp <- table.sp.1 %>%
  distinct(pair_sorted, .keep_all = TRUE)

##Find unique and overlapping
# Find shared instances
shared_instances <- sum(unique_table.sp$pair_sorted %in% unique_table.pearson$pair_sorted)

# Find unique instances in unique_table.sp
unique_instances_sp <- sum(!unique_table.sp$pair_sorted %in% unique_table.pearson$pair_sorted)

# Find unique instances in unique_table.pearson
unique_instances_pearson <- sum(!unique_table.pearson$pair_sorted %in% unique_table.sp$pair_sorted)

# Combine unique instances from both dataframes
unique_instances <- unique_instances_sp + unique_instances_pearson

# Output the results
print(shared_instances)  # Number of shared instances
print(unique_instances)  # Number of unique instances

##Format and Visualize
all_pairs <- unique(c(unique_table.sp$pair_sorted, unique_table.pearson$pair_sorted))

# Create a dataframe with all pairs as rownames
venn_df <- data.frame(
  Row.names = all_pairs,
  Spearman = 0,
  Pearson = 0)

# Set the values to 1 for pairs present in the respective dataframes
venn_df$Spearman[venn_df$Row.names %in% unique_table.sp$pair_sorted] <- 1
venn_df$Pearson[venn_df$Row.names %in% unique_table.pearson$pair_sorted] <- 1

rownames(venn_df) <- venn_df$Row.names
venn_df$Row.names <- NULL

net.venn <- list(Spearman = which(venn_df$Spearman ==1),
                 Pearson = which(venn_df$Pearson == 1))
ggvenn(
  net.venn, columns = c("Spearman", "Pearson"),
  stroke_size = 1, 
  fill_color = c("lightblue3", "#8c3800"),
  text_size = 6,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = TRUE) 
```

```{r}
##Friedman rank-sum tests were used to compare conditional ranking between Spearman and Pearson for each network property.
# Required library
#library(dplyr)

# Get the unique metrics
unique_metrics <- unique(correlation.comparison.2$metric)

# Empty list to hold results
friedman_results <- list()

# Loop over each unique metric
for(i in seq_along(unique_metrics)) {
  
  # Subset the data for the current metric
  current_metric_data <- correlation.comparison.2 %>%
    filter(metric == unique_metrics[i])
  
  # Run the Friedman test
  friedman_test <- friedman.test(value ~ method | Condition, data = current_metric_data)
  
  # Store the results
  friedman_results[[unique_metrics[i]]] <- friedman_test
  
}

# Print the results
friedman_results

##Add to dataframe
correlation.comparison.2 <- correlation.comparison.2 %>%
  mutate(friedman.p = case_when(
    metric == "Centralization degree" ~ "p = 6.334e-05",
    metric == "Cluster count" ~ "p = 0.593",
    metric == "Connectance" ~ "p = 0.01242",
    metric == "Edge count" ~ "p = 6.334e-05",
    metric == "Giant component size" ~ "p = 6.334e-05",
    metric == "Hub count" ~ "p = 6.334e-05",
    metric == "KS stat" ~ "p = 0.006656",
    metric == "Mean degree" ~ "p = 6.334e-05",
    metric == "Modularity" ~ "p = 0.6171",
    metric == "Node count" ~ "p = 6.334e-05",
    TRUE ~ NA_character_  # for all other cases, fill with NA
  ))

view(correlation.comparison.2)

##Let's remake the figure plotting these p-values
ggplot(correlation.comparison.2, aes(x = method, y = value, fill = method)) +
  geom_line(aes(group = Condition), size = 0.7, color = "black") +
  geom_point(shape = 21, size = 6, color = "black", alpha = 0.5) +  # shape = 21 is a circle that can have both fill and color specified
  facet_wrap(~ metric, scales = "free", nrow = 2) +
  theme_bw() +
  theme(
    legend.position = "top",
    axis.title = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(color = "black", face = "bold", size = 12),
    strip.text = element_text(size = 12, face = "bold", color = "black"),
    legend.text = element_text(size = 10),
    legend.title = element_blank(),
    axis.ticks.x = element_blank()) +
  scale_fill_manual(values = c("#8c3800", "lightblue3")) +
  geom_text(
    aes(label = format(friedman.p, scientific = TRUE, digits = 2)),
    #aes(label = friedman.p),
    x = Inf,
    y = -Inf,
    hjust = 1.5,
    vjust = -1,
    color = "black",
    size = 4)

ggsave("correlation.comparison.supp.png", width = 12, height = 6, units = "in", dpi = 900)

##Different orientation
ggplot(correlation.comparison.2, aes(x = method, y = value, color = method)) +
 geom_line(aes(group = Condition), size = 0.7, color = "black") +
  geom_point(size = 4, alpha = 0.5) +
  #geom_point(size = 4) +
  facet_wrap(~ metric, scales = "free", ncol = 2) +
  theme_bw() +
  theme(
    legend.position = "top",
    axis.title = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(color = "black", face = "bold", size = 12),
    strip.text = element_text(size = 12, face = "bold", color = "black"),
    legend.text = element_text(size = 10),
    legend.title = element_blank(),
    axis.ticks.x = element_blank()) +
  scale_color_manual(values = c("#8c3800", "lightblue3")) +
  geom_text(
    aes(label = friedman.p),
    x = Inf,
    y = -Inf,
    hjust = 1.1,
    vjust = -9,
    color = "black",
    size = 4)

ggsave("correlation.comparison.6.png", width = 7, height = 10, units = "in", dpi = 900)
```

```{r}
##Correlations between additional metrics and density
##Can use correlation.comparison
corr.ass <- correlation.comparison[, -c(1:3)]

##wide format
wider_data <- corr.ass %>%
  spread(key = metric, value = value)

##Back to long
longer_data <- wider_data %>%
  pivot_longer(cols = c(3:5, 7:11), names_to = "metric", values_to = "value")

##get node count and edge count together
stacked_data <- longer_data %>%
  pivot_longer(cols = 3:4, names_to = "density", values_to = "value.density")

##plot
ggplot(stacked_data, aes(x = value.density, y = value, fill = method)) +
  geom_point(shape = 21, size = 6, color = "black", alpha = 0.5) +
  #facet_grid(density ~ metric, scales = "free") +
  ggh4x::facet_grid2(density ~ metric, scales = "free", independent = "all") +
  theme_bw() +
  theme(
    legend.position = "top",
    axis.title = element_blank(),
    #axis.text.x = element_blank(),
    axis.text = element_text(color = "black", face = "bold", size = 10),
    strip.text = element_text(size = 12, face = "bold", color = "black"),
    legend.text = element_text(size = 10),
    legend.title = element_blank(),
    axis.ticks.x = element_blank()) +
  scale_fill_manual(values = c("#8c3800", "lightblue3"))

calculate_correlation <- function(x, y) {
  result <- cor.test(x, y, method = "spearman")
  return(list(r = result$estimate, p = result$p.value))
}

# Apply the function to each group and add results to the data frame
stacked_data <- stacked_data %>%
  group_by(method,metric,density) %>%
  mutate(correlation_result = list(calculate_correlation(value, value.density))) %>%
  mutate(r = map_dbl(correlation_result, "r"),
         p_value = map_dbl(correlation_result, "p")) %>%
  ungroup() %>%
  select(-correlation_result)

# Summarize the data to get unique values for each combination of Condition, method, metric, and density
summary_data <- stacked_data %>%
  group_by(Condition, method, metric, density) %>%
  summarise(r = first(r), p_value = first(p_value), .groups = 'drop')

# Plot
ggplot(stacked_data, aes(x = value.density, y = value, fill = method)) +
  geom_point(shape = 21, size = 7, color = "black", alpha = 0.5) +
  ggh4x::facet_grid2(metric~density, scales = "free", independent = "all") +
  theme_bw() +
  theme(
    legend.position = "none",
    axis.title = element_blank(),
    axis.text = element_text(color = "black", face = "bold", size = 14),
    strip.text = element_text(size = 14, face = "bold", color = "black"),
    legend.text = element_text(size = 10),
    legend.title = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  scale_fill_manual(values = c("#8c3800", "lightblue3")) +
  geom_text(data = filter(summary_data, method == "Pearson"), 
            aes(x = Inf, y = -Inf, 
                label = paste("Pearson: r =", round(r, 2), 
                              ", p =", format(p_value, digits = 2, scientific = TRUE))),
            hjust = 1.1, vjust = -1, size = 5) +
  geom_text(data = filter(summary_data, method == "Spearman"), 
            aes(x = Inf, y = -Inf, 
                label = paste("Spearman: r =", round(r, 2), 
                              ", p =", format(p_value, digits = 2, scientific = TRUE))),
            hjust = 1.1, vjust = -2.5, size = 5)

ggsave("metric.comparison.2.png", width = 13, height = 20, units = "in", dpi = 900)

##Play with visualization
ggplot(stacked_data, aes(x = log10(value.density + 1), y = log10(value + 1), fill = method)) +
  geom_point(shape = 21, size = 7, color = "black", alpha = 0.5) +
  ggh4x::facet_grid2(metric ~ density, scales = "free", independent = "all") +
  theme_bw() +
  theme(
    legend.position = "none",
    axis.title = element_blank(),
    axis.text = element_text(color = "black", face = "bold", size = 14),
    strip.text.y = element_text(size = 14, face = "bold", color = "black"),
    strip.text.x = element_text(size = 18, face = "bold", color = "black"), 
    legend.text = element_text(size = 10),
    legend.title = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  scale_fill_manual(values = c("#8c3800", "lightblue3")) +
  #scale_x_continuous(labels = function(x) 10^x - 1) +
  #scale_y_continuous(labels = function(y) 10^y - 1) +
  geom_text(data = filter(summary_data, method == "Pearson"), 
            aes(x = Inf, y = -Inf, 
                label = paste("Pearson: r =", round(r, 2), 
                              ", p =", format(p_value, digits = 2, scientific = TRUE))),
            hjust = 1.1, vjust = -1, size = 5) +
  geom_text(data = filter(summary_data, method == "Spearman"), 
            aes(x = Inf, y = -Inf, 
                label = paste("Spearman: r =", round(r, 2), 
                              ", p =", format(p_value, digits = 2, scientific = TRUE))),
            hjust = 1.1, vjust = -2.5, size = 5)

ggsave("metric.comparison.3.png", width = 13, height = 20, units = "in", dpi = 900)


##Let's observe statistically significant correlations
significant_summary <- stacked_data %>%
  group_by(method, metric, density) %>%
  summarise(
    mean_r = mean(r, na.rm = TRUE),
    mean_p_value = mean(p_value, na.rm = TRUE)
  ) %>%
  filter(mean_p_value < 0.05)
```

```{r}
##Create correlation comparison table
correlation.table <- correlation.comparison.2 %>%
  mutate(
    network = paste0(
      Developmental_Stage, '-',
      ifelse(Treatment == 'Biostimulant', 'Bst', 'Ctrl'), '-',
      ifelse(Cultivar == 'CZ4979X', '79X', '10X')
    )
  )

##Pivot wide
correlation.table.wide <- correlation.table %>%
  pivot_wider(names_from = metric, 
              values_from = value, 
              values_fn = list(value = mean), # you can use other summarizing functions too
              id_cols = c(network, method))

##Reorder
# Define the order for networks
network_order <- c("V1-Bst-79X", "V6-Bst-79X", "R3-Bst-79X", "R6-Bst-79X", "V1-Ctrl-79X",
                   "V6-Ctrl-79X", "R3-Ctrl-79X", "R6-Ctrl-79X", "V1-Bst-10X", "V6-Bst-10X",
                   "R3-Bst-10X", "R6-Bst-10X", "V1-Ctrl-10X", "V6-Ctrl-10X", "R3-Ctrl-10X", "R6-Ctrl-10X")

# Define the order for methods
method_order <- c("Spearman", "Pearson")

# Use factor to arrange dataframe by specific order
correlation.table.wide <- correlation.table.wide %>%
  mutate(network = factor(network, levels = network_order),
         method = factor(method, levels = method_order)) %>%
  arrange(method, network)

# Set network and method back to character to avoid factor level issues in future operations
correlation.table.wide <- correlation.table.wide %>%
  mutate(network = as.character(network),
         method = as.character(method))
```

```{r}
##Now we need to make a heatmap showing all members of condition-specific networks
##We will specify if they are a hub or member for both pearson and spearman networks
##Let's obtain what we used for spearman heatmap- network.heatmap

network.heatmap.pearson <- rbind(R6.79X.trt.heat.pearson, R3.79X.trt.heat.pearson, V6.79X.trt.heat.pearson, V1.79X.trt.heat.pearson,
                         R6.10X.trt.heat.pearson, R3.10X.trt.heat.pearson, V6.10X.trt.heat.pearson, V1.10X.trt.heat.pearson,
                         R6.79X.ctrl.heat.pearson, R3.79X.ctrl.heat.pearson, V6.79X.ctrl.heat.pearson, V1.79X.ctrl.heat.pearson,
                         R6.10X.ctrl.heat.pearson, R3.10X.ctrl.heat.pearson, V6.10X.ctrl.heat.pearson, V1.10X.ctrl.heat.pearson)

##Modify for heatmap
##Add method
network.heatmap.pearson <- network.heatmap.pearson %>%
  mutate(method = "Pearson")

##Get # of times each genus is a hub or member
network.heatmap.pearson.1 <- network.heatmap.pearson %>%
  mutate(category = ifelse(vector >= 0.2, "Pearson hub", "Pearson member")) %>%
  group_by(genus, category) %>%
  summarise(count = n()) %>%
  pivot_wider(names_from = category, values_from = count, values_fill = 0)

##Do the same for spearman
network.heatmap.sp.1 <- network.heatmap %>%
  mutate(category = ifelse(vector >= 0.2, "Spearman hub", "Spearman member")) %>%
  group_by(genus, category) %>%
  summarise(count = n()) %>%
  pivot_wider(names_from = category, values_from = count, values_fill = 0)

##Merge these two
merged_network_heatmap <- full_join(network.heatmap.sp.1, network.heatmap.pearson.1, by = "genus")

##Now, get relative abundance for all members
##we will use the dataframes created for phona analysis
##merged_phona_prok_genus
##merged_phona_euk_genus
prok.ra.net <- merge(merged_network_heatmap, merged_phona_prok_genus, by = c('genus'))
euk.ra.net <- merge(merged_network_heatmap, merged_phona_euk_genus, by = c('genus'))
##Combine
ra.net <- rbind(prok.ra.net, euk.ra.net)
rownames(ra.net) <- ra.net$genus
ra.net.1 <- ra.net[,-(1:10)]
##Transpose
ra.net.1.t <- t(ra.net.1)

##We'll keep the above code in case we want to come back and use relative abundance.
##In the meantime, let's use pearson hub score, since all genera from spearman are also
##present in pearson; therefore, we will use network.heatmap.pearson

##Remove unneeded columns
combined.network.heatmap <- network.heatmap.pearson[,-c(2:4, 7:8)]

##Reshape
combined.network.heatmap <- combined.network.heatmap %>%
  spread(key = Condition, value = vector)
##Add rownames
rownames(combined.network.heatmap) <- combined.network.heatmap$genus
combined.network.heatmap$genus <- NULL
combined.network.heatmap.2 <- combined.network.heatmap[, net.col.order] ##net.col.order from above
combined.network.heatmap.t <- t(combined.network.heatmap.2)

##Now, let's add annotations
##Add bottom annotation
# Create the Domain column based on the Kingdom column
bottom.anno <- ra.net
bottom.anno$Domain <- ifelse(bottom.anno$kingdom == "Bacteria", "Prokaryote", "Eukaryote")
##Remove all columns except Domain and reorder rows according to combined.network.heatmap rownames
bottom.anno <- bottom.anno[, "Domain", drop = FALSE]
rownames_order <- match(rownames(combined.network.heatmap), rownames(bottom.anno))
bottom.anno <- bottom.anno[rownames_order, , drop = FALSE]
##Create
bottom.an <- HeatmapAnnotation(df = bottom.anno,
  which = 'col',
  col = colours.phona, ##From other heatmaps
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), simple_anno_size = unit(0.2, "cm"), show_legend = FALSE, show_annotation_name = FALSE)

##Top annotation
top.anno <- merged_network_heatmap
top.anno <- as.data.frame(top.anno)
rownames(top.anno) <- top.anno$genus
top.anno$genus <- NULL
rownames_order.2 <- match(rownames(combined.network.heatmap), rownames(top.anno))
top.anno <- top.anno[rownames_order.2, , drop = FALSE]
top.anno[is.na(top.anno)] <- 0

##Custom legend
anno_colors <- c("#786060","#a8a890","#c07848","#c0c0c0")
custom_legend <- Legend(at = c(1, 2, 3, 4), 
                        labels = c("Label 1", "Label 2", "Label 3", "Label 4"), 
                        title = "Custom Legend", 
                        type = "points", 
                        pch = 15, 
                        legend_gp = gpar(col = anno_colors, fill = anno_colors))

top.anno.1 <- HeatmapAnnotation("# nodes" = 
                    anno_barplot(top.anno, 
                                 bar_width = 1.2,  # Adjust this value to change the width of the bars
                                 gp = gpar(fill = c("#786060","#a8a890","#c07848","#c0c0c0"), lwd = 0), 
                                 axis = FALSE
                    ), 
                    which = "column", 
                    show_annotation_name = FALSE, 
                    show_legend = TRUE)

##Top annotation 2- just total number of membership across both network types
# Create the new columns 'Hub' and 'Member'
top.anno$Hub <- top.anno$`Spearman hub` + top.anno$`Pearson hub`
top.anno$Member <- top.anno$`Spearman member` + top.anno$`Pearson member`

# Create top.anno.2 without the old columns
top.anno.2 <- top.anno[, !(names(top.anno) %in% c("Spearman hub", "Spearman member", "Pearson hub", "Pearson member"))]

top.anno.3 <- HeatmapAnnotation("# nodes" = 
                    anno_barplot(top.anno.2, 
                                 bar_width = 1.2,  # Adjust this value to change the width of the bars
                                 gp = gpar(fill = c("#8c3800", "lightblue3"), lwd = 0), 
                                 axis = FALSE
                    ), 
                    which = "column", 
                    show_annotation_name = FALSE, 
                    show_legend = TRUE)


##Legend
lgd_list_net <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12),title = "Pearson Hub Score",
        border = "black", lwd = 1,  
        title_position = "topcenter",
        at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0), legend_width = unit(10, "cm"),
        direction = "horizontal",
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"),
        labels = c("","Hub node", "", "", "", ""))

##Right annotation 1
right.anno <- network.heatmap.pearson
##remove unneeded columns
right.anno <- right.anno[ , !(names(right.anno) %in% c("genus", "vector", "value", "method"))]
##Reorder Condition
right.anno$Condition <- factor(right.anno$Condition, levels = net.col.order)
right.anno <- right.anno[order(right.anno$Condition),]
##Filter to unique values in column Condition
right.anno <- right.anno[!duplicated(right.anno$Condition), ]
##Rename column Developmental_Stage to Growth Stage
right.anno <- right.anno %>%
  rename("Growth Stage" = Developmental_Stage)
##Add rownames
rownames(right.anno) <- right.anno$Condition
right.anno$Condition <- NULL
right.anno$`Growth Stage`[right.anno$`Growth Stage` == "R3"] <- "R2" ##Change to R2

right.anno$`Growth Stage` <- factor(right.anno$`Growth Stage`, levels = c("V1", "V6", "R2", "R6"))
##Reorder the columns
right.anno <- right.anno[, c("Treatment", "Cultivar", "Growth Stage")]

##Create annotation
right.annotation <- rowAnnotation(df = right.anno,
  col = net.colours, ##from previous heatmap
  annotation_width = unit(c(1, 2), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"))

##Create annotation for the number of Prokaryotes and Eukaryotes in each pearson network
##merge ra.net and bottom.anno by rownames
right.annotatio.2 <-merge(combined.network.heatmap, bottom.anno, by = "row.names")
##Pivot longer
right.annotatio.2 <- right.annotatio.2 %>%
  pivot_longer(cols = 2:17, names_to = "Condition", values_to = "vector")

##Calculate number of prokaryote and eukaryote for each condition
right.annotatio.2 <- right.annotatio.2 %>%
  filter(!is.na(vector)) %>%
  group_by(Condition, Domain) %>%
  summarise(n = n(), .groups = "drop") %>%
  pivot_wider(names_from = Domain, values_from = n, values_fill = 0)
##Reorder conditions
right.annotatio.2$Condition <- factor(right.annotatio.2$Condition, levels = net.col.order)
right.annotatio.2 <- right.annotatio.2[order(right.annotatio.2$Condition),]
right.annotatio.2 <- as.data.frame(right.annotatio.2)
rownames(right.annotatio.2) <- right.annotatio.2$Condition
right.annotatio.2$Condition <- NULL
##Create annotation
right.annotatio.2 <- right.annotatio.2[, c(2, 1)]


right.annotation.2 = HeatmapAnnotation("# unique obs"= anno_barplot(right.annotatio.2, gp = gpar(fill = c("#a8c0a8","#a890a8"))), 
                               which = c("row"), show_annotation_name = FALSE)


merged.net.heat.1 <- Heatmap(
  combined.network.heatmap.t,
  cluster_columns = FALSE,
  cluster_rows = TRUE,
  na_col = "gray57",
  border = TRUE,
  #rect_gp = gpar(col = "black", lwd = 0.5),
  col = my_palette,
  show_column_names = FALSE,
  show_row_names = FALSE,
  heatmap_legend_param = lgd_list_net, 
  right_annotation = right.annotation, ##Had to change after the fact
  #right_annotation = right.annotation.2,
  bottom_annotation = bottom.an,
  top_annotation = top.anno.3,
  column_split = bottom.anno$Domain,
  row_gap = unit(c(2), "mm"),
  row_title = NULL,
  column_title = NULL,
  column_names_rot = 45,
  row_names_gp = gpar(fontsize = 10))

merged.net.heat.2<- merged.net.heat.1 + right.annotation.2
 
merged.net.heat.3 <- draw(merged.net.heat.2, heatmap_legend_side = "bottom", legend_grouping = "original")

tiff("merged.net.heat.tiff", width = 10, height = 4, units = "in", res = 900)
merged.net.heat.3
dev.off() 
```

```{r}
##Create Venn diagram of overlapping and shared members between
##correlation methods for condition-specific networks 
##Will use merged_network_heatmap

# Replace NA values with 0 in the dataframe
merged_network_heatmap[is.na(merged_network_heatmap)] <- 0

# Helper functions to determine presence in Spearman and Pearson
is_present <- function(hub, member) {
  if(hub > 0 | member > 0) return(1) else return(0)
}

# Create condition.venn dataframe
condition.venn <- merged_network_heatmap %>%
  rowwise() %>%
  mutate(
    Spearman = is_present(`Spearman hub`, `Spearman member`),
    Pearson = is_present(`Pearson hub`, `Pearson member`)
  ) %>%
  select(genus, Spearman, Pearson)


##Now for ggvenn
condition.venn$genus <- NULL

condition.venn.2 <- list(Spearman = which(condition.venn$Spearman ==1),
                 Pearson = which(condition.venn$Pearson == 1))

ggvenn(
  condition.venn.2, columns = c("Spearman", "Pearson"),
  stroke_size = 1, 
  fill_color = c("lightblue3", "#8c3800"),
  text_size = 4,
  set_name_size = 6, 
  fill_alpha = 0.5,
  auto_scale = TRUE) 

##Now, we need a venn diagram of associations
# List of all the data frame names
df_names <- c('V1.10X.ctrl.pearson.table', 'V6.10X.ctrl.pearson.table', 'R3.10X.ctrl.pearson.table', 
              'R6.10X.ctrl.pearson.table', 'V1.79X.ctrl.pearson.table', 'V6.79X.ctrl.pearson.table', 
              'R3.79X.ctrl.pearson.table', 'R6.79X.ctrl.pearson.table', 'V1.10X.trt.pearson.table', 
              'V6.10X.trt.pearson.table', 'R3.10X.trt.pearson.table', 'R6.10X.trt.pearson.table', 
              'V1.79X.trt.pearson.table', 'V6.79X.trt.pearson.table', 'R3.79X.trt.pearson.table', 
              'R6.79X.trt.pearson.table')

# Function to modify and add required columns
modify_df <- function(df_name) {
  df <- get(df_name)  # Get the dataframe from its name
  
  # Extract the name for the 'name' column
  df_name_parts <- unlist(strsplit(df_name, "\\.")) 
  df$name <- paste(df_name_parts[1], df_name_parts[2], df_name_parts[3], sep = ".")
  
  # Add the 'type' column
  df$type <- "Pearson"
  
  return(df)
}

# Apply the function to all data frames and rbind them
pearson.associations <- do.call(rbind, lapply(df_names, modify_df))

##Now for Spearman
# Modify the df_names list
df_names_spearman <- gsub(".pearson", "", df_names)

# Function to modify and add required columns for Spearman
modify_df_spearman <- function(df_name) {
  df <- get(df_name)  # Get the dataframe from its name
  
  # Extract the name for the 'name' column
  df_name_parts <- unlist(strsplit(df_name, "\\.")) 
  df$name <- paste(df_name_parts[1], df_name_parts[2], df_name_parts[3], sep = ".")
  
  # Add the 'type' column
  df$type <- "Spearman"
  
  return(df)
}

# Apply the function to all Spearman data frames and rbind them
spearman.associations <- do.call(rbind, lapply(df_names_spearman, modify_df_spearman))

# Now, rbind pearson.associations and spearman.associations
all_associations <- rbind(pearson.associations, spearman.associations)


# Function to combine and order values of X and Y
combine_and_order <- function(x, y) {
  sorted_vals <- sort(c(x, y))
  return(paste(sorted_vals[1], sorted_vals[2], sep = "_"))
}

all_associations$XY <- mapply(combine_and_order, all_associations$X, all_associations$Y)

##Venn diagram
sets_list.association <- all_associations %>%
  group_by(type) %>%
  summarise(XY = list(unique(XY))) %>%
  deframe()

 ggvenn(
  sets_list.association, columns = c("Spearman", "Pearson"),
  stroke_size = 1, 
  fill_color = c("lightblue3","#8c3800"),
  text_size = 4,
  set_name_size = 8, 
  fill_alpha = 0.5,
  auto_scale = TRUE) 
```

```{r}
##Create full list of co-occurrences for phona analysis- based on global networks
all.pairs <- as.data.frame(all_pairs)

##Let's add Pearson coefficients and qvalues for those found in Pearson and shared
##Add Spearman coefficients and qvalues for those found exclusively in Spearman
##First, those found in Pearson and shared
all.pairs.2 <- merge(all.pairs, unique_table.pearson, 
                   by.x = "all_pairs", 
                   by.y = "pair_sorted",
                   all = FALSE)

##Get Spearman pairs not present in all.pairs.2
unique_table.sp.2 <- unique_table.sp %>%
  dplyr::filter(!(pair_sorted %in% all.pairs.2$all_pairs))

##Restructure to facilitate rbind
unique_table.sp.2 <- unique_table.sp.2 %>%
  dplyr::rename(all_pairs = pair_sorted) %>%
  dplyr::select(all_pairs, Treatment, X, Y, rho, p, pair, method)

##combine
all.pairs.3 <- rbind(all.pairs.2, unique_table.sp.2)
all.pairs.3 <- as.data.frame(all.pairs.3)
```

```{r}
##Now, let's look at edaphic measurements
##First, rename Field_ID. For simplicity, "Field_ID", "Field_ID.2", and "Field_ID.3" are provided in the .RData file.
#Field_ID.3 <- Field_ID
#Field_ID.3 <- Field_ID.3 %>%
  #rename(`Soil pH` = Soil_pH, `Buffer pH` = Buffer_pH, `K/Mg` = K_Mg, `Ca/Mg` = Ca_Mg)

Field_ID.3.long <- Field_ID.3 %>%
  gather(key = metric, value = value, 8:31)

##Change R3 to R2, the correct sampling stage
Field_ID.3.long$Developmental_Stage <- as.character(Field_ID.3.long$Developmental_Stage)
Field_ID.3.long$Developmental_Stage[Field_ID.3.long$Developmental_Stage == "R3"] <- "R2"
Field_ID.3.long$Developmental_Stage <- as.factor(Field_ID.3.long$Developmental_Stage)

##Visualize
Field_ID.3.long$Developmental_Stage <- factor(Field_ID.3.long$Developmental_Stage, levels = c("V1", "V6", "R2", "R6"))

ggplot(Field_ID.3.long, aes(x = Developmental_Stage, y = value, fill = interaction(Treatment, Cultivar))) + 
  geom_col(aes(y = 0), alpha = 0, width = 1) +  # Temporary invisible layer
  geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="V6") -0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="V6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
 geom_rect(aes(xmin = which(levels(as.factor(Developmental_Stage))=="R6")-0.5,
                xmax = which(levels(as.factor(Developmental_Stage))=="R6") +0.5,
                ymin=-Inf,
                ymax=Inf), alpha = 0.5, fill = "lightgray") +
  geom_boxplot(alpha = 0.5) +
  geom_dotplot(binaxis='y', alpha = 0.8, stackdir='center', position=position_dodge(0.75)) +
  facet_wrap(~metric, scales = "free", nrow = 4) +
  theme_bw() +
  scale_fill_manual(values=c("lightblue3", "antiquewhite", "#8c3800", "#c0c0c0")) +
  theme() +
  theme(axis.title = element_blank(), axis.text.y = element_text(size = 12, color = "black"), axis.ticks = element_blank(), 
        strip.text = element_text(size = 14, face = "bold"), axis.text.x = element_text(size = 12, color = "black", face = "bold"), 
        legend.title = element_text(size = 14, face = "bold"), legend.text = element_text(size = 12), legend.position = 'top') +
  labs(fill = "Treatment + Cultivar") 

ggsave("Edaphic.2.tiff", width = 14, height = 6, units = "in", dpi = 900)
```

```{r}
##Let's construct mixed models for edaphic measures, starting with NO3
##First, remove rows that do not correspond to samples
#Field_ID <- Field_ID[1:(nrow(Field_ID) - 2), ]

##Add Location to the dataframe
#Field_ID$Location <- ifelse(Field_ID$Block %in% c("A", "B"), "North",
                                  #ifelse(Field_ID$Block %in% c("C", "D", "E", "F"), "Middle", "South"))

#Field_ID$Location <- factor(Field_ID$Location)

#Field_ID$Treatment <- gsub("Prebiotic", "Biostimulant", Field_ID$Treatment)

##Get just timepoints succeeding prebiotic application
#Field_ID.2 <- Field_ID %>%
  #filter(Developmental_Stage != "V1")

###Checking the normality using Shapiro-Wilk test###
shapiro_result.NO3 <- shapiro.test(Field_ID.2$NO3) #W = 0.90371, p-value = 0.0008303

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.NO3$p.value < 0.05) {
distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$NO3, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  
  
###Double check with qqp###

NO3 <- Field_ID.2$NO3 + 1 

gamma.no3 <- fitdistr(NO3, "gamma")
qqp(NO3, "gamma", shape = gamma.no3$estimate[[1]], rate = gamma.no3$estimate[[2]])

qqp(NO3, "lnorm")
qqp(NO3, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with lnorm distribution
full_model.no3 <- glmmTMB(NO3 ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                          data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.no3 <- dredge(full_model.no3, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.no3)

##Select the best model based on AIC
best_model.no3 <- get.models(models.no3, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.no3) ##Developmental Stage is Significant

###Marginalize the coefficients due to the use of a nonlinear link function###
no3.margins <- as.data.frame(marginal_effects(best_model.no3, type = "response"))
mean_est.no3 <- c(mean(no3.margins$dydx_CultivarCZ4979X), mean(no3.margins$dydx_Developmental_StageR6), mean(no3.margins$dydx_Developmental_StageV6),
                  mean(no3.margins$dydx_TreatmentControl))
print(mean_est.no3)

no3.margins.2 <- no3.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
no3.margins.2$mean_est <- c(0.7986249, 3.7058012, 4.2220122, 0.4750648)

no3.margins.2$metric <- 'NO3'

##Print model output
tab_model(best_model.no3)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.no3.1.cult <- powerSim(best_model.no3, fixed("Cultivar"))
power.sim.no3.1.cult

power.sim.no3.1.trt <- powerSim(best_model.no3, fixed("Treatment"))
power.sim.no3.1.trt 

power.sim.no3.1.ds <- powerSim(best_model.no3, fixed("Developmental_Stage"))
power.sim.no3.1.ds

##Decompose R2 to get contribution of each fixed effect
no3.r2 <- glmm.hp(best_model.no3, type = "adjR2", commonality = FALSE)

print(no3.r2)
plot.glmmhp(no3.r2)

no3.margins.2$delta <- c(4.42, 93.81, 93.81, 1.77) ##Must put for developmental stage twice

##Assess model performance
standardized.resid.no3 <- resid(best_model.no3, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.no3 <- sum(resid(best_model.no3, type = "pearson")^2)
1 - pchisq(dat.resid.no3, df.residual(best_model.no3))

# Assessing Deviance (G2)
deviance_value.no3 <- 1 - pchisq(as.numeric(-2 * logLik(best_model.no3)), df.residual(best_model.no3))

# Simulating datasets
dat.sim.no3 <- simulate(best_model.no3, nsim = 250)

# Empirical cumulative density function calculations
resid.list.no3 <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.no3)) {
    e.no3 <- ecdf(dat.sim.no3[[i]] + runif(length(dat.sim.no3[[i]]), -0.5, 0.5))
    resid.list.no3[[i]] <- e.no3(resid(best_model.no3) + runif(length(resid(best_model.no3)), -0.5, 0.5))
    plot(e.no3, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.no3 <- do.call(c, resid.list.no3)

# Quantile-quantile plot
qqnorm(all.resid.no3, main = "QQ Plot")
qqline(all.resid.no3, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.no3), standardized.resid.no3, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Format baseline data
#Field_ID.3 <- Field_ID %>%
  #filter(Developmental_Stage == "V1")

##Baseline NO3
shapiro_result.base.no3 <- shapiro.test(Field_ID.3$NO3) #W = 0.89653, p-value = 0.07074; almost significant

###Double check with qqp###
base.no3 <- Field_ID.3$NO3 + 1

nbinom.base.no3 <- fitdistr(base.no3, "Negative Binomial")
qqp(base.no3, "nbinom", size = nbinom.base.no3$estimate[[1]], mu = nbinom.base.no3$estimate[[2]])

gamma.base.no3 <- fitdistr(base.no3, "gamma")
qqp(base.no3, "gamma", shape = gamma.base.no3$estimate[[1]], rate = gamma.base.no3$estimate[[2]])

qqp(base.no3, "lnorm")
qqp(base.no3, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.no3 <- glmmTMB(NO3 ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.no3 <- dredge(full_model.base.no3, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.no3)

##Select the best model based on AIC
best_model.base.no3 <- get.models(models.base.no3, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.no3) ##Treatment signif

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##P
###Checking the normality using Shapiro-Wilk test###
shapiro_result.P <- shapiro.test(Field_ID.2$P) #W = 0.97509, p-value = 0.3942

###Double check with qqp###

P <- Field_ID.2$P + 1 

gamma.p <- fitdistr(P, "gamma")
qqp(P, "gamma", shape = gamma.p$estimate[[1]], rate = gamma.p$estimate[[2]])

qqp(P, "lnorm")
qqp(P, "norm")


###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.p <- glmmTMB(P ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.p <- dredge(full_model.p, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.p)

##Select the best model based on AIC
best_model.p <- get.models(models.p, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.p) ##Developmental Stage, Treatment, and Cultivar are Significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
p.margins <- as.data.frame(marginal_effects(best_model.p, type = "response"))
mean_est.p <- c(mean(p.margins$dydx_CultivarCZ4979X), mean(p.margins$dydx_Developmental_StageR6), mean(p.margins$dydx_Developmental_StageV6),
                  mean(p.margins$dydx_TreatmentControl))
print(mean_est.p)

p.margins.2 <- p.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
p.margins.2$mean_est <- c(-3.874999, -9.312499,  2.187507,  5.624995)

p.margins.2$metric <- 'P'

##Print model output
tab_model(best_model.p)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.p.1.cult <- powerSim(best_model.p, fixed("Cultivar"))
power.sim.p.1.cult

power.sim.p.1.trt <- powerSim(best_model.p, fixed("Treatment"))
power.sim.p.1.trt

power.sim.p.1.ds <- powerSim(best_model.p, fixed("Developmental_Stage"))
power.sim.p.1.ds

##Decompose R2 to get contribution of each fixed effect
p.r2 <- glmm.hp(best_model.p, type = "adjR2", commonality = FALSE)

print(p.r2)
plot.glmmhp(p.r2)

p.margins.2$delta <- c(10.27, 68.06, 68.06, 21.67) ##Must put for developmental stage twice

##Assess model performance
standardized.resid.p <- resid(best_model.p, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.p <- sum(resid(best_model.p, type = "pearson")^2)
1 - pchisq(dat.resid.p, df.residual(best_model.p))

# Assessing Deviance (G2)
deviance_value.p <- 1 - pchisq(as.numeric(-2 * logLik(best_model.p)), df.residual(best_model.p))

# Simulating datasets
dat.sim.p <- simulate(best_model.p, nsim = 250)

# Empirical cumulative density function calculations
resid.list.p <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.p)) {
    e.p <- ecdf(dat.sim.p[[i]] + runif(length(dat.sim.p[[i]]), -0.5, 0.5))
    resid.list.p[[i]] <- e.p(resid(best_model.p) + runif(length(resid(best_model.p)), -0.5, 0.5))
    plot(e.p, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.p <- do.call(c, resid.list.p)

# Quantile-quantile plot
qqnorm(all.resid.p, main = "QQ Plot")
qqline(all.resid.p, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.p), standardized.resid.p, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline P
shapiro_result.base.p <- shapiro.test(Field_ID.3$P) #W = 0.95967, p-value = 0.6558

###Double check with qqp###
base.p <- Field_ID.3$P + 1

nbinom.base.p <- fitdistr(base.p, "Negative Binomial")
qqp(base.p, "nbinom", size = nbinom.base.p$estimate[[1]], mu = nbinom.base.p$estimate[[2]])

gamma.base.p <- fitdistr(base.p, "gamma")
qqp(base.p, "gamma", shape = gamma.base.p$estimate[[1]], rate = gamma.base.p$estimate[[2]])

qqp(base.p, "lnorm")
qqp(base.p, "norm")

###Will use a normal distribution
##Full model
full_model.base.p <- glmmTMB(P ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.p <- dredge(full_model.base.p, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.p)

##Select the best model based on AIC
best_model.base.p <- get.models(models.base.p, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.p)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##K
###Checking the normality using Shapiro-Wilk test###
shapiro_result.K <- shapiro.test(Field_ID.2$K) #W = 0.96014, p-value = 0.1023

###Double check with qqp###

K <- Field_ID.2$K + 1 

gamma.k <- fitdistr(K, "gamma")
qqp(K, "gamma", shape = gamma.k$estimate[[1]], rate = gamma.k$estimate[[2]])

qqp(K, "lnorm")
qqp(K, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.k <- glmmTMB(K ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.k <- dredge(full_model.k, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.k)

##Select the best model based on AIC
best_model.k <- get.models(models.k, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.k) ##Developmental Stage, Treatment, Cultivar, and some interactions are Significant


###Get estimates from model output
k.margins <- data.frame(
  Variable = c("dydx_CultivarCZ4979X", "dydx_Developmental_StageR6", "dydx_Developmental_StageV6",
               "dydx_TreatmentControl", "CultivarCZ4979X:Developmental_StageR6",
               "CultivarCZ4979X:Developmental_StageV6", "CultivarCZ4979X:TreatmentControl"),
  mean_est = c(26.208, -5.625, 42.500, 15.000, -11.875, 30.000, -21.167))

k.margins$metric <- 'K'

##Print model output
tab_model(best_model.k)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.k.1.cult <- powerSim(best_model.k, fixed("Cultivar"))
power.sim.k.1.cult

power.sim.k.1.trt <- powerSim(best_model.k, fixed("Treatment"))
power.sim.k.1.trt

power.sim.k.1.ds <- powerSim(best_model.k, fixed("Developmental_Stage"))
power.sim.k.1.ds

##Decompose R2 to get contribution of each fixed effect- come back to this since we have interactions
##First, must model interactions as fixed effects as recommended by glmm.hp authors

##Create columns specifying interactions
# Assuming 'Field_ID.2' is your dataframe
Field_ID.2$Treatment_Cultivar <- paste(Field_ID.2$Treatment, Field_ID.2$Cultivar, sep = "_")
Field_ID.2$Treatment_Dev_Stage <- paste(Field_ID.2$Treatment, Field_ID.2$Developmental_Stage, sep = "_")
Field_ID.2$Cultivar_Dev_Stage <- paste(Field_ID.2$Cultivar, Field_ID.2$Developmental_Stage, sep = "_")

##K interaction model
k.interaction <- glmmTMB(K ~ Treatment + Developmental_Stage + Cultivar +
                             Cultivar_Dev_Stage + Treatment_Cultivar +(1 | Location), 
                             data = Field_ID.2, family = gaussian(), na.action = "na.fail") ##Triggered convergence warning

k.r2 <- glmm.hp(k.interaction, type = "adjR2", commonality = FALSE) ##More convergence warnings

print(k.r2)
#plot.glmmhp(k.r2)

summary(best_model.k) 
summary(k.interaction)

k.margins$delta <- c(3.44, 40.02, 40.02, 0.21, 50.23, 50.23, 6.11) ##Ensure this is put in same order as fixed effects

##Assess model performance
standardized.resid.k <- resid(best_model.k, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.k <- sum(resid(best_model.k, type = "pearson")^2)
1 - pchisq(dat.resid.k, df.residual(best_model.k))

# Assessing Deviance (G2)
deviance_value.k <- 1 - pchisq(as.numeric(-2 * logLik(best_model.k)), df.residual(best_model.k))

# Simulating datasets
dat.sim.k <- simulate(best_model.k, nsim = 250)

# Empirical cumulative density function calculations
resid.list.k <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.k)) {
    e.k <- ecdf(dat.sim.k[[i]] + runif(length(dat.sim.k[[i]]), -0.5, 0.5))
    resid.list.k[[i]] <- e.k(resid(best_model.k) + runif(length(resid(best_model.k)), -0.5, 0.5))
    plot(e.k, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.k <- do.call(c, resid.list.k)

# Quantile-quantile plot
qqnorm(all.resid.k, main = "QQ Plot")
qqline(all.resid.k, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.k), standardized.resid.k, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline K
shapiro_result.base.k <- shapiro.test(Field_ID.3$K) #W = 0.92651, p-value = 0.2145

###Double check with qqp###
base.k <- Field_ID.3$K + 1

nbinom.base.k <- fitdistr(base.k, "Negative Binomial")
qqp(base.k, "nbinom", size = nbinom.base.k$estimate[[1]], mu = nbinom.base.k$estimate[[2]])

gamma.base.k <- fitdistr(base.k, "gamma")
qqp(base.k, "gamma", shape = gamma.base.k$estimate[[1]], rate = gamma.base.k$estimate[[2]])

qqp(base.k, "lnorm")
qqp(base.k, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.k <- glmmTMB(K ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.k <- dredge(full_model.base.k, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.k)

##Select the best model based on AIC
best_model.base.k <- get.models(models.base.k, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.k) ##Cultivar signif

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Ca
###Checking the normality using Shapiro-Wilk test###
shapiro_result.ca <- shapiro.test(Field_ID.2$Ca) #W = 0.96551, p-value = 0.1683

###Double check with qqp###

ca <- Field_ID.2$Ca + 1 

gamma.ca <- fitdistr(ca, "gamma")
qqp(ca, "gamma", shape = gamma.ca$estimate[[1]], rate = gamma.ca$estimate[[2]])

qqp(ca, "lnorm")
qqp(ca, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.ca <- glmmTMB(Ca ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.ca <- dredge(full_model.ca, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.ca)

##Select the best model based on AIC
best_model.ca <- get.models(models.ca, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.ca) ##Developmental Stage, Treatment, are Significant


###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
ca.margins <- as.data.frame(marginal_effects(best_model.ca, type = "response"))
mean_est.ca <- c(mean(ca.margins$dydx_CultivarCZ4979X), mean(ca.margins$dydx_Developmental_StageR6), mean(ca.margins$dydx_Developmental_StageV6),
                  mean(ca.margins$dydx_TreatmentControl))
print(mean_est.ca)

ca.margins.2 <- ca.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
ca.margins.2$mean_est <- c(12.87474, -57.56253,  50.56248,  36.37507)

ca.margins.2$metric <- 'Ca'

##Print model output
tab_model(best_model.ca)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.ca.1.cult <- powerSim(best_model.ca, fixed("Cultivar"))
power.sim.ca.1.cult

power.sim.ca.1.trt <- powerSim(best_model.ca, fixed("Treatment"))
power.sim.ca.1.trt

power.sim.ca.1.ds <- powerSim(best_model.ca, fixed("Developmental_Stage"))
power.sim.ca.1.ds

##Decompose R2 to get contribution of each fixed effect
ca.r2 <- glmm.hp(best_model.ca, type = "adjR2", commonality = FALSE)

print(ca.r2)
plot.glmmhp(ca.r2)

ca.margins.2$delta <- c(1.78, 83.98, 83.98, 14.24) ##Must put for developmental stage twice

##Assess model performance
standardized.resid.ca <- resid(best_model.ca, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.ca <- sum(resid(best_model.ca, type = "pearson")^2)
1 - pchisq(dat.resid.ca, df.residual(best_model.ca))

# Assessing Deviance (G2)
deviance_value.ca <- 1 - pchisq(as.numeric(-2 * logLik(best_model.ca)), df.residual(best_model.ca))

# Simulating datasets
dat.sim.ca <- simulate(best_model.ca, nsim = 250)

# Empirical cumulative density function calculations
resid.list.ca <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.ca)) {
    e.ca <- ecdf(dat.sim.ca[[i]] + runif(length(dat.sim.ca[[i]]), -0.5, 0.5))
    resid.list.ca[[i]] <- e.ca(resid(best_model.ca) + runif(length(resid(best_model.ca)), -0.5, 0.5))
    plot(e.ca, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.ca <- do.call(c, resid.list.ca)

# Quantile-quantile plot
qqnorm(all.resid.ca, main = "QQ Plot")
qqline(all.resid.ca, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.ca), standardized.resid.ca, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Ca
shapiro_result.base.ca <- shapiro.test(Field_ID.3$Ca) #W = 0.98087, p-value = 0.9701

###Double check with qqp###
base.ca <- Field_ID.3$Ca + 1

nbinom.base.ca <- fitdistr(base.ca, "Negative Binomial")
qqp(base.ca, "nbinom", size = nbinom.base.ca$estimate[[1]], mu = nbinom.base.ca$estimate[[2]])

gamma.base.ca <- fitdistr(base.ca, "gamma")
qqp(base.ca, "gamma", shape = gamma.base.ca$estimate[[1]], rate = gamma.base.ca$estimate[[2]])

qqp(base.ca, "lnorm")
qqp(base.ca, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.ca <- glmmTMB(Ca ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.ca <- dredge(full_model.base.ca, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.ca)

##Select the best model based on AIC
best_model.base.ca <- get.models(models.base.ca, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.ca)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Mg
###Checking the normality using Shapiro-Wilk test###
shapiro_result.mg <- shapiro.test(Field_ID.2$Mg) #W = 0.94238, p-value = 0.02006

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.mg$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$Mg, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###

mg <- Field_ID.2$Mg + 1 


gamma.mg <- fitdistr(mg, "gamma")
qqp(mg, "gamma", shape = gamma.mg$estimate[[1]], rate = gamma.mg$estimate[[2]])

qqp(mg, "lnorm")
qqp(mg, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with lnorm distribution
full_model.mg <- glmmTMB(Mg ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.mg <- dredge(full_model.mg, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.mg)

##Select the best model based on AIC
best_model.mg <- get.models(models.mg, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.mg) ##Cultivar is marginally significant Significant

###Marginalize the coefficients due to the use of a nonlinear link function###
mg.margins <- as.data.frame(marginal_effects(best_model.mg, type = "response"))
mean_est.mg <- c(mean(mg.margins$dydx_CultivarCZ4979X), mean(mg.margins$dydx_Developmental_StageR6), mean(mg.margins$dydx_Developmental_StageV6),
                  mean(mg.margins$dydx_TreatmentControl))
print(mean_est.mg)

mg.margins.2 <- mg.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
mg.margins.2$mean_est <- c(8.495499, -8.259567,  4.580076,  4.112055)

mg.margins.2$metric <- 'Mg'

##Print model output
tab_model(best_model.mg)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.mg.1.cult <- powerSim(best_model.mg, fixed("Cultivar"))
power.sim.mg.1.cult

power.sim.mg.1.trt <- powerSim(best_model.mg, fixed("Treatment"))
power.sim.mg.1.trt

power.sim.mg.1.ds <- powerSim(best_model.mg, fixed("Developmental_Stage"))
power.sim.mg.1.ds


##Decompose R2 to get contribution of each fixed effect
mg.r2 <- glmm.hp(best_model.mg, type = "adjR2", commonality = FALSE)

print(mg.r2)
plot.glmmhp(mg.r2)

mg.margins.2$delta <- c(0,0,0,0) ##Showing NA, will report approximately same value for each

##Assess model performance
standardized.resid.mg <- resid(best_model.mg, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.mg <- sum(resid(best_model.mg, type = "pearson")^2)
1 - pchisq(dat.resid.mg, df.residual(best_model.mg))


# Assessing Deviance (G2)
deviance_value.mg <- 1 - pchisq(as.numeric(-2 * logLik(best_model.mg)), df.residual(best_model.mg))

# Simulating datasets
dat.sim.mg <- simulate(best_model.mg, nsim = 250)

# Empirical cumulative density function calculations
resid.list.mg <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.mg)) {
    e.mg <- ecdf(dat.sim.mg[[i]] + runif(length(dat.sim.mg[[i]]), -0.5, 0.5))
    resid.list.mg[[i]] <- e.mg(resid(best_model.mg) + runif(length(resid(best_model.mg)), -0.5, 0.5))
    plot(e.mg, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.mg <- do.call(c, resid.list.mg)

# Quantile-quantile plot
qqnorm(all.resid.mg, main = "QQ Plot")
qqline(all.resid.mg, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.mg), standardized.resid.mg, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Mg
shapiro_result.base.mg <- shapiro.test(Field_ID.3$Mg) #W = 0.9408, p-value = 0.3589

###Double check with qqp###
base.mg <- Field_ID.3$Mg + 1

nbinom.base.mg <- fitdistr(base.mg, "Negative Binomial")
qqp(base.mg, "nbinom", size = nbinom.base.mg$estimate[[1]], mu = nbinom.base.mg$estimate[[2]])

gamma.base.mg <- fitdistr(base.mg, "gamma")
qqp(base.mg, "gamma", shape = gamma.base.mg$estimate[[1]], rate = gamma.base.mg$estimate[[2]])

qqp(base.mg, "lnorm")
qqp(base.mg, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.mg <- glmmTMB(Mg ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.mg <- dredge(full_model.base.mg, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.mg)

##Select the best model based on AIC
best_model.base.mg <- get.models(models.base.mg, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.mg)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##S
###Checking the normality using Shapiro-Wilk test###
shapiro_result.s <- shapiro.test(Field_ID.2$S) #W = 0.97342, p-value = 0.3421

###Double check with qqp###

s <- Field_ID.2$S + 1 

#nbinom.shannon <- fitdistr(shannon, "Negative Binomial")
#qqp(shannon, "nbinom", size = nbinom.shannon$estimate[[1]], mu = nbinom.shannon$estimate[[2]])

gamma.s <- fitdistr(s, "gamma")
qqp(s, "gamma", shape = gamma.s$estimate[[1]], rate = gamma.s$estimate[[2]])

qqp(s, "lnorm")
qqp(s, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.s <- glmmTMB(S ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.s <- dredge(full_model.s, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.s)

##Select the best model based on AIC
best_model.s <- get.models(models.s, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.s) ##Cultivar is marginally significant Significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
s.margins <- as.data.frame(marginal_effects(best_model.s, type = "response"))
mean_est.s <- c(mean(s.margins$dydx_CultivarCZ4979X), mean(s.margins$dydx_Developmental_StageR6), mean(s.margins$dydx_Developmental_StageV6),
                  mean(s.margins$dydx_TreatmentControl))
print(mean_est.s)

s.margins.2 <- s.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
s.margins.2$mean_est <- c(-1.6250002, 1.3125004,  0.6250004,  0.2916660)

s.margins.2$metric <- 'S'

##Print model output
tab_model(best_model.s)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.s.1.cult <- powerSim(best_model.s, fixed("Cultivar"))
power.sim.s.1.cult

power.sim.s.1.trt <- powerSim(best_model.s, fixed("Treatment"))
power.sim.s.1.trt

power.sim.s.1.ds <- powerSim(best_model.s, fixed("Developmental_Stage"))
power.sim.s.1.ds


##Decompose R2 to get contribution of each fixed effect
s.r2 <- glmm.hp(best_model.s, type = "adjR2", commonality = FALSE)

print(s.r2)
plot.glmmhp(s.r2)

s.margins.2$delta <- c(68.12, 29.69, 29.69, 2.19) 

##Assess model performance
standardized.resid.s <- resid(best_model.s, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.s <- sum(resid(best_model.s, type = "pearson")^2)
1 - pchisq(dat.resid.s, df.residual(best_model.s))


# Assessing Deviance (G2)
deviance_value.s <- 1 - pchisq(as.numeric(-2 * logLik(best_model.s)), df.residual(best_model.s))

# Simulating datasets
dat.sim.s <- simulate(best_model.s, nsim = 250)

# Empirical cumulative density function calculations
resid.list.s <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.s)) {
    e.s <- ecdf(dat.sim.s[[i]] + runif(length(dat.sim.s[[i]]), -0.5, 0.5))
    resid.list.s[[i]] <- e.s(resid(best_model.s) + runif(length(resid(best_model.s)), -0.5, 0.5))
    plot(e.s, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.s <- do.call(c, resid.list.s)

# Quantile-quantile plot
qqnorm(all.resid.s, main = "QQ Plot")
qqline(all.resid.s, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.s), standardized.resid.s, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline S
shapiro_result.base.s <- shapiro.test(Field_ID.3$S) #W = 0.95068, p-value = 0.5006

###Double check with qqp###
base.s <- Field_ID.3$S + 1

nbinom.base.s <- fitdistr(base.s, "Negative Binomial")
qqp(base.s, "nbinom", size = nbinom.base.s$estimate[[1]], mu = nbinom.base.s$estimate[[2]])

gamma.base.s <- fitdistr(base.s, "gamma")
qqp(base.s, "gamma", shape = gamma.base.s$estimate[[1]], rate = gamma.base.s$estimate[[2]])

qqp(base.s, "lnorm")
qqp(base.s, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.s <- glmmTMB(S ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.s <- dredge(full_model.base.s, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.s)

##Select the best model based on AIC
best_model.base.s <- get.models(models.base.s, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.s)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##B
###Checking the normality using Shapiro-Wilk test###
shapiro_result.b <- shapiro.test(Field_ID.2$B) #W = 0.76731, p-value = 2.554e-07

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.b$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") 
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$B, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###

b <- Field_ID.2$B + 1 

gamma.b <- fitdistr(b, "gamma")
qqp(b, "gamma", shape = gamma.b$estimate[[1]], rate = gamma.b$estimate[[2]])

qqp(b, "lnorm")
qqp(b, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with lnorm distribution
full_model.b <- glmmTMB(B ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.b <- dredge(full_model.b, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.b)

##Select the best model based on AIC
best_model.b <- get.models(models.b, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.b) ##Cultivar and Developmental Stage are significant Significant

###Marginalize the coefficients###
b.margins <- as.data.frame(marginal_effects(best_model.b, type = "response"))
mean_est.b <- c(mean(b.margins$dydx_CultivarCZ4979X), mean(b.margins$dydx_Developmental_StageR6), mean(b.margins$dydx_Developmental_StageV6),
                  mean(b.margins$dydx_TreatmentControl))
print(mean_est.b)

b.margins.2 <- b.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
b.margins.2$mean_est <- c(0.02553917, -0.08310288,  0.04348868,  0.01328542)

b.margins.2$metric <- 'B'

##Print model output
tab_model(best_model.b)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.b.1.cult <- powerSim(best_model.b, fixed("Cultivar"))
power.sim.b.1.cult

power.sim.b.1.trt <- powerSim(best_model.b, fixed("Treatment"))
power.sim.b.1.trt 

power.sim.b.1.ds <- powerSim(best_model.b, fixed("Developmental_Stage"))
power.sim.b.1.ds


##Decompose R2 to get contribution of each fixed effect
b.r2 <- glmm.hp(best_model.b, type = "adjR2", commonality = FALSE)

print(b.r2)
plot.glmmhp(b.r2)

b.margins.2$delta <- c(19.78, 71.83, 71.83, 8.39) 

##Assess model performance
standardized.resid.b <- resid(best_model.b, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.b <- sum(resid(best_model.b, type = "pearson")^2)
1 - pchisq(dat.resid.b, df.residual(best_model.b))

# Assessing Deviance (G2)
deviance_value.b <- 1 - pchisq(as.numeric(-2 * logLik(best_model.b)), df.residual(best_model.b))

# Simulating datasets
dat.sim.b <- simulate(best_model.b, nsim = 250)

# Empirical cumulative density function calculations
resid.list.b <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.b)) {
    e.b <- ecdf(dat.sim.b[[i]] + runif(length(dat.sim.b[[i]]), -0.5, 0.5))
    resid.list.b[[i]] <- e.b(resid(best_model.b) + runif(length(resid(best_model.b)), -0.5, 0.5))
    plot(e.b, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.b <- do.call(c, resid.list.b)

# Quantile-quantile plot
qqnorm(all.resid.b, main = "QQ Plot")
qqline(all.resid.b, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.b), standardized.resid.b, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline B
shapiro_result.base.b <- shapiro.test(Field_ID.3$B) #W = 0.62089, p-value = 0.00002566

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.base.b$p.value < 0.05) {
distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.3$B, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- gamma
}  

###Double check with qqp###
base.b <- Field_ID.3$B + 1

#nbinom.base.b <- fitdistr(base.b, "Negative Binomial")
#qqp(base.b, "nbinom", size = nbinom.base.b$estimate[[1]], mu = nbinom.base.b$estimate[[2]])

gamma.base.b <- fitdistr(base.b, "gamma")
qqp(base.b, "gamma", shape = gamma.base.b$estimate[[1]], rate = gamma.base.b$estimate[[2]])

qqp(base.b, "lnorm")
qqp(base.b, "norm")

###Will use a gamma distribution
##Full model
full_model.base.b <- glmmTMB(B ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = Gamma(link = "log"),
                          na.action = "na.fail")

##Model selection
models.base.b <- dredge(full_model.base.b, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.b)

##Select the best model based on AIC
best_model.base.b <- get.models(models.base.b, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.b)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Cu
###Checking the normality using Shapiro-Wilk test###
shapiro_result.cu <- shapiro.test(Field_ID.2$Cu) #W = 0.90727, p-value = 0.001088

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.cu$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") 
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$Cu, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###

cu <- Field_ID.2$Cu + 1 

gamma.cu <- fitdistr(cu, "gamma")
qqp(cu, "gamma", shape = gamma.cu$estimate[[1]], rate = gamma.cu$estimate[[2]])

qqp(cu, "lnorm")
qqp(cu, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with lnorm distribution
full_model.cu <- glmmTMB(Cu ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.cu <- dredge(full_model.cu, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.cu)

##Select the best model based on AIC
best_model.cu <- get.models(models.cu, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.cu) ##No significance

###Marginalize the coefficients###
cu.margins <- as.data.frame(marginal_effects(best_model.cu, type = "response"))
mean_est.cu <- c(mean(cu.margins$dydx_CultivarCZ4979X), mean(cu.margins$dydx_Developmental_StageR6), mean(cu.margins$dydx_Developmental_StageV6),
                  mean(cu.margins$dydx_TreatmentControl))
print(mean_est.cu)

cu.margins.2 <- cu.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
cu.margins.2$mean_est <- c(0.009012009, -0.071409640,  0.072808949,  0.102602207)

cu.margins.2$metric <- 'Cu'

##Print model output
tab_model(best_model.cu)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.cu.1.cult <- powerSim(best_model.cu, fixed("Cultivar"))
power.sim.cu.1.cult

power.sim.cu.1.trt <- powerSim(best_model.cu, fixed("Treatment"))
power.sim.cu.1.trt

power.sim.cu.1.ds <- powerSim(best_model.cu, fixed("Developmental_Stage"))
power.sim.cu.1.ds

##Decompose R2 to get contribution of each fixed effect
cu.r2 <- glmm.hp(best_model.cu, type = "adjR2", commonality = FALSE) 

print(cu.r2)
plot.glmmhp(cu.r2)

cu.margins.2$delta <- c(0, 56.9, 56.9, 43.1) 

##Assess model performance
standardized.resid.cu <- resid(best_model.cu, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.cu <- sum(resid(best_model.cu, type = "pearson")^2)
1 - pchisq(dat.resid.cu, df.residual(best_model.cu))

# Assessing Deviance (G2)
deviance_value.cu <- 1 - pchisq(as.numeric(-2 * logLik(best_model.cu)), df.residual(best_model.cu))

# Simulating datasets
dat.sim.cu <- simulate(best_model.cu, nsim = 250)

# Empirical cumulative density function calculations
resid.list.cu <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.cu)) {
    e.cu <- ecdf(dat.sim.cu[[i]] + runif(length(dat.sim.cu[[i]]), -0.5, 0.5))
    resid.list.cu[[i]] <- e.cu(resid(best_model.cu) + runif(length(resid(best_model.cu)), -0.5, 0.5))
    plot(e.cu, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.cu <- do.call(c, resid.list.cu)

# Quantile-quantile plot
qqnorm(all.resid.cu, main = "QQ Plot")
qqline(all.resid.cu, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.cu), standardized.resid.cu, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Cu
shapiro_result.base.cu <- shapiro.test(Field_ID.3$Cu) #W = 0.89429, p-value = 0.06516

###Double check with qqp###
base.cu <- Field_ID.3$Cu + 1

#nbinom.base.cu <- fitdistr(base.cu, "Negative Binomial")
#qqp(base.cu, "nbinom", size = nbinom.base.cu$estimate[[1]], mu = nbinom.base.cu$estimate[[2]])

gamma.base.cu <- fitdistr(base.cu, "gamma")
qqp(base.cu, "gamma", shape = gamma.base.cu$estimate[[1]], rate = gamma.base.cu$estimate[[2]])

qqp(base.cu, "lnorm")
qqp(base.cu, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.cu <- glmmTMB(Cu ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.cu <- dredge(full_model.base.cu, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.cuase.cu)

##Select the best model based on AIC
best_model.base.cu <- get.models(models.base.cu, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.cu)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Fe
###Checking the normality using Shapiro-Wilk test###
shapiro_result.fe <- shapiro.test(Field_ID.2$Fe) #W = 0.94367, p-value = 0.02251

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.fe$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$Fe, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit ##lnorm
}  

###Double check with qqp###

fe<- Field_ID.2$Fe + 1 

gamma.fe <- fitdistr(fe, "gamma")
qqp(fe, "gamma", shape = gamma.fe$estimate[[1]], rate = gamma.fe$estimate[[2]])

qqp(fe, "lnorm")
qqp(fe, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with lnorm distribution
full_model.fe <- glmmTMB(Fe ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.fe <- dredge(full_model.fe, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.fe)

##Select the best model based on AIC
best_model.fe <- get.models(models.fe, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.fe) ##Cultivar and Treatment are significant

###Marginalize the coefficients###
fe.margins <- as.data.frame(marginal_effects(best_model.fe, type = "response"))
mean_est.fe <- c(mean(fe.margins$dydx_CultivarCZ4979X), mean(fe.margins$dydx_Developmental_StageR6), mean(fe.margins$dydx_Developmental_StageV6),
                  mean(fe.margins$dydx_TreatmentControl))
print(mean_est.fe)

fe.margins.2 <- fe.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
fe.margins.2$mean_est <- c(-9.562628,  1.668945, -4.557962, 13.415665)

fe.margins.2$metric <- 'Fe'

##Print model output
tab_model(best_model.fe)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.fe.1.cult <- powerSim(best_model.fe, fixed("Cultivar"))
power.sim.fe.1.cult

power.sim.fe.1.trt <- powerSim(best_model.fe, fixed("Treatment"))
power.sim.fe.1.trt

power.sim.fe.1.ds <- powerSim(best_model.fe, fixed("Developmental_Stage"))
power.sim.fe.1.ds


##Decompose R2 to get contribution of each fixed effect
fe.r2 <- glmm.hp(best_model.fe, type = "adjR2", commonality = FALSE) 

print(fe.r2)
plot.glmmhp(fe.r2)

fe.margins.2$delta <- c(0,0,0,0) ##Not calculated, will put NA for now

##Assess model performance
standardized.resid.fe <- resid(best_model.fe, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.fe <- sum(resid(best_model.fe, type = "pearson")^2)
1 - pchisq(dat.resid.fe, df.residual(best_model.fe))


# Assessing Deviance (G2)
deviance_value.fe <- 1 - pchisq(as.numeric(-2 * logLik(best_model.fe)), df.residual(best_model.fe))

# Simulating datasets
dat.sim.fe <- simulate(best_model.fe, nsim = 250)

# Empirical cumulative density function calculations
resid.list.fe <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.fe)) {
    e.fe <- ecdf(dat.sim.fe[[i]] + runif(length(dat.sim.fe[[i]]), -0.5, 0.5))
    resid.list.fe[[i]] <- e.fe(resid(best_model.fe) + runif(length(resid(best_model.fe)), -0.5, 0.5))
    plot(e.fe, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.fe <- do.call(c, resid.list.fe)

# Quantile-quantile plot
qqnorm(all.resid.fe, main = "QQ Plot")
qqline(all.resid.fe, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.fe), standardized.resid.fe, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Fe
shapiro_result.base.fe <- shapiro.test(Field_ID.3$Fe) #W = 0.9618, p-value = 0.6945

###Double check with qqp###
base.fe <- Field_ID.3$Fe + 1

nbinom.base.fe <- fitdistr(base.fe, "Negative Binomial")
qqp(base.fe, "nbinom", size = nbinom.base.fe$estimate[[1]], mu = nbinom.base.fe$estimate[[2]])

gamma.base.fe <- fitdistr(base.fe, "gamma")
qqp(base.fe, "gamma", shape = gamma.base.fe$estimate[[1]], rate = gamma.base.fe$estimate[[2]])

qqp(base.fe, "lnorm")
qqp(base.fe, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.fe <- glmmTMB(Fe ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.fe <- dredge(full_model.base.fe, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.fe)

##Select the best model based on AIC
best_model.base.fe <- get.models(models.base.fe, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.fe) ##Cultivar is marginally significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Mn
###Checking the normality using Shapiro-Wilk test###
shapiro_result.mn <- shapiro.test(Field_ID.2$Mn) #W = 0.96198, p-value = 0.1214

###Double check with qqp###

mn <- Field_ID.2$Mn + 1 

gamma.mn <- fitdistr(mn, "gamma")
qqp(mn, "gamma", shape = gamma.mn$estimate[[1]], rate = gamma.mn$estimate[[2]])

qqp(mn, "lnorm")
qqp(mn, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.mn <- glmmTMB(Mn ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.mn <- dredge(full_model.mn, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.mn)

##Select the best model based on AIC
best_model.mn <- get.models(models.mn, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.mn) ##No significance

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
mn.margins <- as.data.frame(marginal_effects(best_model.mn, type = "response"))
mean_est.mn <- c(mean(mn.margins$dydx_CultivarCZ4979X), mean(mn.margins$dydx_Developmental_StageR6), mean(mn.margins$dydx_Developmental_StageV6),
                  mean(mn.margins$dydx_TreatmentControl))
print(mean_est.mn)

mn.margins.2 <- mn.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
mn.margins.2$mean_est <- c(4.083327, -11.937495,   6.000012,  -9.666653)

mn.margins.2$metric <- 'Mn'

##Print model output
tab_model(best_model.mn)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.mn.1.cult <- powerSim(best_model.mn, fixed("Cultivar"))
power.sim.mn.1.cult

power.sim.mn.1.trt <- powerSim(best_model.mn, fixed("Treatment"))
power.sim.mn.1.trt

power.sim.mn.1.ds <- powerSim(best_model.mn, fixed("Developmental_Stage"))
power.sim.mn.1.ds

##Decompose R2 to get contribution of each fixed effect
mn.r2 <- glmm.hp(best_model.mn, type = "adjR2", commonality = FALSE) 

print(mn.r2)
plot.glmmhp(mn.r2)

mn.margins.2$delta <- c(5.07, 66.74, 66.74, 28.19) 

##Assess model performance
standardized.resid.mn <- resid(best_model.mn, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.mn <- sum(resid(best_model.mn, type = "pearson")^2)
1 - pchisq(dat.resid.mn, df.residual(best_model.mn))

# Assessing Deviance (G2)
deviance_value.mn <- 1 - pchisq(as.numeric(-2 * logLik(best_model.mn)), df.residual(best_model.mn))

# Simulating datasets
dat.sim.mn <- simulate(best_model.mn, nsim = 250)

# Empirical cumulative density function calculations
resid.list.mn <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.mn)) {
    e.mn <- ecdf(dat.sim.mn[[i]] + runif(length(dat.sim.mn[[i]]), -0.5, 0.5))
    resid.list.mn[[i]] <- e.mn(resid(best_model.mn) + runif(length(resid(best_model.mn)), -0.5, 0.5))
    plot(e.mn, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.mn <- do.call(c, resid.list.mn)

# Quantile-quantile plot
qqnorm(all.resid.mn, main = "QQ Plot")
qqline(all.resid.mn, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.mn), standardized.resid.mn, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Mn
shapiro_result.base.mn <- shapiro.test(Field_ID.3$Mn) #W = 0.91387, p-value = 0.1343

###Double check with qqp###
base.mn <- Field_ID.3$Mn + 1

nbinom.base.mn <- fitdistr(base.mn, "Negative Binomial")
qqp(base.mn, "nbinom", size = nbinom.base.mn$estimate[[1]], mu = nbinom.base.mn$estimate[[2]])

gamma.base.mn <- fitdistr(base.mn, "gamma")
qqp(base.mn, "gamma", shape = gamma.base.mn$estimate[[1]], rate = gamma.base.mn$estimate[[2]])

qqp(base.mn, "lnorm")
qqp(base.mn, "norm")

###Will use a normal distribution; fits all distributions similarly
##Full model
full_model.base.mn <- glmmTMB(Mn ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.mn <- dredge(full_model.base.mn, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.mn)

##Select the best model based on AIC
best_model.base.mn <- get.models(models.base.mn, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.mn)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Zn
###Checking the normality using Shapiro-Wilk test###
shapiro_result.zn <- shapiro.test(Field_ID.2$Zn) #W = 0.9759, p-value = 0.4215

###Double check with qqp###

zn <- Field_ID.2$Zn + 1 

gamma.zn <- fitdistr(zn, "gamma")
qqp(zn, "gamma", shape = gamma.zn$estimate[[1]], rate = gamma.zn$estimate[[2]])

qqp(zn, "lnorm")
qqp(zn, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.zn <- glmmTMB(Zn ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.zn <- dredge(full_model.zn, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.zn)

##Select the best model based on AIC
best_model.zn <- get.models(models.zn, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.zn) ##No significance

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
zn.margins <- as.data.frame(marginal_effects(best_model.zn, type = "response"))
mean_est.zn <- c(mean(zn.margins$dydx_CultivarCZ4979X), mean(zn.margins$dydx_Developmental_StageR6), mean(zn.margins$dydx_Developmental_StageV6),
                  mean(zn.margins$dydx_TreatmentControl))
print(mean_est.zn)

zn.margins.2 <- zn.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
zn.margins.2$mean_est <- c(-0.1375000, -0.2750000,  0.1625001,  0.1541665)

zn.margins.2$metric <- 'Zn'

##Print model output
tab_model(best_model.zn)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.zn.1.cult <- powerSim(best_model.zn, fixed("Cultivar"))
power.sim.zn.1.cult

power.sim.zn.1.trt <- powerSim(best_model.zn, fixed("Treatment"))
power.sim.zn.1.trt

power.sim.zn.1.ds <- powerSim(best_model.zn, fixed("Developmental_Stage"))
power.sim.zn.1.ds

##Decompose R2 to get contribution of each fixed effect
zn.r2 <- glmm.hp(best_model.zn, type = "adjR2", commonality = FALSE) 

print(zn.r2)
plot.glmmhp(zn.r2)

zn.margins.2$delta <- c(10.93, 75.35, 75.35, 13.72) 

##Assess model performance
standardized.resid.zn <- resid(best_model.zn, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.zn <- sum(resid(best_model.zn, type = "pearson")^2)
1 - pchisq(dat.resid.zn, df.residual(best_model.zn))

# Assessing Deviance (G2)
deviance_value.zn <- 1 - pchisq(as.numeric(-2 * logLik(best_model.zn)), df.residual(best_model.zn))

# Simulating datasets
dat.sim.zn <- simulate(best_model.zn, nsim = 250)

# Empirical cumulative density function calculations
resid.list.zn <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.zn)) {
    e.zn <- ecdf(dat.sim.zn[[i]] + runif(length(dat.sim.zn[[i]]), -0.5, 0.5))
    resid.list.zn[[i]] <- e.zn(resid(best_model.zn) + runif(length(resid(best_model.zn)), -0.5, 0.5))
    plot(e.zn, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.zn <- do.call(c, resid.list.zn)

# Quantile-quantile plot
qqnorm(all.resid.zn, main = "QQ Plot")
qqline(all.resid.zn, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.zn), standardized.resid.zn, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Zn
shapiro_result.base.zn <- shapiro.test(Field_ID.3$Zn) #W = 0.6886, p-value = 0.0001267

if (shapiro_result.base.zn$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") 
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.3$Zn, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###
base.zn <- Field_ID.3$Zn + 1

#nbinom.base.zn <- fitdistr(base.zn, "Negative Binomial")
#qqp(base.zn, "nbinom", size = nbinom.base.zn$estimate[[1]], mu = nbinom.base.zn$estimate[[2]])

gamma.base.zn <- fitdistr(base.zn, "gamma")
qqp(base.zn, "gamma", shape = gamma.base.zn$estimate[[1]], rate = gamma.base.zn$estimate[[2]])

qqp(base.zn, "lnorm")
qqp(base.zn, "norm")

###Will use lnorm distribution
##Full model
full_model.base.zn <- glmmTMB(Zn ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "log"),
                          na.action = "na.fail")

##Model selection
models.base.zn <- dredge(full_model.base.zn, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.zn)

##Select the best model based on AIC
best_model.base.zn <- get.models(models.base.zn, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.zn)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Na
###Checking the normality using Shapiro-Wilk test###
shapiro_result.na <- shapiro.test(Field_ID.2$Na) #W = 0.97403, p-value = 0.3604

###Double check with qqp###

na <- Field_ID.2$Na + 1 

gamma.na <- fitdistr(na, "gamma")
qqp(na, "gamma", shape = gamma.na$estimate[[1]], rate = gamma.na$estimate[[2]])

qqp(na, "lnorm")
qqp(na, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.na <- glmmTMB(Na ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.na <- dredge(full_model.na, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.na)

##Select the best model based on AIC
best_model.na <- get.models(models.na, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.na) ##Cultivar and developmental stage show some significance

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
na.margins <- as.data.frame(marginal_effects(best_model.na, type = "response"))
mean_est.na <- c(mean(na.margins$dydx_CultivarCZ4979X), mean(na.margins$dydx_Developmental_StageR6), mean(na.margins$dydx_Developmental_StageV6),
                  mean(na.margins$dydx_TreatmentControl))
print(mean_est.na)

na.margins.2 <- na.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
na.margins.2$mean_est <- c(1.0833333,  1.0000000,  2.1249999, -0.1666667)

na.margins.2$metric <- 'Na'

##Print model output
tab_model(best_model.na)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.na.1.cult <- powerSim(best_model.na, fixed("Cultivar"))
power.sim.na.1.cult

power.sim.na.1.trt <- powerSim(best_model.na, fixed("Treatment"))
power.sim.na.1.trt

power.sim.na.1.ds <- powerSim(best_model.na, fixed("Developmental_Stage"))
power.sim.na.1.ds

##Decompose R2 to get contribution of each fixed effect
na.r2 <- glmm.hp(best_model.na, type = "adjR2", commonality = FALSE) 

print(na.r2)
plot.glmmhp(na.r2)

na.margins.2$delta <- c(27.86, 71.51, 71.51, 0.63)

##Assess model performance
standardized.resid.na <- resid(best_model.na, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.na <- sum(resid(best_model.na, type = "pearson")^2)
1 - pchisq(dat.resid.na, df.residual(best_model.na))

# Assessing Deviance (G2)
deviance_value.na <- 1 - pchisq(as.numeric(-2 * logLik(best_model.na)), df.residual(best_model.na))

# Simulating datasets
dat.sim.na <- simulate(best_model.na, nsim = 250)

# Empirical cumulative density function calculations
resid.list.na <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.na)) {
    e.na <- ecdf(dat.sim.na[[i]] + runif(length(dat.sim.na[[i]]), -0.5, 0.5))
    resid.list.na[[i]] <- e.na(resid(best_model.na) + runif(length(resid(best_model.na)), -0.5, 0.5))
    plot(e.na, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.na <- do.call(c, resid.list.na)

# Quantile-quantile plot
qqnorm(all.resid.na, main = "QQ Plot")
qqline(all.resid.na, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.na), standardized.resid.na, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Na
shapiro_result.base.na <- shapiro.test(Field_ID.3$Na) #W = 0.89254, p-value = 0.06112

###Double check with qqp###
base.na <- Field_ID.3$Na + 1

nbinom.base.na <- fitdistr(base.na, "Negative Binomial")
qqp(base.na, "nbinom", size = nbinom.base.na$estimate[[1]], mu = nbinom.base.na$estimate[[2]])

gamma.base.na <- fitdistr(base.na, "gamma")
qqp(base.na, "gamma", shape = gamma.base.na$estimate[[1]], rate = gamma.base.na$estimate[[2]])

qqp(base.na, "lnorm")
qqp(base.na, "norm")

###Will use a normal distribution
##Full model
full_model.base.na <- glmmTMB(Na ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.na <- dredge(full_model.base.na, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.na)

##Select the best model based on AIC
best_model.base.na <- get.models(models.base.na, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.na) ##treatment marginally significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Soil ph
###Checking the normality using Shapiro-Wilk test###
shapiro_result.ph <- shapiro.test(Field_ID.2$Soil_pH) #W = 0.94524, p-value = 0.02593

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.ph$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$Soil_pH, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###

ph <- Field_ID.2$Soil_pH + 1 

gamma.ph <- fitdistr(ph, "gamma")
qqp(ph, "gamma", shape = gamma.ph$estimate[[1]], rate = gamma.ph$estimate[[2]])

qqp(ph, "lnorm")
qqp(ph, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with lnorm distribution
full_model.ph <- glmmTMB(Soil_pH ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.ph <- dredge(full_model.ph, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.ph)

##Select the best model based on AIC
best_model.ph <- get.models(models.ph, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.ph) ##Marginal significance- treatment


###Marginalize the coefficients###
ph.margins <- as.data.frame(marginal_effects(best_model.ph, type = "response"))
mean_est.ph <- c(mean(ph.margins$dydx_CultivarCZ4979X), mean(ph.margins$dydx_Developmental_StageR6), mean(ph.margins$dydx_Developmental_StageV6),
                  mean(ph.margins$dydx_TreatmentControl))
print(mean_est.ph)

ph.margins.2 <- ph.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
ph.margins.2$mean_est <- c(0.05241788, 0.02497222, 0.06842118, 0.07891151)

ph.margins.2$metric <- 'Soil pH'

##Print model output
tab_model(best_model.ph)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.ph.1.cult <- powerSim(best_model.ph, fixed("Cultivar"))
power.sim.ph.1.cult

power.sim.ph.1.trt <- powerSim(best_model.ph, fixed("Treatment"))
power.sim.ph.1.trt

power.sim.ph.1.ds <- powerSim(best_model.ph, fixed("Developmental_Stage"))
power.sim.ph.1.ds

##Decompose R2 to get contribution of each fixed effect
ph.r2 <- glmm.hp(best_model.ph, type = "adjR2", commonality = FALSE) 

print(ph.r2)
plot.glmmhp(ph.r2)

ph.margins.2$delta <- c(23.08, 25.64, 25.64, 51.28) 

##Assess model performance
standardized.resid.ph <- resid(best_model.ph, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.ph <- sum(resid(best_model.ph, type = "pearson")^2)
1 - pchisq(dat.resid.ph, df.residual(best_model.ph))

# Assessing Deviance (G2)
deviance_value.ph <- 1 - pchisq(as.numeric(-2 * logLik(best_model.ph)), df.residual(best_model.ph))

# Simulating datasets
dat.sim.ph <- simulate(best_model.ph, nsim = 250)

# Empirical cumulative density function calculations
resid.list.ph <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.ph)) {
    e.ph <- ecdf(dat.sim.ph[[i]] + runif(length(dat.sim.ph[[i]]), -0.5, 0.5))
    resid.list.ph[[i]] <- e.ph(resid(best_model.ph) + runif(length(resid(best_model.ph)), -0.5, 0.5))
    plot(e.ph, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.ph <- do.call(c, resid.list.ph)

# Quantile-quantile plot
qqnorm(all.resid.ph, main = "QQ Plot")
qqline(all.resid.ph, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.ph), standardized.resid.ph, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Soil pH
shapiro_result.base.ph <- shapiro.test(Field_ID.3$Soil_pH) #W = 0.8809, p-value = 0.04008

if (shapiro_result.base.ph$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") 
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.3$Soil_pH, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###
base.ph <- Field_ID.3$Soil_pH + 1

#nbinom.base.ph <- fitdistr(base.ph, "Negative Binomial")
#qqp(base.ph, "nbinom", size = nbinom.base.ph$estimate[[1]], mu = nbinom.base.ph$estimate[[2]])

gamma.base.ph <- fitdistr(base.ph, "gamma")
qqp(base.ph, "gamma", shape = gamma.base.ph$estimate[[1]], rate = gamma.base.ph$estimate[[2]])

qqp(base.ph, "lnorm")
qqp(base.ph, "norm")

###Will use lnorm distribution
##Full model
full_model.base.ph <- glmmTMB(Soil_pH ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "log"),
                          na.action = "na.fail")

##Model selection
models.base.ph <- dredge(full_model.base.ph, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.ph)

##Select the best model based on AIC
best_model.base.ph <- get.models(models.base.ph, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.ph)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Buffer ph
###Checking the normality using Shapiro-Wilk test###
shapiro_result.buffer <- shapiro.test(Field_ID.2$Buffer_pH) #W = 0.94012, p-value = 0.0164

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.buffer$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$Buffer_pH, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- norm
}  

###Double check with qqp###

buf <- Field_ID.2$Buffer_pH + 1 

gamma.buf <- fitdistr(buf, "gamma")
qqp(buf, "gamma", shape = gamma.buf$estimate[[1]], rate = gamma.buf$estimate[[2]])

qqp(ph, "lnorm")
qqp(ph, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.buf <- glmmTMB(Buffer_pH ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.buf <- dredge(full_model.buf, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.buf)

##Select the best model based on AIC
best_model.buf <- get.models(models.buf, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.buf) ##No significance

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
buf.margins <- as.data.frame(marginal_effects(best_model.buf, type = "response"))
mean_est.buf <- c(mean(buf.margins$dydx_CultivarCZ4979X), mean(buf.margins$dydx_Developmental_StageR6), mean(buf.margins$dydx_Developmental_StageV6),
                  mean(buf.margins$dydx_TreatmentControl))
print(mean_est.buf)

buf.margins.2 <- buf.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
buf.margins.2$mean_est <- c(0.009999998, 0.017500000, 0.001250000, 0.011666670)

buf.margins.2$metric <- 'Buffer pH'

##Print model output
tab_model(best_model.buf)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.buf.1.cult <- powerSim(best_model.buf, fixed("Cultivar"))
power.sim.buf.1.cult

power.sim.buf.1.trt <- powerSim(best_model.buf, fixed("Treatment"))
power.sim.buf.1.trt

power.sim.buf.1.ds <- powerSim(best_model.buf, fixed("Developmental_Stage"))
power.sim.buf.1.ds

##Decompose R2 to get contribution of each fixed effect
buf.r2 <- glmm.hp(best_model.buf, type = "adjR2", commonality = FALSE) 

print(buf.r2)
plot.glmmhp(buf.r2)

buf.margins.2$delta <- c(20.28, 51.89, 51.89, 27.83)

##Assess model performance
standardized.resid.buf <- resid(best_model.buf, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.buf <- sum(resid(best_model.buf, type = "pearson")^2)
1 - pchisq(dat.resid.buf, df.residual(best_model.buf))

# Assessing Deviance (G2)
deviance_value.buf <- 1 - pchisq(as.numeric(-2 * logLik(best_model.buf)), df.residual(best_model.buf))

# Simulating datasets
dat.sim.buf <- simulate(best_model.buf, nsim = 250)

# Empirical cumulative density function calculations
resid.list.buf <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.buf)) {
    e.buf <- ecdf(dat.sim.buf[[i]] + runif(length(dat.sim.buf[[i]]), -0.5, 0.5))
    resid.list.buf[[i]] <- e.buf(resid(best_model.buf) + runif(length(resid(best_model.buf)), -0.5, 0.5))
    plot(e.buf, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.buf <- do.call(c, resid.list.buf)

# Quantile-quantile plot
qqnorm(all.resid.buf, main = "QQ Plot")
qqline(all.resid.buf, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.buf), standardized.resid.buf, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Buffer pH
shapiro_result.base.buf <- shapiro.test(Field_ID.3$Buffer_pH) #W = 0.90487, p-value = 0.09623

###Double check with qqp###
base.buf <- Field_ID.3$Buffer_pH + 1

#nbinom.base.buf <- fitdistr(base.buf, "Negative Binomial")
#qqp(base.buf, "nbinom", size = nbinom.base.buf$estimate[[1]], mu = nbinom.base.buf$estimate[[2]])

gamma.base.buf <- fitdistr(base.buf, "gamma")
qqp(base.buf, "gamma", shape = gamma.base.buf$estimate[[1]], rate = gamma.base.buf$estimate[[2]])

qqp(base.buf, "lnorm")
qqp(base.buf, "norm")

###Will use a normal distribution
##Full model
full_model.base.buf <- glmmTMB(Buffer_pH ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.buf <- dredge(full_model.base.buf, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.buf)

##Select the best model based on AIC
best_model.base.buf <- get.models(models.base.buf, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.buf)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##SOM
###Checking the normality using Shapiro-Wilk test###
shapiro_result.som <- shapiro.test(Field_ID.2$SOM) #W = 0.96532, p-value = 0.1654

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.som$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$SOM, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- norm
}  

###Double check with qqp###

som <- Field_ID.2$SOM + 1 

gamma.som <- fitdistr(som, "gamma")
qqp(som, "gamma", shape = gamma.som$estimate[[1]], rate = gamma.som$estimate[[2]])

qqp(som, "lnorm")
qqp(som, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.som <- glmmTMB(SOM ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.som <- dredge(full_model.som, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.som)

##Select the best model based on AIC
best_model.som <- get.models(models.som, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.som) ##Treatment, Developmental_Stage, Cultivar, and interactions are signficant

###Get estimates from model output
som.margins <- data.frame(
  Variable = c("dydx_CultivarCZ4979X", "dydx_Developmental_StageR6", "dydx_Developmental_StageV6",
               "dydx_TreatmentControl", "CultivarCZ4979X:Developmental_StageR6",
               "CultivarCZ4979X:Developmental_StageV6", "CultivarCZ4979X:TreatmentControl"),
  mean_est = c(0.2875, 0.2250, 0.3000, 0.3083, 0.1500, -0.4625, -0.4750))

som.margins$metric <- 'SOM'

##Print model output
tab_model(best_model.som)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.som.1.cult <- powerSim(best_model.som, fixed("Cultivar"))
power.sim.som.1.cult

power.sim.som.1.trt <- powerSim(best_model.som, fixed("Treatment"))
power.sim.som.1.trt

power.sim.som.1.ds <- powerSim(best_model.som, fixed("Developmental_Stage"))
power.sim.som.1.ds

##Decompose R2 to get contribution of each fixed effect
##Must create second model with interactions modeled as fixed effects
som.interaction <- glmmTMB(SOM ~ Treatment + Developmental_Stage + Cultivar +
                             Cultivar_Dev_Stage + Treatment_Cultivar +(1 | Location), 
                             data = Field_ID.2, family = gaussian(), na.action = "na.fail") ##Triggered convergence warning

som.r2 <- glmm.hp(som.interaction, type = "adjR2", commonality = FALSE) ##triggered warning

print(som.r2)
plot.glmmhp(som.r2)

summary(best_model.som)
som.margins$delta <- c(0.51, 16.67, 16.67, 1.26, 51.38, 51.38, 30.19)

##Assess model performance
standardized.resid.som <- resid(best_model.som, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.som <- sum(resid(best_model.som, type = "pearson")^2)
1 - pchisq(dat.resid.som, df.residual(best_model.som))

# Assessing Deviance (G2)
deviance_value.som <- 1 - pchisq(as.numeric(-2 * logLik(best_model.som)), df.residual(best_model.som))

# Simulating datasets
dat.sim.som <- simulate(best_model.som, nsim = 250)

# Empirical cumulative density function calculations
resid.list.som <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.som)) {
    e.som <- ecdf(dat.sim.som[[i]] + runif(length(dat.sim.som[[i]]), -0.5, 0.5))
    resid.list.som[[i]] <- e.som(resid(best_model.som) + runif(length(resid(best_model.som)), -0.5, 0.5))
    plot(e.som, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.som <- do.call(c, resid.list.som)

# Quantile-quantile plot
qqnorm(all.resid.som, main = "QQ Plot")
qqline(all.resid.som, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.som), standardized.resid.som, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline SOM
shapiro_result.base.som <- shapiro.test(Field_ID.3$SOM) #W = 0.91683, p-value = 0.15

###Double check with qqp###
base.som <- Field_ID.3$SOM + 1

#nbinom.base.som <- fitdistr(base.som, "Negative Binomial")
#qqp(base.som, "nbinom", size = nbinom.base.som$estimate[[1]], mu = nbinom.base.som$estimate[[2]])

gamma.base.som <- fitdistr(base.som, "gamma")
qqp(base.som, "gamma", shape = gamma.base.som$estimate[[1]], rate = gamma.base.som$estimate[[2]])

qqp(base.som, "lnorm")
qqp(base.som, "norm")

###Will use a normal distribution
##Full model
full_model.base.som <- glmmTMB(SOM ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.som <- dredge(full_model.base.som, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.som)

##Select the best model based on AIC
best_model.base.som <- get.models(models.base.som, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.som) ##Cultivar is significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##CEC
###Checking the normality using Shapiro-Wilk test###
shapiro_result.cec <- shapiro.test(Field_ID.2$CEC) #W = 0.97098, p-value = 0.2765

###Double check with qqp###

cec <- Field_ID.2$CEC + 1 

gamma.cec <- fitdistr(cec, "gamma")
qqp(cec, "gamma", shape = gamma.cec$estimate[[1]], rate = gamma.cec$estimate[[2]])

qqp(cec, "lnorm")
qqp(cec, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.cec <- glmmTMB(CEC ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.cec <- dredge(full_model.cec, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.cec)

##Select the best model based on AIC
best_model.cec <- get.models(models.cec, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.cec) ##Developmental Stage is significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
cec.margins <- as.data.frame(marginal_effects(best_model.cec, type = "response"))
mean_est.cec <- c(mean(cec.margins$dydx_CultivarCZ4979X), mean(cec.margins$dydx_Developmental_StageR6), mean(cec.margins$dydx_Developmental_StageV6),
                  mean(cec.margins$dydx_TreatmentControl))
print(mean_est.cec)

cec.margins.2 <- cec.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
cec.margins.2$mean_est <- c(0.07083334, -0.56250006,  0.41874994,  0.12083339)

cec.margins.2$metric <- 'CEC'

##Print model output
tab_model(best_model.cec)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.cec.1.cult <- powerSim(best_model.cec, fixed("Cultivar"))
power.sim.cec.1.cult

power.sim.cec.1.trt <- powerSim(best_model.cec, fixed("Treatment"))
power.sim.cec.1.trt

power.sim.cec.1.ds <- powerSim(best_model.cec, fixed("Developmental_Stage"))
power.sim.cec.1.ds


##Decompose R2 to get contribution of each fixed effect
cec.r2 <- glmm.hp(best_model.cec, type = "adjR2", commonality = FALSE) 

print(cec.r2)
plot.glmmhp(cec.r2)

cec.margins.2$delta <- c(0.76, 97.05, 97.05, 2.19)

##Assess model performance
standardized.resid.cec <- resid(best_model.cec, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.cec <- sum(resid(best_model.cec, type = "pearson")^2)
1 - pchisq(dat.resid.cec, df.residual(best_model.cec))

# Assessing Deviance (G2)
deviance_value.cec <- 1 - pchisq(as.numeric(-2 * logLik(best_model.cec)), df.residual(best_model.cec))

# Simulating datasets
dat.sim.cec <- simulate(best_model.cec, nsim = 250)

# Empirical cumulative density function calculations
resid.list.cec <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.cec)) {
    e.cec <- ecdf(dat.sim.cec[[i]] + runif(length(dat.sim.cec[[i]]), -0.5, 0.5))
    resid.list.cec[[i]] <- e.cec(resid(best_model.cec) + runif(length(resid(best_model.cec)), -0.5, 0.5))
    plot(e.cec, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.cec <- do.call(c, resid.list.cec)

# Quantile-quantile plot
qqnorm(all.resid.cec, main = "QQ Plot")
qqline(all.resid.cec, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.cec), standardized.resid.cec, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline CEC
shapiro_result.base.cec <- shapiro.test(Field_ID.3$CEC) #W = 0.871, p-value = 0.02817

if (shapiro_result.base.cec$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") 
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.3$CEC, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###
base.cec <- Field_ID.3$CEC + 1

#nbinom.base.cec <- fitdistr(base.cec, "Negative Binomial")
#qqp(base.cec, "nbinom", size = nbinom.base.cec$estimate[[1]], mu = nbinom.base.cec$estimate[[2]])

gamma.base.cec <- fitdistr(base.cec, "gamma")
qqp(base.cec, "gamma", shape = gamma.base.cec$estimate[[1]], rate = gamma.base.cec$estimate[[2]])

qqp(base.cec, "lnorm")
qqp(base.cec, "norm")

###Will use lnorm distribution
##Full model
full_model.base.cec <- glmmTMB(CEC ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "log"),
                          na.action = "na.fail")

##Model selection
models.base.cec <- dredge(full_model.base.cec, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.cec)

##Select the best model based on AIC
best_model.base.cec <- get.models(models.base.cec, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.cec)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##K/Mg
###Checking the normality using Shapiro-Wilk test###
shapiro_result.kmg <- shapiro.test(Field_ID.2$K_Mg) #W = 0.9411, p-value = 0.01789

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.kmg$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$K_Mg, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###

kmg <- Field_ID.2$K_Mg + 1 

gamma.kmg <- fitdistr(kmg, "gamma")
qqp(kmg, "gamma", shape = gamma.kmg$estimate[[1]], rate = gamma.kmg$estimate[[2]])

qqp(kmg, "lnorm")
qqp(kmg, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.kmg <- glmmTMB(K_Mg ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.kmg <- dredge(full_model.kmg, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.kmg)

##Select the best model based on AIC
best_model.kmg <- get.models(models.kmg, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.kmg) ##Developmental Stage and Cultivar are significant

###Marginalize the coefficients###
kmg.margins <- as.data.frame(marginal_effects(best_model.kmg, type = "response"))
mean_est.kmg <- c(mean(kmg.margins$dydx_CultivarCZ4979X), mean(kmg.margins$dydx_Developmental_StageR6), mean(kmg.margins$dydx_Developmental_StageV6),
                  mean(kmg.margins$dydx_TreatmentControl))
print(mean_est.kmg)

kmg.margins.2 <- kmg.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
kmg.margins.2$mean_est <- c(0.025683259, -0.013743938,  0.112916942,  0.005353891)

kmg.margins.2$metric <- 'K/Mg'

##Print model output
tab_model(best_model.kmg)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.kmg.1.cult <- powerSim(best_model.kmg, fixed("Cultivar"))
power.sim.kmg.1.cult

power.sim.kmg.1.trt <- powerSim(best_model.kmg, fixed("Treatment"))
power.sim.kmg.1.trt

power.sim.kmg.1.ds <- powerSim(best_model.kmg, fixed("Developmental_Stage"))
power.sim.kmg.1.ds

##Decompose R2 to get contribution of each fixed effect
kmg.r2 <- glmm.hp(best_model.kmg, type = "adjR2", commonality = FALSE) 

print(kmg.r2)
plot.glmmhp(kmg.r2)

kmg.margins.2$delta <- c(7.43, 92.16, 92.16, 0.41)

##Assess model performance
standardized.resid.kmg <- resid(best_model.kmg, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.kmg <- sum(resid(best_model.kmg, type = "pearson")^2)
1 - pchisq(dat.resid.kmg, df.residual(best_model.kmg))

# Assessing Deviance (G2)
deviance_value.kmg <- 1 - pchisq(as.numeric(-2 * logLik(best_model.kmg)), df.residual(best_model.kmg))

# Simulating datasets
dat.sim.kmg <- simulate(best_model.kmg, nsim = 250)

# Empirical cumulative density function calculations
resid.list.kmg <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.kmg)) {
    e.kmg <- ecdf(dat.sim.kmg[[i]] + runif(length(dat.sim.kmg[[i]]), -0.5, 0.5))
    resid.list.kmg[[i]] <- e.kmg(resid(best_model.kmg) + runif(length(resid(best_model.kmg)), -0.5, 0.5))
    plot(e.kmg, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.kmg <- do.call(c, resid.list.kmg)

# Quantile-quantile plot
qqnorm(all.resid.kmg, main = "QQ Plot")
qqline(all.resid.kmg, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.kmg), standardized.resid.kmg, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline K/Mg
shapiro_result.base.kmg <- shapiro.test(Field_ID.3$K_Mg) #W = 0.95147, p-value = 0.5133

###Double check with qqp###
base.kmg <- Field_ID.3$K_Mg + 1

#nbinom.base.kmg <- fitdistr(base.kmg, "Negative Binomial")
#qqp(base.kmg, "nbinom", size = nbinom.base.kmg$estimate[[1]], mu = nbinom.base.kmg$estimate[[2]])

gamma.base.kmg <- fitdistr(base.kmg, "gamma")
qqp(base.kmg, "gamma", shape = gamma.base.kmg$estimate[[1]], rate = gamma.base.kmg$estimate[[2]])

qqp(base.kmg, "lnorm")
qqp(base.kmg, "norm")

###Will use a normal distribution
##Full model
full_model.base.kmg <- glmmTMB(K_Mg ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.kmg <- dredge(full_model.base.kmg, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.kmg)

##Select the best model based on AIC
best_model.base.kmg <- get.models(models.base.kmg, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.kmg)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Ca/Mg
###Checking the normality using Shapiro-Wilk test###
shapiro_result.camg <- shapiro.test(Field_ID.2$Ca_Mg) #W = 0.97281, p-value = 0.3247

###Double check with qqp###

camg <- Field_ID.2$Ca_Mg + 1 

gamma.camg <- fitdistr(camg, "gamma")
qqp(camg, "gamma", shape = gamma.camg$estimate[[1]], rate = gamma.camg$estimate[[2]])

qqp(camg, "lnorm")
qqp(camg, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.camg <- glmmTMB(Ca_Mg ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.camg <- dredge(full_model.camg, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.camg)

##Select the best model based on AIC
best_model.camg <- get.models(models.camg, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.camg) ##Developmental Stage and Cultivar are significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
camg.margins <- as.data.frame(marginal_effects(best_model.camg, type = "response"))
mean_est.camg <- c(mean(camg.margins$dydx_CultivarCZ4979X), mean(camg.margins$dydx_Developmental_StageR6), mean(camg.margins$dydx_Developmental_StageV6),
                  mean(camg.margins$dydx_TreatmentControl))
print(mean_est.camg)

camg.margins.2 <- camg.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
camg.margins.2$mean_est <- c(-0.12041665, -0.08562497,  0.10062498,  0.06041663)

camg.margins.2$metric <- 'Ca/Mg'

##Print model output
tab_model(best_model.camg)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.camg.1.cult <- powerSim(best_model.camg, fixed("Cultivar"))
power.sim.camg.1.cult

power.sim.camg.1.trt <- powerSim(best_model.camg, fixed("Treatment"))
power.sim.camg.1.trt

power.sim.camg.1.ds <- powerSim(best_model.camg, fixed("Developmental_Stage"))
power.sim.camg.1.ds

##Decompose R2 to get contribution of each fixed effect
camg.r2 <- glmm.hp(best_model.camg, type = "adjR2", commonality = FALSE) 

print(camg.r2)
plot.glmmhp(camg.r2)

camg.margins.2$delta <- c(35.09, 56.06, 56.06, 8.85)

##Assess model performance
standardized.resid.camg <- resid(best_model.camg, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.camg <- sum(resid(best_model.camg, type = "pearson")^2)
1 - pchisq(dat.resid.camg, df.residual(best_model.camg))

# Assessing Deviance (G2)
deviance_value.camg <- 1 - pchisq(as.numeric(-2 * logLik(best_model.camg)), df.residual(best_model.camg))

# Simulating datasets
dat.sim.camg <- simulate(best_model.camg, nsim = 250)

# Empirical cumulative density function calculations
resid.list.camg <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.camg)) {
    e.camg <- ecdf(dat.sim.camg[[i]] + runif(length(dat.sim.camg[[i]]), -0.5, 0.5))
    resid.list.camg[[i]] <- e.camg(resid(best_model.camg) + runif(length(resid(best_model.camg)), -0.5, 0.5))
    plot(e.camg, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.camg <- do.call(c, resid.list.camg)

# Quantile-quantile plot
qqnorm(all.resid.camg, main = "QQ Plot")
qqline(all.resid.camg, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.camg), standardized.resid.camg, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline Ca/Mg
shapiro_result.base.camg <- shapiro.test(Field_ID.3$Ca_Mg) #W = 0.92805, p-value = 0.227

###Double check with qqp###
base.camg <- Field_ID.3$Ca_Mg + 1

#nbinom.base.camg <- fitdistr(base.camg, "Negative Binomial")
#qqp(base.camg, "nbinom", size = nbinom.base.camg$estimate[[1]], mu = nbinom.base.camg$estimate[[2]])

gamma.base.camg <- fitdistr(base.camg, "gamma")
qqp(base.camg, "gamma", shape = gamma.base.camg$estimate[[1]], rate = gamma.base.camg$estimate[[2]])

qqp(base.camg, "lnorm")
qqp(base.camg, "norm")

###Will use a normal distribution
##Full model
full_model.base.camg <- glmmTMB(Ca_Mg ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.camg <- dredge(full_model.base.camg, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.camg)

##Select the best model based on AIC
best_model.base.camg <- get.models(models.base.camg, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.camg) ##Cultivar marginally significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##GBA3
###Checking the normality using Shapiro-Wilk test###
shapiro_result.gba <- shapiro.test(Field_ID.2$GBA3) #W = 0.98727, p-value = 0.8768

###Double check with qqp###

gba <- Field_ID.2$GBA3 + 1 

gamma.gba <- fitdistr(gba, "gamma")
qqp(gba, "gamma", shape = gamma.gba$estimate[[1]], rate = gamma.gba$estimate[[2]])

qqp(gba, "lnorm")
qqp(gba, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.gba <- glmmTMB(GBA3 ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.gba <- dredge(full_model.gba, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.gba)

##Select the best model based on AIC
best_model.gba <- get.models(models.gba, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.gba) ##Cultivar is significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
gba.margins <- as.data.frame(marginal_effects(best_model.gba, type = "response"))
mean_est.gba <- c(mean(gba.margins$dydx_CultivarCZ4979X), mean(gba.margins$dydx_Developmental_StageR6), mean(gba.margins$dydx_Developmental_StageV6),
                  mean(gba.margins$dydx_TreatmentControl))
print(mean_est.gba)

gba.margins.2 <- gba.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
gba.margins.2$mean_est <- c(6.23750721, -0.51870033, -0.08126292, 0.80414448)

gba.margins.2$metric <- 'GBA3'

##Print model output
tab_model(best_model.gba)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.gba.1.cult <- powerSim(best_model.gba, fixed("Cultivar"))
power.sim.gba.1.cult

power.sim.gba.1.trt <- powerSim(best_model.gba, fixed("Treatment"))
power.sim.gba.1.trt

power.sim.gba.1.ds <- powerSim(best_model.gba, fixed("Developmental_Stage"))
power.sim.gba.1.ds

##Decompose R2 to get contribution of each fixed effect
gba.r2 <- glmm.hp(best_model.gba, type = "adjR2", commonality = FALSE) 

print(gba.r2)
plot.glmmhp(gba.r2)

gba.margins.2$delta <- c(97.84, 0.50, 0.50, 1.66)

##Assess model performance
standardized.resid.gba <- resid(best_model.gba, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.gba <- sum(resid(best_model.gba, type = "pearson")^2)
1 - pchisq(dat.resid.gba, df.residual(best_model.gba))

# Assessing Deviance (G2)
deviance_value.gba <- 1 - pchisq(as.numeric(-2 * logLik(best_model.gba)), df.residual(best_model.gba))

# Simulating datasets
dat.sim.gba <- simulate(best_model.gba, nsim = 250)

# Empirical cumulative density function calculations
resid.list.gba <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.gba)) {
    e.gba <- ecdf(dat.sim.gba[[i]] + runif(length(dat.sim.gba[[i]]), -0.5, 0.5))
    resid.list.gba[[i]] <- e.gba(resid(best_model.gba) + runif(length(resid(best_model.gba)), -0.5, 0.5))
    plot(e.gba, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.gba <- do.call(c, resid.list.gba)

# Quantile-quantile plot
qqnorm(all.resid.gba, main = "QQ Plot")
qqline(all.resid.gba, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.gba), standardized.resid.gba, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline GBA
shapiro_result.base.gba <- shapiro.test(Field_ID.3$GBA3) #W = 0.91057, p-value = 0.1189

###Double check with qqp###
base.gba <- Field_ID.3$GBA3 + 1

#nbinom.base.gba <- fitdistr(base.gba, "Negative Binomial")
#qqp(base.gba, "nbinom", size = nbinom.base.gba$estimate[[1]], mu = nbinom.base.gba$estimate[[2]])

gamma.base.gba <- fitdistr(base.gba, "gamma")
qqp(base.gba, "gamma", shape = gamma.base.gba$estimate[[1]], rate = gamma.base.gba$estimate[[2]])

qqp(base.gba, "lnorm")
qqp(base.gba, "norm")

###Will use a normal distribution
##Full model
full_model.base.gba <- glmmTMB(GBA3 ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.gba <- dredge(full_model.base.gba, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.gba)

##Select the best model based on AIC
best_model.base.gba <- get.models(models.base.gba, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.gba) ##Cultivar is significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##PDE
###Checking the normality using Shapiro-Wilk test###
shapiro_result.pde <- shapiro.test(Field_ID.2$PDE) #W = 0.95431, p-value = 0.05951

###Double check with qqp###

pde <- Field_ID.2$PDE + 1 

gamma.pde <- fitdistr(pde, "gamma")
qqp(pde, "gamma", shape = gamma.pde$estimate[[1]], rate = gamma.pde$estimate[[2]])

qqp(pde, "lnorm")
qqp(pde, "norm")

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.pde <- glmmTMB(PDE ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.pde <- dredge(full_model.pde, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.pde)

##Select the best model based on AIC
best_model.pde <- get.models(models.pde, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.pde) ##No significance

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
pde.margins <- as.data.frame(marginal_effects(best_model.pde, type = "response"))
mean_est.pde <- c(mean(pde.margins$dydx_CultivarCZ4979X), mean(pde.margins$dydx_Developmental_StageR6), mean(pde.margins$dydx_Developmental_StageV6),
                  mean(pde.margins$dydx_TreatmentControl))
print(mean_est.pde)

pde.margins.2 <- pde.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
pde.margins.2$mean_est <- c(4.9541717, 5.0687262, 5.2187454, 0.1041743)

pde.margins.2$metric <- 'PDE'

##Print model output
tab_model(best_model.pde)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.pde.1.cult <- powerSim(best_model.pde, fixed("Cultivar"))
power.sim.pde.1.cult

power.sim.pde.1.trt <- powerSim(best_model.pde, fixed("Treatment"))
power.sim.pde.1.trt

power.sim.pde.1.ds <- powerSim(best_model.pde, fixed("Developmental_Stage"))
power.sim.pde.1.ds

##Decompose R2 to get contribution of each fixed effect
pde.r2 <- glmm.hp(best_model.pde, type = "adjR2", commonality = FALSE) 

print(pde.r2)
plot.glmmhp(pde.r2)

pde.margins.2$delta <- c(51.08, 48.92, 48.92, 0.00)

##Assess model performance
standardized.resid.pde <- resid(best_model.pde, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.pde <- sum(resid(best_model.pde, type = "pearson")^2)
1 - pchisq(dat.resid.pde, df.residual(best_model.pde))

# Assessing Deviance (G2)
deviance_value.pde <- 1 - pchisq(as.numeric(-2 * logLik(best_model.pde)), df.residual(best_model.pde))

# Simulating datasets
dat.sim.pde <- simulate(best_model.pde, nsim = 250)

# Empirical cumulative density function calculations
resid.list.pde <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.pde)) {
    e.pde <- ecdf(dat.sim.pde[[i]] + runif(length(dat.sim.pde[[i]]), -0.5, 0.5))
    resid.list.pde[[i]] <- e.pde(resid(best_model.pde) + runif(length(resid(best_model.pde)), -0.5, 0.5))
    plot(e.pde, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.pde <- do.call(c, resid.list.pde)

# Quantile-quantile plot
qqnorm(all.resid.pde, main = "QQ Plot")
qqline(all.resid.pde, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.pde), standardized.resid.pde, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline PDE
shapiro_result.base.pde <- shapiro.test(Field_ID.3$PDE) #W = 0.97685, p-value = 0.9337

###Double check with qqp###
base.pde <- Field_ID.3$PDE + 1

#nbinom.base.pde <- fitdistr(base.pde, "Negative Binomial")
#qqp(base.pde, "nbinom", size = nbinom.base.pde$estimate[[1]], mu = nbinom.base.pde$estimate[[2]])

gamma.base.pde <- fitdistr(base.pde, "gamma")
qqp(base.pde, "gamma", shape = gamma.base.pde$estimate[[1]], rate = gamma.base.pde$estimate[[2]])

qqp(base.pde, "lnorm")
qqp(base.pde, "norm")

###Will use a normal distribution
##Full model
full_model.base.pde <- glmmTMB(PDE ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.pde <- dredge(full_model.base.pde, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.pde)

##Select the best model based on AIC
best_model.base.pde <- get.models(models.base.pde, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.pde) ##Treatment is significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##NAG
###Checking the normality using Shapiro-Wilk test###
shapiro_result.nag <- shapiro.test(Field_ID.2$NAG) #W = 0.94733, p-value = 0.03134

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.nag$p.value < 0.05) {
distributions <- c("norm","lnorm", "gamma", "nbinom", "pois", "binom") ##will also check norm since barely significant
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Field_ID.2$NAG, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}  

###Double check with qqp###

nag <- Field_ID.2$NAG + 1 

gamma.nag <- fitdistr(nag, "gamma")
qqp(nag, "gamma", shape = gamma.nag$estimate[[1]], rate = gamma.nag$estimate[[2]])

qqp(nag, "lnorm")
qqp(nag, "norm")

##test says lnorm, but qqp suggests norm; thus, will use norm

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.nag <- glmmTMB(NAG ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.nag <- dredge(full_model.nag, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.nag)

##Select the best model based on AIC
best_model.nag <- get.models(models.nag, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.nag) ##Cultivar is significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
nag.margins <- as.data.frame(marginal_effects(best_model.nag, type = "response"))
mean_est.nag <- c(mean(nag.margins$dydx_CultivarCZ4979X), mean(nag.margins$dydx_Developmental_StageR6), mean(nag.margins$dydx_Developmental_StageV6),
                  mean(nag.margins$dydx_TreatmentControl))
print(mean_est.nag)

nag.margins.2 <- nag.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
nag.margins.2$mean_est <- c(3.8583382,  0.3437262, -0.2312649, -2.1833266)

nag.margins.2$metric <- 'NAG'

##Print model output
tab_model(best_model.nag)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.nag.1.cult <- powerSim(best_model.nag, fixed("Cultivar"))
power.sim.nag.1.cult

power.sim.nag.1.trt <- powerSim(best_model.nag, fixed("Treatment"))
power.sim.nag.1.trt

power.sim.nag.1.ds <- powerSim(best_model.nag, fixed("Developmental_Stage"))
power.sim.nag.1.ds

##Decompose R2 to get contribution of each fixed effect
nag.r2 <- glmm.hp(best_model.nag, type = "adjR2", commonality = FALSE) 

print(nag.r2)
plot.glmmhp(nag.r2)

nag.margins.2$delta <- c(74.87, 1.16, 1.16, 23.97)

##Assess model performance
standardized.resid.nag <- resid(best_model.nag, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.nag <- sum(resid(best_model.nag, type = "pearson")^2)
1 - pchisq(dat.resid.nag, df.residual(best_model.nag))

# Assessing Deviance (G2)
deviance_value.nag <- 1 - pchisq(as.numeric(-2 * logLik(best_model.nag)), df.residual(best_model.nag))

# Simulating datasets
dat.sim.nag <- simulate(best_model.nag, nsim = 250)

# Empirical cumulative density function calculations
resid.list.nag <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.nag)) {
    e.nag <- ecdf(dat.sim.nag[[i]] + runif(length(dat.sim.nag[[i]]), -0.5, 0.5))
    resid.list.nag[[i]] <- e.nag(resid(best_model.nag) + runif(length(resid(best_model.nag)), -0.5, 0.5))
    plot(e.nag, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.nag <- do.call(c, resid.list.nag)

# Quantile-quantile plot
qqnorm(all.resid.nag, main = "QQ Plot")
qqline(all.resid.nag, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.nag), standardized.resid.nag, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline NAG
shapiro_result.base.nag <- shapiro.test(Field_ID.3$NAG) #W = 0.97987, p-value = 0.9626

###Double check with qqp###
base.nag <- Field_ID.3$NAG + 1

#nbinom.base.nag <- fitdistr(base.nag, "Negative Binomial")
#qqp(base.nag, "nbinom", size = nbinom.base.nag$estimate[[1]], mu = nbinom.base.nag$estimate[[2]])

gamma.base.nag <- fitdistr(base.nag, "gamma")
qqp(base.nag, "gamma", shape = gamma.base.nag$estimate[[1]], rate = gamma.base.nag$estimate[[2]])

qqp(base.nag, "lnorm")
qqp(base.nag, "norm")

###Will use a normal distribution
##Full model
full_model.base.nag <- glmmTMB(NAG ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.nag <- dredge(full_model.base.nag, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.nag)

##Select the best model based on AIC
best_model.base.nag <- get.models(models.base.nag, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.nag)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##ACP
###Checking the normality using Shapiro-Wilk test###
shapiro_result.acp <- shapiro.test(Field_ID.2$ACP) #W = 0.97522, p-value = 0.3984

###Double check with qqp###

acp <- Field_ID.2$ACP + 1 

gamma.acp <- fitdistr(acp, "gamma")
qqp(acp, "gamma", shape = gamma.acp$estimate[[1]], rate = gamma.acp$estimate[[2]])

qqp(acp, "lnorm")
qqp(acp, "norm")

##test says lnorm, but qqp suggests norm; thus, will use norm

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.acp <- glmmTMB(ACP ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.acp <- dredge(full_model.acp, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.acp)

##Select the best model based on AIC
best_model.acp <- get.models(models.acp, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.acp) ##No significance

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
acp.margins <- as.data.frame(marginal_effects(best_model.acp, type = "response"))
mean_est.acp <- c(mean(acp.margins$dydx_CultivarCZ4979X), mean(acp.margins$dydx_Developmental_StageR6), mean(acp.margins$dydx_Developmental_StageV6),
                  mean(acp.margins$dydx_TreatmentControl))
print(mean_est.acp)

acp.margins.2 <- acp.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
acp.margins.2$mean_est <- c(5.937694,  7.393851, -5.112395,  7.628028)

acp.margins.2$metric <- 'ACP'

##Print model output
tab_model(best_model.acp)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.acp.1.cult <- powerSim(best_model.acp, fixed("Cultivar"))
power.sim.acp.1.cult

power.sim.acp.1.trt <- powerSim(best_model.acp, fixed("Treatment"))
power.sim.acp.1.trt

power.sim.acp.1.ds <- powerSim(best_model.acp, fixed("Developmental_Stage"))
power.sim.acp.1.ds

##Decompose R2 to get contribution of each fixed effect
acp.r2 <- glmm.hp(best_model.acp, type = "adjR2", commonality = FALSE) 

print(acp.r2)
plot.glmmhp(acp.r2)

acp.margins.2$delta <- c(17.67, 53.01, 53.01, 29.32)

##Assess model performance
standardized.resid.acp <- resid(best_model.acp, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.acp <- sum(resid(best_model.acp, type = "pearson")^2)
1 - pchisq(dat.resid.acp, df.residual(best_model.acp))

# Assessing Deviance (G2)
deviance_value.acp <- 1 - pchisq(as.numeric(-2 * logLik(best_model.acp)), df.residual(best_model.acp))

# Simulating datasets
dat.sim.acp <- simulate(best_model.acp, nsim = 250)

# Empirical cumulative density function calculations
resid.list.acp <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.acp)) {
    e.acp <- ecdf(dat.sim.acp[[i]] + runif(length(dat.sim.acp[[i]]), -0.5, 0.5))
    resid.list.acp[[i]] <- e.acp(resid(best_model.acp) + runif(length(resid(best_model.acp)), -0.5, 0.5))
    plot(e.acp, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.acp <- do.call(c, resid.list.acp)

# Quantile-quantile plot
qqnorm(all.resid.acp, main = "QQ Plot")
qqline(all.resid.acp, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.acp), standardized.resid.acp, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline ACP
shapiro_result.base.acp <- shapiro.test(Field_ID.3$ACP) #W = 0.90527, p-value = 0.09763

###Double check with qqp###
base.acp <- Field_ID.3$ACP + 1

#nbinom.base.acp <- fitdistr(base.acp, "Negative Binomial")
#qqp(base.acp, "nbinom", size = nbinom.base.acp$estimate[[1]], mu = nbinom.base.acp$estimate[[2]])

gamma.base.acp <- fitdistr(base.acp, "gamma")
qqp(base.acp, "gamma", shape = gamma.base.acp$estimate[[1]], rate = gamma.base.acp$estimate[[2]])

qqp(base.acp, "lnorm")
qqp(base.acp, "norm")

###Will use a normal distribution
##Full model
full_model.base.acp <- glmmTMB(ACP ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.acp <- dredge(full_model.base.acp, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.acp)

##Select the best model based on AIC
best_model.base.acp <- get.models(models.base.acp, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.acp) ##Treatment and interaction are significant

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##ALP
###Checking the normality using Shapiro-Wilk test###
shapiro_result.alp <- shapiro.test(Field_ID.2$ALP) #W = 0.95381, p-value = 0.05681

###Double check with qqp###

alp <- Field_ID.2$ALP + 1 

gamma.alp <- fitdistr(alp, "gamma")
qqp(alp, "gamma", shape = gamma.alp$estimate[[1]], rate = gamma.alp$estimate[[2]])

qqp(alp, "lnorm")
qqp(alp, "norm")

##test says lnorm, but qqp suggests norm; thus, will use norm

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.alp <- glmmTMB(ALP ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.alp <- dredge(full_model.alp, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.alp)

##Select the best model based on AIC
best_model.alp <- get.models(models.alp, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.alp) ##Cultivar and Developmental Stage are significant

###Marginalize the coefficients- not necessary but will not change the output; will do for consistency###
alp.margins <- as.data.frame(marginal_effects(best_model.alp, type = "response"))
mean_est.alp <- c(mean(alp.margins$dydx_CultivarCZ4979X), mean(alp.margins$dydx_Developmental_StageR6), mean(alp.margins$dydx_Developmental_StageV6),
                  mean(alp.margins$dydx_TreatmentControl))
print(mean_est.alp)

alp.margins.2 <- alp.margins %>%
  rownames_to_column(var = "Predictor") %>%
  gather(key = "Variable", value = "marg_est", -Predictor) %>%
  distinct(Variable, .keep_all = TRUE)

##Add mean marginal estimate
alp.margins.2$mean_est <- c(6.237489,  4.425000,  6.312499, -1.637501)

alp.margins.2$metric <- 'ALP'

##Print model output
tab_model(best_model.alp)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.alp.1.cult <- powerSim(best_model.alp, fixed("Cultivar"))
power.sim.alp.1.cult

power.sim.alp.1.trt <- powerSim(best_model.alp, fixed("Treatment"))
power.sim.alp.1.trt

power.sim.alp.1.ds <- powerSim(best_model.alp, fixed("Developmental_Stage"))
power.sim.alp.1.ds

##Decompose R2 to get contribution of each fixed effect
alp.r2 <- glmm.hp(best_model.alp, type = "adjR2", commonality = FALSE) 

print(alp.r2)
plot.glmmhp(alp.r2)

alp.margins.2$delta <- c(55.91, 40.24, 40.24, 3.86)

##Assess model performance
standardized.resid.alp <- resid(best_model.alp, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.alp <- sum(resid(best_model.alp, type = "pearson")^2)
1 - pchisq(dat.resid.alp, df.residual(best_model.alp))

# Assessing Deviance (G2)
deviance_value.alp <- 1 - pchisq(as.numeric(-2 * logLik(best_model.alp)), df.residual(best_model.alp))

# Simulating datasets
dat.sim.alp <- simulate(best_model.alp, nsim = 250)

# Empirical cumulative density function calculations
resid.list.alp <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.alp)) {
    e.alp <- ecdf(dat.sim.alp[[i]] + runif(length(dat.sim.alp[[i]]), -0.5, 0.5))
    resid.list.alp[[i]] <- e.alp(resid(best_model.alp) + runif(length(resid(best_model.alp)), -0.5, 0.5))
    plot(e.alp, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.alp <- do.call(c, resid.list.alp)

# Quantile-quantile plot
qqnorm(all.resid.alp, main = "QQ Plot")
qqline(all.resid.alp, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.alp), standardized.resid.alp, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline ALP
shapiro_result.base.alp <- shapiro.test(Field_ID.3$ALP) #W = 0.93488, p-value = 0.291

###Double check with qqp###
base.alp <- Field_ID.3$ALP + 1

#nbinom.base.alp <- fitdistr(base.alp, "Negative Binomial")
#qqp(base.alp, "nbinom", size = nbinom.base.alp$estimate[[1]], mu = nbinom.base.alp$estimate[[2]])

gamma.base.alp <- fitdistr(base.alp, "gamma")
qqp(base.alp, "gamma", shape = gamma.base.alp$estimate[[1]], rate = gamma.base.alp$estimate[[2]])

qqp(base.alp, "lnorm")
qqp(base.alp, "norm")

###Will use a normal distribution
##Full model
full_model.base.alp <- glmmTMB(ALP ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.alp <- dredge(full_model.base.alp, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.alp)

##Select the best model based on AIC
best_model.base.alp <- get.models(models.base.alp, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.alp)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##ARS
###Checking the normality using Shapiro-Wilk test###
shapiro_result.ars <- shapiro.test(Field_ID.2$ARS) #W = 0.96468, p-value = 0.1559

###Double check with qqp###

ars <- Field_ID.2$ARS + 1 

gamma.ars <- fitdistr(ars, "gamma")
qqp(ars, "gamma", shape = gamma.ars$estimate[[1]], rate = gamma.ars$estimate[[2]])

qqp(ars, "lnorm")
qqp(ars, "norm")

##test says lnorm, but qqp suggests norm; thus, will use norm

###Constructing the GLMM with stepwise selection###
# Fit the full model with norm distribution
full_model.ars <- glmmTMB(ARS ~ (Treatment + Developmental_Stage + Cultivar)^3 + (1 | Location),
                        data = Field_ID.2, family = gaussian(), na.action = "na.fail")

##Perform model selection using dredge
models.ars <- dredge(full_model.ars, m.min = 1, fixed = c("cond(Cultivar)", "cond(Developmental_Stage)", "cond(Treatment)"))

##Print all models
print(models.ars)

##Select the best model based on AIC
best_model.ars <- get.models(models.ars, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.ars) ##Cultivar, Developmental Stage, and Cultivar:Treatment are significant

###Get estimates from model output
ars.margins <- data.frame(
  Variable = c("dydx_CultivarCZ4979X", "dydx_Developmental_StageR6", "dydx_Developmental_StageV6",
               "dydx_TreatmentControl", "CultivarCZ4979X:TreatmentControl"),
  mean_est = c(9.758, 11.831, -2.213, 5.900, -12.217))

ars.margins$metric <- 'ARS'

##Print model output
tab_model(best_model.ars)

###Conduct Power Analysis###
##library(simr)
##https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504 
##https://humburg.github.io/Power-Analysis/simr_power_analysis.html 
##https://meera.seas.umich.edu/power-analysis-statistical-significance-effect-size.html 

##powerSim: estimate power by simulation
power.sim.ars.1.cult <- powerSim(best_model.ars, fixed("Cultivar"))

power.sim.ars.1.cult #30.10%

power.sim.ars.1.trt <- powerSim(best_model.ars, fixed("Treatment"))

power.sim.ars.1.trt #30.10%

power.sim.ars.1.ds <- powerSim(best_model.ars, fixed("Developmental_Stage"))

power.sim.ars.1.ds #30.10%


##Decompose R2 to get contribution of each fixed effect
##Must create second model with interactions modeled as fixed effects
ars.interaction <- glmmTMB(ARS ~ Treatment + Developmental_Stage + Cultivar +
                            Treatment_Cultivar +(1 | Location), 
                             data = Field_ID.2, family = gaussian(), na.action = "na.fail") ##Triggered convergence warning


ars.r2 <- glmm.hp(ars.interaction, type = "adjR2", commonality = FALSE) 

print(ars.r2)
plot.glmmhp(ars.r2)

summary(best_model.ars)

ars.margins$delta <- c(3.29, 74.99, 74.99, 0.00, 21.72)

##Assess model performance
standardized.resid.ars <- resid(best_model.ars, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.ars <- sum(resid(best_model.ars, type = "pearson")^2)
1 - pchisq(dat.resid.ars, df.residual(best_model.ars))

# Assessing Deviance (G2)
deviance_value.ars <- 1 - pchisq(as.numeric(-2 * logLik(best_model.ars)), df.residual(best_model.ars))

# Simulating datasets
dat.sim.ars <- simulate(best_model.ars, nsim = 250)

# Empirical cumulative density function calculations
resid.list.ars <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.ars)) {
    e.ars <- ecdf(dat.sim.ars[[i]] + runif(length(dat.sim.ars[[i]]), -0.5, 0.5))
    resid.list.ars[[i]] <- e.ars(resid(best_model.ars) + runif(length(resid(best_model.ars)), -0.5, 0.5))
    plot(e.ars, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.ars <- do.call(c, resid.list.ars)

# Quantile-quantile plot
qqnorm(all.resid.ars, main = "QQ Plot")
qqline(all.resid.ars, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.ars), standardized.resid.ars, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

```{r}
##Baseline ARS
shapiro_result.base.ars <- shapiro.test(Field_ID.3$ARS) #W = 0.96278, p-value = 0.7125

###Double check with qqp###
base.ars <- Field_ID.3$ARS + 1

#nbinom.base.ars <- fitdistr(base.ars, "Negative Binomial")
#qqp(base.ars, "nbinom", size = nbinom.base.ars$estimate[[1]], mu = nbinom.base.ars$estimate[[2]])

gamma.base.ars <- fitdistr(base.ars, "gamma")
qqp(base.ars, "gamma", shape = gamma.base.ars$estimate[[1]], rate = gamma.base.ars$estimate[[2]])

qqp(base.ars, "lnorm")
qqp(base.ars, "norm")

###Will use a normal distribution
##Full model
full_model.base.ars <- glmmTMB(ARS ~ (Treatment + Cultivar)^2 + (1 | Location),
                          data = Field_ID.3,
                          family = gaussian(link = "identity"),
                          na.action = "na.fail")

##Model selection
models.base.ars <- dredge(full_model.base.ars, m.min = 1, fixed = c("cond(Cultivar)","cond(Treatment)"))

##Print all models
#print(models.base.ars)

##Select the best model based on AIC
best_model.base.ars <- get.models(models.base.ars, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.base.ars)

##No need to do additional assessment; goal is to see if any statistical significance in baseline
```

```{r}
##Let's visualize the edaphic glmms
##First, add dummy columns to dataframes containing manually extracted estimates. This will facilitate rbind
##Add 'Predictor' and 'marg_est' columns filled with NA to 'ars.margins'
ars.margins$Predictor <- NA
ars.margins$marg_est <- NA

##Add 'Predictor' and 'marg_est' columns filled with NA to 'som.margins'
som.margins$Predictor <- NA
som.margins$marg_est <- NA

##Add 'Predictor' and 'marg_est' columns filled with NA to 'k.margins'
k.margins$Predictor <- NA
k.margins$marg_est <- NA

edaphic.glm <- rbind(no3.margins.2, p.margins.2, k.margins, ca.margins.2, mg.margins.2,
                     s.margins.2, b.margins.2, cu.margins.2, fe.margins.2, mn.margins.2,
                     zn.margins.2, na.margins.2, ph.margins.2, buf.margins.2, som.margins,
                     cec.margins.2, kmg.margins.2, camg.margins.2, gba.margins.2, pde.margins.2,
                     nag.margins.2, acp.margins.2, alp.margins.2, ars.margins)

##Rename predictors
edaphic.glm$Variable[edaphic.glm$Variable == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
edaphic.glm$Variable[edaphic.glm$Variable  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'
edaphic.glm$Variable[edaphic.glm$Variable  == 'dydx_Developmental_StageV6'] <- 'V6 vs R2'
edaphic.glm$Variable[edaphic.glm$Variable  == 'dydx_Developmental_StageR6'] <- 'R6 vs R2'
edaphic.glm$Variable[edaphic.glm$Variable  == 'dydx_Developmental_StageR6'] <- 'R6 vs R2'
edaphic.glm$Variable[edaphic.glm$Variable  == 'CultivarCZ4979X:Developmental_StageR6'] <- 'CZ4979X:R6'
edaphic.glm$Variable[edaphic.glm$Variable  == 'CultivarCZ4979X:Developmental_StageV6'] <- 'CZ4979X:V6'
edaphic.glm$Variable[edaphic.glm$Variable  == 'CultivarCZ4979X:TreatmentControl'] <- 'CZ4979X:Control'
edaphic.glm$Variable[edaphic.glm$Variable  == 'CultivarCZ4979X:Developmental_StageR6'] <- 'CZ4979X:R6'
edaphic.glm$Variable[edaphic.glm$Variable  == 'CultivarCZ4979X:Developmental_StageV6'] <- 'CZ4979X:V6'
edaphic.glm$Variable[edaphic.glm$Variable  == 'CultivarCZ4979X:TreatmentControl'] <- 'CZ4979X:Control'

ggplot(edaphic.glm, aes(x = Variable, y = mean_est)) +
  geom_col(width = 0.3, fill = "#a890a8", colour = "black") +
  geom_point(shape = 21, fill = "#a890a8", colour = "black", aes(size = delta)) +
  scale_size(range = c(3, 8)) +
  geom_hline(yintercept = 0, linetype = "dashed", lwd = 1) +
  facet_wrap(~metric, scales = "free_x", nrow = 4) +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 10, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 12, face = 'bold'),
        axis.text.x = element_text(size = 10, color = "black"),
        strip.text = element_text(size = 12, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 10),
        legend.position = 'none')

ggsave("Edaphic.glm.2.tiff", width = 15, height = 8, units = "in", dpi = 900)

##Summary data for manuscript
# Calculate Mean + SEM for 'delta' grouped by 'metric' and 'Variable'
delta_summary_metric <- edaphic.glm %>%
  group_by(metric) %>%
  summarise(
    delta_mean = mean(delta, na.rm = TRUE),
    delta_SEM = sd(delta, na.rm = TRUE) / sqrt(n())
  )

delta_summary_variable <- edaphic.glm %>%
  group_by(Variable) %>%
  summarise(
    delta_mean = mean(delta, na.rm = TRUE),
    delta_SEM = sd(delta, na.rm = TRUE) / sqrt(n())
  )

# Calculate Mean + SEM for 'mean_est' grouped by 'metric' and 'Variable'
mean_est_summary_metric <- edaphic.glm %>%
  group_by(metric) %>%
  summarise(
    mean_est_mean = mean(mean_est, na.rm = TRUE),
    mean_est_SEM = sd(mean_est, na.rm = TRUE) / sqrt(n())
  )

mean_est_summary_variable <- edaphic.glm %>%
  group_by(Variable) %>%
  summarise(
    mean_est_mean = mean(mean_est, na.rm = TRUE),
    mean_est_SEM = sd(mean_est, na.rm = TRUE) / sqrt(n())
  )
```

```{r}
##Let's look for a correlation between nutrients
library(corrplot)
library(Hmisc)

columns_of_interest <- Field_ID.3[, 8:31]

# Step 3: Calculate the Spearman correlation matrix
correlation_matrix.nut <- rcorr(as.matrix(columns_of_interest), type = "spearman")

corrplot(correlation_matrix.nut$r, p.mat = correlation_matrix.nut$P, method = 'color', diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
         insig = 'label_sig', pch.col = 'black', order = 'AOE', outline = TRUE, tl.col = "black",
         tl.cex = 0.5, col=colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10))


##Find number of positive and negative correlations
##Subset the correlations where p-value is less than 0.05
significant_correlations <- na.omit(correlation_matrix.nut$r[correlation_matrix.nut$P < 0.05])

##Count the number of positive and negative correlations
positive_correlations_count <- sum(significant_correlations > 0, na.rm = TRUE)/2
negative_correlations_count <- sum(significant_correlations < 0, na.rm = TRUE)/2

##For total count, must divide by 2, as each correlation is represented twice
cat("Total number of statistically significant correlations:", length(significant_correlations)/2, "\n")
cat("Number of positive correlations:", positive_correlations_count, "\n")
cat("Number of negative correlations:", negative_correlations_count, "\n")

##Which nutrients have the most? 
# Compute the number of significant associations for each nutrient
significant_counts <- apply(correlation_matrix.nut$P < 0.05, 1, function(x) sum(x, na.rm = TRUE))

# Identify the nutrient(s) with the highest number of significant associations
max_significant_count <- max(significant_counts, na.rm = TRUE)
top_nutrients <- names(significant_counts[significant_counts == max_significant_count])

# For the identified nutrient(s), count the number of positive and negative significant correlations
get_signs <- function(nutrient) {
  significant_correlations <- correlation_matrix.nut$r[nutrient, ][correlation_matrix.nut$P[nutrient, ] < 0.05]
  
  list(
    positive = sum(significant_correlations > 0, na.rm = TRUE),
    negative = sum(significant_correlations < 0, na.rm = TRUE)
  )
}

results <- lapply(top_nutrients, get_signs)

# Combine and present the results
nut.top <- data.frame(
  nutrient = rep(top_nutrients, each = length(top_nutrients)),
  total_significant = max_significant_count,
  positive = unlist(sapply(results, `[[`, 'positive')),
  negative = unlist(sapply(results, `[[`, 'negative'))
)
```

```{r}
##Phona for edaphic characteristics
library(PhONA)
##Will use filtered_obj
view(sample_data(filtered_obj))

##Need to update the phyloseq object so that all edaphic measurements are in the sample data
edaphic.otu <- as.data.frame(otu_table(filtered_obj))

##Need to remove genera not found in co-occurrence matrix
##Make rownames a column
edaphic.otu$genus <- rownames(edaphic.otu)

##Remove rows if genus is not found in correlation table
allowed_values.edaphic <- unique(c(all.pairs.3$X, all.pairs.3$Y))

##Subset based on matching values in the genus column
edaphic.otu.filtered <- edaphic.otu[edaphic.otu$genus %in% allowed_values.edaphic, ]

##Filter taxa to those found in at least 4 samples- not doing this at this point; will filter after network construction
#nonzero_mat.edaphic <- edaphic.otu.filtered != 0
#nonzero_counts.edaphic <- rowSums(nonzero_mat.edaphic)
#num_columns.edaphic <- ncol(edaphic.otu.filtered)
#edaphic.otu.filtered <- edaphic.otu.filtered[nonzero_counts.edaphic >= 4, ]

##Remove genus column
edaphic.otu.filtered <- edaphic.otu.filtered[, -which(names(edaphic.otu.filtered) == "genus")]

##Convert back to phyloseq object##
edaphic.otu.1 <- otu_table(edaphic.otu.filtered, taxa_are_rows = TRUE)

##Now for tax table
edaphic.tax <- as.data.frame(tax_table(filtered_obj))
edaphic.tax.filtered <- edaphic.tax[edaphic.tax$genus %in% allowed_values.edaphic, ]
edaphic.tax.filtered <- edaphic.tax.filtered[rownames(edaphic.tax.filtered) %in% rownames(edaphic.otu.filtered), ]

##Generate random character strings
n <- nrow(edaphic.tax.filtered)
random_species <- replicate(n, paste0(sample(letters, 5, replace = TRUE), collapse = ""))
##Add the "Species" column to the dataframe
edaphic.tax.filtered$Species <- random_species

##Capitalize column names
colnames(edaphic.tax.filtered) <- paste0(toupper(substr(colnames(edaphic.tax.filtered), 1, 1)), substr(colnames(edaphic.tax.filtered), 2, nchar(colnames(edaphic.tax.filtered))))

edaphic.tax.1 <- as.matrix(edaphic.tax.filtered)
edaphic.tax.2 <- tax_table(edaphic.tax.1)

edaphic.sample <- sample_data(Field_ID.3)
rownames(edaphic.sample) <- paste("sample", 1:nrow(edaphic.sample), sep = "_")

##Create edaphic phyloseq object
edaphic.phona <- phyloseq(edaphic.otu.1, edaphic.tax.2, edaphic.sample)

##Now, further modify to meet phona expectations
set.seed(7)
edaphic.phona.phyobj = taxacolor(phyobj = edaphic.phona, coloredby = "Kingdom")

##Modify colors
edaphic.tax.3 <- as.data.frame(tax_table(edaphic.phona.phyobj))

edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#A8A5D9'] <- '#a8a890'
edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#D3D2B6'] <- '#c07848'
edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#BD5BD2'] <- '#a890a8'
edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#DD7A7C'] <- '#c0c0c0'
edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#80E37E'] <- '#a8c0a8'
edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#85DBD2'] <- '#d8c0a8'
edaphic.tax.3$Kingdom_color[edaphic.tax.3$Kingdom_color == '#DBD75D'] <- '#786060'

##Convert back to taxtable and create phyloseq 
edaphic.tax.3 <- as.matrix(edaphic.tax.3)
edaphic.tax.4 <- tax_table(edaphic.tax.3)
edaphic.phona.phyobj <- phyloseq(edaphic.otu.1, edaphic.tax.4, edaphic.sample)

##Now, let's create the correlation matrices
##Will use all.pairs.3 since we are using the entire dataset
unique_values.edaphic <- unique(c(all.pairs.3$X, all.pairs.3$Y))
##Now filter to those found in edaphic.tax.3
edaphic.tax.3 <- as.data.frame(edaphic.tax.3)
unique_values.edaphic <- unique_values.edaphic[unique_values.edaphic %in% edaphic.tax.3$Genus]

##Create an empty matrix with dimensions equal to the length of unique values
matrix_size <- length(unique_values.edaphic)
matrix_result <- matrix(0, nrow = matrix_size, ncol = matrix_size)

##Set row and column names of the matrix
rownames(matrix_result) <- unique_values.edaphic
colnames(matrix_result) <- unique_values.edaphic

##Fill the matrix with corresponding values from all.pairs.3$rho
for (i in 1:nrow(all.pairs.3)) {
  x <- match(all.pairs.3$X[i], unique_values.edaphic)
  y <- match(all.pairs.3$Y[i], unique_values.edaphic)
  matrix_result[x, y] <- all.pairs.3$rho[i]
}

##Replace NA values with 0
matrix_result[is.na(matrix_result)] <- 0

##Fill diagonal elements with 1
diag(matrix_result) <- 1

##Convert the matrix to a square matrix
core_matrix.edaphic <- as.matrix(matrix_result)

##Now for the p-value matrix
##same as above

##Create an empty matrix with dimensions equal to the length of unique values
matrix_size <- length(unique_values.edaphic)
matrix_result <- matrix(0, nrow = matrix_size, ncol = matrix_size)

##Set row and column names of the matrix
rownames(matrix_result) <- unique_values.edaphic
colnames(matrix_result) <- unique_values.edaphic

##Fill the matrix with corresponding values from all.pairs.3$rho
for (i in 1:nrow(all.pairs.3)) {
  x <- match(all.pairs.3$X[i], unique_values.edaphic)
  y <- match(all.pairs.3$Y[i], unique_values.edaphic)
  matrix_result[x, y] <- all.pairs.3$p[i]
}

##Replace NA values with 1
matrix_result[is.na(matrix_result)] <- 1

##Fill diagonal elements with 0
diag(matrix_result) <- 0

##Convert the matrix to a square matrix
p_matrix.edaphic <- as.matrix(matrix_result)
```

```{r}
#PhONA for NO3
phona.no3 <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "NO3",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "NO3",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.no3)

phona.no3.sum <- as.data.frame(summarizePhONA(phona.no3$phona_graph, phona.no3$roles))
phona.no3.role <- as.data.frame(phona.no3$roles)
# Create the Domain column based on the Kingdom column
phona.no3.role$Domain <- ifelse(phona.no3.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.no3.role$metric <- "NO3"

##Add hub score
phona.no3.hub <- as.data.frame(hub_score(phona.no3$phona_graph))

phona.no3.hub$name <- rownames(phona.no3.hub)

##Merge hub and role by 'name'
phona.no3.role.2 <- merge(phona.no3.hub, phona.no3.role, by = c('name'))

##Clean up visualization
layout.no3 <- layout.sphere(phona.no3$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.no3 <- names(V(phona.no3$phona_graph))
print(vertex_names.no3)

vertex_names.no3.2 <- vertex_names.no3[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.no3 <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.no3.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.no3 <- intermediate_df.no3[match(vertex_names.no3.2, intermediate_df.no3$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.no3 <- intermediate_df.no3$Kingdom_color
ordered_color_list.no3 <- c(ordered_color_list.no3)

plot(phona.no3$phona_graph, layout = layout.no3, edge.color = phona.edge.2, vertex.color = ordered_color_list.no3, 
      vertex.size = 12, vertex.label = "")
title_text <- "NO3"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for P
phona.p <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "P",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "P",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.p)

phona.p.sum <- as.data.frame(summarizePhONA(phona.p$phona_graph, phona.p$roles))
phona.p.role <- as.data.frame(phona.p$roles)
# Create the Domain column based on the Kingdom column
phona.p.role$Domain <- ifelse(phona.p.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.p.role$metric <- "P"

##Add hub score
phona.p.hub <- as.data.frame(hub_score(phona.p$phona_graph))

phona.p.hub$name <- rownames(phona.p.hub)

##Merge hub and role by 'name'
phona.p.role.2 <- merge(phona.p.hub, phona.p.role, by = c('name'))

##Clean up visualization
layout.p <- layout.sphere(phona.p$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.p <- names(V(phona.p$phona_graph))
print(vertex_names.p)

vertex_names.p.2 <- vertex_names.p[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.p <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.p.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.p <- intermediate_df.p[match(vertex_names.p.2, intermediate_df.p$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.p <- intermediate_df.p$Kingdom_color
ordered_color_list.p <- c(ordered_color_list.p)

plot(phona.p$phona_graph, layout = layout.p, edge.color = phona.edge.2, vertex.color = ordered_color_list.p, 
      vertex.size = 12, vertex.label = "")
title_text <- "P"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for K
phona.k <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "K",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "K",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.k)

phona.k.sum <- as.data.frame(summarizePhONA(phona.k$phona_graph, phona.k$roles))
phona.k.role <- as.data.frame(phona.k$roles)
# Create the Domain column based on the Kingdom column
phona.k.role$Domain <- ifelse(phona.k.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.k.role$metric <- "K"

##Add hub score
phona.k.hub <- as.data.frame(hub_score(phona.k$phona_graph))

phona.k.hub$name <- rownames(phona.k.hub)

##Merge hub and role by 'name'
phona.k.role.2 <- merge(phona.k.hub, phona.k.role, by = c('name'))

##Clean up visualization
layout.k <- layout.sphere(phona.k$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.k <- names(V(phona.k$phona_graph))
print(vertex_names.k)

vertex_names.k.2 <- vertex_names.k[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.k <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.k.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.k <- intermediate_df.k[match(vertex_names.k.2, intermediate_df.k$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.k <- intermediate_df.k$Kingdom_color
ordered_color_list.k <- c(ordered_color_list.k)

plot(phona.k$phona_graph, layout = layout.k, edge.color = phona.edge.2, vertex.color = ordered_color_list.k, 
      vertex.size = 12, vertex.label = "")
title_text <- "K"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Ca
phona.ca <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Ca",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Ca",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.ca)

phona.ca.sum <- as.data.frame(summarizePhONA(phona.ca$phona_graph, phona.ca$roles))
phona.ca.role <- as.data.frame(phona.ca$roles)
# Create the Domain column based on the Kingdom column
phona.ca.role$Domain <- ifelse(phona.ca.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.ca.role$metric <- "Ca"

##Add hub score
phona.ca.hub <- as.data.frame(hub_score(phona.ca$phona_graph))

phona.ca.hub$name <- rownames(phona.ca.hub)

##Merge hub and role by 'name'
phona.ca.role.2 <- merge(phona.ca.hub, phona.ca.role, by = c('name'))

##Clean up visualization
layout.ca <- layout.sphere(phona.ca$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.ca <- names(V(phona.ca$phona_graph))
print(vertex_names.ca)

vertex_names.ca.2 <- vertex_names.ca[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.ca <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.ca.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.ca <- intermediate_df.ca[match(vertex_names.ca.2, intermediate_df.ca$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.ca <- intermediate_df.ca$Kingdom_color
ordered_color_list.ca <- c(ordered_color_list.ca)

plot(phona.ca$phona_graph, layout = layout.ca, edge.color = phona.edge.2, vertex.color = ordered_color_list.ca, 
      vertex.size = 12, vertex.label = "")
title_text <- "Ca"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Mg
phona.mg <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Mg",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Mg",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.mg)

phona.mg.sum <- as.data.frame(summarizePhONA(phona.mg$phona_graph, phona.mg$roles))
phona.mg.role <- as.data.frame(phona.mg$roles)
# Create the Domain column based on the Kingdom column
phona.mg.role$Domain <- ifelse(phona.mg.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.mg.role$metric <- "Mg"

##Add hub score
phona.mg.hub <- as.data.frame(hub_score(phona.mg$phona_graph))

phona.mg.hub$name <- rownames(phona.mg.hub)

##Merge hub and role by 'name'
phona.mg.role.2 <- merge(phona.mg.hub, phona.mg.role, by = c('name'))

##Clean up visualization
layout.mg <- layout.sphere(phona.mg$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.mg <- names(V(phona.mg$phona_graph))
print(vertex_names.mg)

vertex_names.mg.2 <- vertex_names.mg[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.mg <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.mg.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.mg <- intermediate_df.mg[match(vertex_names.mg.2, intermediate_df.mg$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.mg <- intermediate_df.mg$Kingdom_color
ordered_color_list.mg <- c(ordered_color_list.mg)

plot(phona.mg$phona_graph, layout = layout.mg, edge.color = phona.edge.2, vertex.color = ordered_color_list.mg, 
      vertex.size = 12, vertex.label = "")
title_text <- "Mg"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for S- used lm
phona.s <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lm",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "S",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "S",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.s)

phona.s.sum <- as.data.frame(summarizePhONA(phona.s$phona_graph, phona.s$roles))
phona.s.role <- as.data.frame(phona.s$roles)
# Create the Domain column based on the Kingdom column
phona.s.role$Domain <- ifelse(phona.s.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.s.role$metric <- "S"

##Add hub score
phona.s.hub <- as.data.frame(hub_score(phona.s$phona_graph))

phona.s.hub$name <- rownames(phona.s.hub)

##Merge hub and role by 'name'
phona.s.role.2 <- merge(phona.s.hub, phona.s.role, by = c('name'))

##Clean up visualization
layout.s <- layout.sphere(phona.s$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.s <- names(V(phona.s$phona_graph))
print(vertex_names.s)

vertex_names.s.2 <- vertex_names.s[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.s <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.s.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.s <- intermediate_df.s[match(vertex_names.s.2, intermediate_df.s$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.s <- intermediate_df.s$Kingdom_color
ordered_color_list.s <- c(ordered_color_list.s)

plot(phona.s$phona_graph, layout = layout.s, edge.color = phona.edge.2, vertex.color = ordered_color_list.s, 
      vertex.size = 12, vertex.label = "")
title_text <- "S"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for B
phona.b <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "B",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "B",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.b)

phona.b.sum <- as.data.frame(summarizePhONA(phona.b$phona_graph, phona.b$roles))
phona.b.role <- as.data.frame(phona.b$roles)
# Create the Domain column based on the Kingdom column
phona.b.role$Domain <- ifelse(phona.b.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.b.role$metric <- "B"

##Add hub score
phona.b.hub <- as.data.frame(hub_score(phona.b$phona_graph))

phona.b.hub$name <- rownames(phona.b.hub)

##Merge hub and role by 'name'
phona.b.role.2 <- merge(phona.b.hub, phona.b.role, by = c('name'))

##Clean up visualization
layout.b <- layout.sphere(phona.b$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.b <- names(V(phona.b$phona_graph))
print(vertex_names.b)

vertex_names.b.2 <- vertex_names.b [-1]

# Filter tax.data.2 based on genus_order
intermediate_df.b <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.b.2 , ]

# Order the intermediate dataframe based on genus_order
intermediate_df.b <- intermediate_df.b[match(vertex_names.b.2 , intermediate_df.b$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.b <- intermediate_df.b$Kingdom_color
ordered_color_list.b <- c(ordered_color_list.b)

plot(phona.b$phona_graph, layout = layout.b, edge.color = phona.edge.2, vertex.color = ordered_color_list.b, 
      vertex.size = 12, vertex.label = "")
title_text <- "B"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Cu
phona.cu <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Cu",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Cu",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.cu)

phona.cu.sum <- as.data.frame(summarizePhONA(phona.cu$phona_graph, phona.cu$roles))
phona.cu.role <- as.data.frame(phona.cu$roles)
# Create the Domain column based on the Kingdom column
phona.cu.role$Domain <- ifelse(phona.cu.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.cu.role$metric <- "Cu"

##Add hub score
phona.cu.hub <- as.data.frame(hub_score(phona.cu$phona_graph))

phona.cu.hub$name <- rownames(phona.cu.hub)

##Merge hub and role by 'name'
phona.cu.role.2 <- merge(phona.cu.hub, phona.cu.role, by = c('name'))

##Clean up visualization
layout.cu <- layout.sphere(phona.cu$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.cu <- names(V(phona.cu$phona_graph))
print(vertex_names.cu)

vertex_names.cu.2 <- vertex_names.cu[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.cu <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.cu.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.cu <- intermediate_df.cu[match(vertex_names.cu.2, intermediate_df.cu$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.cu <- intermediate_df.cu$Kingdom_color
ordered_color_list.cu <- c(ordered_color_list.cu)

plot(phona.cu$phona_graph, layout = layout.cu, edge.color = phona.edge.2, vertex.color = ordered_color_list.cu, 
      vertex.size = 12, vertex.label = "")
title_text <- "Cu"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Fe
phona.fe <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Fe",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Fe",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.fe)

phona.fe.sum <- as.data.frame(summarizePhONA(phona.fe$phona_graph, phona.fe$roles))
phona.fe.role <- as.data.frame(phona.fe$roles)
# Create the Domain column based on the Kingdom column
phona.fe.role$Domain <- ifelse(phona.fe.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.fe.role$metric <- "Fe"

##Add hub score
phona.fe.hub <- as.data.frame(hub_score(phona.fe$phona_graph))

phona.fe.hub$name <- rownames(phona.fe.hub)

##Merge hub and role by 'name'
phona.fe.role.2 <- merge(phona.fe.hub, phona.fe.role, by = c('name'))

##Clean up visualization
layout.fe <- layout.sphere(phona.fe$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.fe <- names(V(phona.fe$phona_graph))
print(vertex_names.fe)

vertex_names.fe.2 <- vertex_names.fe[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.fe <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.fe.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.fe <- intermediate_df.fe[match(vertex_names.fe.2, intermediate_df.fe$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.fe <- intermediate_df.fe$Kingdom_color
ordered_color_list.fe <- c(ordered_color_list.fe)

plot(phona.fe$phona_graph, layout = layout.fe, edge.color = phona.edge.2, vertex.color = ordered_color_list.fe, 
      vertex.size = 12, vertex.label = "")
title_text <- "Fe"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Mn
phona.mn <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Mn",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Mn",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.mn)

phona.mn.sum <- as.data.frame(summarizePhONA(phona.mn$phona_graph, phona.mn$roles))
phona.mn.role <- as.data.frame(phona.mn$roles)
# Create the Domain column based on the Kingdom column
phona.mn.role$Domain <- ifelse(phona.mn.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.mn.role$metric <- "Mn"

##Add hub score
phona.mn.hub <- as.data.frame(hub_score(phona.mn$phona_graph))

phona.mn.hub$name <- rownames(phona.mn.hub)

##Merge hub and role by 'name'
phona.mn.role.2 <- merge(phona.mn.hub, phona.mn.role, by = c('name'))

##Clean up visualization
layout.mn <- layout.sphere(phona.mn$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.mn <- names(V(phona.mn$phona_graph))
print(vertex_names.mn)

vertex_names.mn.2 <- vertex_names.mn[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.mn <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.mn.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.mn <- intermediate_df.mn[match(vertex_names.mn.2, intermediate_df.mn$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.mn <- intermediate_df.mn$Kingdom_color
ordered_color_list.mn <- c(ordered_color_list.mn)

plot(phona.mn$phona_graph, layout = layout.mn, edge.color = phona.edge.2, vertex.color = ordered_color_list.mn, 
      vertex.size = 12, vertex.label = "")
title_text <- "Mn"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)

```

```{r}
#PhONA for Zn- lm
phona.zn <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lm",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Zn",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Zn",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.zn)

phona.zn.sum <- as.data.frame(summarizePhONA(phona.zn$phona_graph, phona.zn$roles))
phona.zn.role <- as.data.frame(phona.zn$roles)
# Create the Domain column based on the Kingdom column
phona.zn.role$Domain <- ifelse(phona.zn.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.zn.role$metric <- "Zn"

##Add hub score
phona.zn.hub <- as.data.frame(hub_score(phona.zn$phona_graph))

phona.zn.hub$name <- rownames(phona.zn.hub)

##Merge hub and role by 'name'
phona.zn.role.2 <- merge(phona.zn.hub, phona.zn.role, by = c('name'))

##Clean up visualization
layout.zn <- layout.sphere(phona.zn$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.zn <- names(V(phona.zn$phona_graph))
print(vertex_names.zn)

vertex_names.zn.2 <- vertex_names.zn[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.zn <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.zn.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.zn <- intermediate_df.zn[match(vertex_names.zn.2, intermediate_df.zn$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.zn <- intermediate_df.zn$Kingdom_color
ordered_color_list.zn <- c(ordered_color_list.zn)

plot(phona.zn$phona_graph, layout = layout.zn, edge.color = phona.edge.2, vertex.color = ordered_color_list.zn, 
      vertex.size = 12, vertex.label = "")
title_text <- "Zn"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Na
phona.na <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Na",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Na",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.na)

phona.na.sum <- as.data.frame(summarizePhONA(phona.na$phona_graph, phona.na$roles))
phona.na.role <- as.data.frame(phona.na$roles)
# Create the Domain column based on the Kingdom column
phona.na.role$Domain <- ifelse(phona.na.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.na.role$metric <- "Na"

##Add hub score
phona.na.hub <- as.data.frame(hub_score(phona.na$phona_graph))

phona.na.hub$name <- rownames(phona.na.hub)

##Merge hub and role by 'name'
phona.na.role.2 <- merge(phona.na.hub, phona.na.role, by = c('name'))

##Clean up visualization
layout.na <- layout.sphere(phona.na$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.na <- names(V(phona.na$phona_graph))
print(vertex_names.na)

vertex_names.na.2 <- vertex_names.na[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.na <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.na.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.na <- intermediate_df.na[match(vertex_names.na.2, intermediate_df.na$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.na <- intermediate_df.na$Kingdom_color
ordered_color_list.na <- c(ordered_color_list.na)

plot(phona.na$phona_graph, layout = layout.na, edge.color = phona.edge.2, vertex.color = ordered_color_list.na, 
      vertex.size = 12, vertex.label = "")
title_text <- "Na"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for soil ph
phona.ph <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Soil pH",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Soil pH",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.ph)

phona.ph.sum <- as.data.frame(summarizePhONA(phona.ph$phona_graph, phona.ph$roles))
phona.ph.role <- as.data.frame(phona.ph$roles)
# Create the Domain column based on the Kingdom column
phona.ph.role$Domain <- ifelse(phona.ph.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.ph.role$metric <- "Soil pH"

##Add hub score
phona.ph.hub <- as.data.frame(hub_score(phona.ph$phona_graph))

phona.ph.hub$name <- rownames(phona.ph.hub)

##Merge hub and role by 'name'
phona.ph.role.2 <- merge(phona.ph.hub, phona.ph.role, by = c('name'))

##Clean up visualization
layout.ph <- layout.sphere(phona.ph$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.ph <- names(V(phona.ph$phona_graph))
print(vertex_names.ph)

vertex_names.ph.2 <- vertex_names.ph[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.ph <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.ph.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.ph <- intermediate_df.ph[match(vertex_names.ph.2, intermediate_df.ph$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.ph <- intermediate_df.ph$Kingdom_color
ordered_color_list.ph <- c(ordered_color_list.ph)

plot(phona.ph$phona_graph, layout = layout.ph, edge.color = phona.edge.2, vertex.color = ordered_color_list.ph, 
      vertex.size = 12, vertex.label = "")
title_text <- "Soil pH"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for buffer ph- must use lm
phona.buf <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lm",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Buffer pH",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Buffer pH",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.buf)

phona.buf.sum <- as.data.frame(summarizePhONA(phona.buf$phona_graph, phona.buf$roles))
phona.buf.role <- as.data.frame(phona.buf$roles)
# Create the Domain column based on the Kingdom column
phona.buf.role$Domain <- ifelse(phona.buf.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.buf.role$metric <- "Buffer pH"

##Add hub score
phona.buf.hub <- as.data.frame(hub_score(phona.buf$phona_graph))

phona.buf.hub$name <- rownames(phona.buf.hub)

##Merge hub and role by 'name'
phona.buf.role.2 <- merge(phona.buf.hub, phona.buf.role, by = c('name'))

##Clean up visualization
layout.buf <- layout.sphere(phona.buf$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.buf <- names(V(phona.buf$phona_graph))
print(vertex_names.buf)

vertex_names.buf.2 <- vertex_names.buf[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.buf <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.buf.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.buf <- intermediate_df.buf[match(vertex_names.buf.2, intermediate_df.buf$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.buf <- intermediate_df.buf$Kingdom_color
ordered_color_list.buf <- c(ordered_color_list.buf)

plot(phona.buf$phona_graph, layout = layout.buf, edge.color = phona.edge.2, vertex.color = ordered_color_list.buf, 
      vertex.size = 12, vertex.label = "")
title_text <- "Buffer pH"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for SOM
phona.som <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "SOM",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "SOM",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.som)

phona.som.sum <- as.data.frame(summarizePhONA(phona.som$phona_graph, phona.som$roles))
phona.som.role <- as.data.frame(phona.som$roles)
# Create the Domain column based on the Kingdom column
phona.som.role$Domain <- ifelse(phona.som.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.som.role$metric <- "SOM"

##Add hub score
phona.som.hub <- as.data.frame(hub_score(phona.som$phona_graph))

phona.som.hub$name <- rownames(phona.som.hub)

##Merge hub and role by 'name'
phona.som.role.2 <- merge(phona.som.hub, phona.som.role, by = c('name'))

##Clean up visualization
layout.som <- layout.sphere(phona.som$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.som <- names(V(phona.som$phona_graph))
print(vertex_names.som)

vertex_names.som.2 <- vertex_names.som[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.som <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.som.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.som <- intermediate_df.som[match(vertex_names.som.2, intermediate_df.som$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.som <- intermediate_df.som$Kingdom_color
ordered_color_list.som <- c(ordered_color_list.som)

plot(phona.som$phona_graph, layout = layout.som, edge.color = phona.edge.2, vertex.color = ordered_color_list.som, 
      vertex.size = 12, vertex.label = "")
title_text <- "SOM"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for CEC
phona.cec <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "CEC",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "CEC",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.cec)

phona.cec.sum <- as.data.frame(summarizePhONA(phona.cec$phona_graph, phona.cec$roles))
phona.cec.role <- as.data.frame(phona.cec$roles)
# Create the Domain column based on the Kingdom column
phona.cec.role$Domain <- ifelse(phona.cec.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.cec.role$metric <- "CEC"

##Add hub score
phona.cec.hub <- as.data.frame(hub_score(phona.cec$phona_graph))

phona.cec.hub$name <- rownames(phona.cec.hub)

##Merge hub and role by 'name'
phona.cec.role.2 <- merge(phona.cec.hub, phona.cec.role, by = c('name'))

##Clean up visualization
layout.cec <- layout.sphere(phona.cec$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.cec <- names(V(phona.cec$phona_graph))
print(vertex_names.cec)

vertex_names.cec.2 <- vertex_names.cec[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.cec <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.cec.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.cec <- intermediate_df.cec[match(vertex_names.cec.2, intermediate_df.cec$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.cec <- intermediate_df.cec$Kingdom_color
ordered_color_list.cec <- c(ordered_color_list.cec)

plot(phona.cec$phona_graph, layout = layout.cec, edge.color = phona.edge.2, vertex.color = ordered_color_list.cec, 
      vertex.size = 12, vertex.label = "")
title_text <- "CEC"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for K/Mg
phona.kmg <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "K/Mg",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "K/Mg",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.kmg)

phona.kmg.sum <- as.data.frame(summarizePhONA(phona.kmg$phona_graph, phona.kmg$roles))
phona.kmg.role <- as.data.frame(phona.kmg$roles)
# Create the Domain column based on the Kingdom column
phona.kmg.role$Domain <- ifelse(phona.kmg.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.kmg.role$metric <- "K/Mg"

##Add hub score
phona.kmg.hub <- as.data.frame(hub_score(phona.kmg$phona_graph))

phona.kmg.hub$name <- rownames(phona.kmg.hub)

##Merge hub and role by 'name'
phona.kmg.role.2 <- merge(phona.kmg.hub, phona.kmg.role, by = c('name'))

##Clean up visualization
layout.kmg <- layout.sphere(phona.kmg$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.kmg <- names(V(phona.kmg$phona_graph))
print(vertex_names.kmg)

vertex_names.kmg.2 <- vertex_names.kmg[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.kmg <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.kmg.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.kmg <- intermediate_df.kmg[match(vertex_names.kmg.2, intermediate_df.kmg$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.kmg <- intermediate_df.kmg$Kingdom_color
ordered_color_list.kmg <- c(ordered_color_list.kmg)

plot(phona.kmg$phona_graph, layout = layout.kmg, edge.color = phona.edge.2, vertex.color = ordered_color_list.kmg, 
      vertex.size = 12, vertex.label = "")
title_text <- "K/Mg"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for Ca/Mg- had to use lm
phona.camg <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lm",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Ca/Mg",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Ca/Mg",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.camg)

phona.camg.sum <- as.data.frame(summarizePhONA(phona.camg$phona_graph, phona.camg$roles))
phona.camg.role <- as.data.frame(phona.camg$roles)
# Create the Domain column based on the Kingdom column
phona.camg.role$Domain <- ifelse(phona.camg.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.camg.role$metric <- "Ca/Mg"

##Add hub score
phona.camg.hub <- as.data.frame(hub_score(phona.camg$phona_graph))

phona.camg.hub$name <- rownames(phona.camg.hub)

##Merge hub and role by 'name'
phona.camg.role.2 <- merge(phona.camg.hub, phona.camg.role, by = c('name'))

##Clean up visualization
layout.camg <- layout.sphere(phona.camg$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.camg <- names(V(phona.camg$phona_graph))
print(vertex_names.camg)

vertex_names.camg.2 <- vertex_names.camg[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.camg <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.camg.2 , ]

# Order the intermediate dataframe based on genus_order
intermediate_df.camg <- intermediate_df.camg[match(vertex_names.camg.2 , intermediate_df.camg$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.camg <- intermediate_df.camg$Kingdom_color
ordered_color_list.camg <- c(ordered_color_list.camg)

plot(phona.camg$phona_graph, layout = layout.camg, edge.color = phona.edge.2, vertex.color = ordered_color_list.camg, 
      vertex.size = 12, vertex.label = "")
title_text <- "Ca/Mg"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for GBA3
phona.gba <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "GBA3",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "GBA3",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.gba)

phona.gba.sum <- as.data.frame(summarizePhONA(phona.gba$phona_graph, phona.gba$roles))
phona.gba.role <- as.data.frame(phona.gba$roles)
# Create the Domain column based on the Kingdom column
phona.gba.role$Domain <- ifelse(phona.gba.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.gba.role$metric <- "GBA3"

##Add hub score
phona.gba.hub <- as.data.frame(hub_score(phona.gba$phona_graph))

phona.gba.hub$name <- rownames(phona.gba.hub)

##Merge hub and role by 'name'
phona.gba.role.2 <- merge(phona.gba.hub, phona.gba.role, by = c('name'))

##Clean up visualization
layout.gba <- layout.sphere(phona.gba$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.gba <- names(V(phona.gba$phona_graph))
print(vertex_names.gba)

vertex_names.gba.2 <- vertex_names.gba[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.gba <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.gba.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.gba <- intermediate_df.gba[match(vertex_names.gba.2, intermediate_df.gba$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.gba <- intermediate_df.gba$Kingdom_color
ordered_color_list.gba <- c(ordered_color_list.gba)

plot(phona.gba$phona_graph, layout = layout.gba, edge.color = phona.edge.2, vertex.color = ordered_color_list.gba, 
      vertex.size = 12, vertex.label = "")
title_text <- "GBA3"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for PDE
phona.pde <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "PDE",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "PDE",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.pde)

phona.pde.sum <- as.data.frame(summarizePhONA(phona.pde$phona_graph, phona.pde$roles))
phona.pde.role <- as.data.frame(phona.pde$roles)
# Create the Domain column based on the Kingdom column
phona.pde.role$Domain <- ifelse(phona.pde.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.pde.role$metric <- "PDE"

##Add hub score
phona.pde.hub <- as.data.frame(hub_score(phona.pde$phona_graph))

phona.pde.hub$name <- rownames(phona.pde.hub)

##Merge hub and role by 'name'
phona.pde.role.2 <- merge(phona.pde.hub, phona.pde.role, by = c('name'))

##Clean up visualization
layout.pde <- layout.sphere(phona.pde$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.pde <- names(V(phona.pde$phona_graph))
print(vertex_names.pde)

vertex_names.pde.2 <- vertex_names.pde[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.pde <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.pde.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.pde <- intermediate_df.pde[match(vertex_names.pde.2, intermediate_df.pde$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.pde <- intermediate_df.pde$Kingdom_color
ordered_color_list.pde <- c(ordered_color_list.pde)

plot(phona.pde$phona_graph, layout = layout.pde, edge.color = phona.edge.2, vertex.color = ordered_color_list.pde, 
      vertex.size = 12, vertex.label = "")
title_text <- "PDE"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for NAG
phona.nag <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "NAG",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "NAG",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.nag)

phona.nag.sum <- as.data.frame(summarizePhONA(phona.nag$phona_graph, phona.nag$roles))
phona.nag.role <- as.data.frame(phona.nag$roles)
# Create the Domain column based on the Kingdom column
phona.nag.role$Domain <- ifelse(phona.nag.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.nag.role$metric <- "NAG"

##Add hub score
phona.nag.hub <- as.data.frame(hub_score(phona.nag$phona_graph))

phona.nag.hub$name <- rownames(phona.nag.hub)

##Merge hub and role by 'name'
phona.nag.role.2 <- merge(phona.nag.hub, phona.nag.role, by = c('name'))

##Clean up visualization
layout.nag <- layout.sphere(phona.nag$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.nag <- names(V(phona.nag$phona_graph))
print(vertex_names.nag)

vertex_names.nag.2 <- vertex_names.nag[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.nag <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.nag.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.nag <- intermediate_df.nag[match(vertex_names.nag.2, intermediate_df.nag$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.nag <- intermediate_df.nag$Kingdom_color
ordered_color_list.nag <- c(ordered_color_list.nag)

plot(phona.nag$phona_graph, layout = layout.nag, edge.color = phona.edge.2, vertex.color = ordered_color_list.nag, 
      vertex.size = 12, vertex.label = "")
title_text <- "NAG"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for ACP
phona.acp <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "ACP",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "ACP",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.acp)

phona.acp.sum <- as.data.frame(summarizePhONA(phona.acp$phona_graph, phona.acp$roles))
phona.acp.role <- as.data.frame(phona.acp$roles)
# Create the Domain column based on the Kingdom column
phona.acp.role$Domain <- ifelse(phona.acp.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.acp.role$metric <- "ACP"

##Add hub score
phona.acp.hub <- as.data.frame(hub_score(phona.acp$phona_graph))

phona.acp.hub$name <- rownames(phona.acp.hub)

##Merge hub and role by 'name'
phona.acp.role.2 <- merge(phona.acp.hub, phona.acp.role, by = c('name'))

##Clean up visualization
layout.acp <- layout.sphere(phona.acp$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.acp <- names(V(phona.acp$phona_graph))
print(vertex_names.acp)

vertex_names.acp.2 <- vertex_names.acp[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.acp <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.acp.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.acp <- intermediate_df.acp[match(vertex_names.acp.2, intermediate_df.acp$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.acp <- intermediate_df.acp$Kingdom_color
ordered_color_list.acp <- c(ordered_color_list.acp)

plot(phona.acp$phona_graph, layout = layout.acp, edge.color = phona.edge.2, vertex.color = ordered_color_list.acp, 
      vertex.size = 12, vertex.label = "")
title_text <- "ACP"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for ALP
phona.alp <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "ALP",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "ALP",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.alp)

phona.alp.sum <- as.data.frame(summarizePhONA(phona.alp$phona_graph, phona.alp$roles))
phona.alp.role <- as.data.frame(phona.alp$roles)
# Create the Domain column based on the Kingdom column
phona.alp.role$Domain <- ifelse(phona.alp.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.alp.role$metric <- "ALP"

##Add hub score
phona.alp.hub <- as.data.frame(hub_score(phona.alp$phona_graph))

phona.alp.hub$name <- rownames(phona.alp.hub)

##Merge hub and role by 'name'
phona.alp.role.2 <- merge(phona.alp.hub, phona.alp.role, by = c('name'))

##Clean up visualization
layout.alp <- layout.sphere(phona.alp$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.alp <- names(V(phona.alp$phona_graph))
print(vertex_names.alp)

vertex_names.alp.2 <- vertex_names.alp[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.alp <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.alp.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.alp <- intermediate_df.alp[match(vertex_names.alp.2, intermediate_df.alp$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.alp <- intermediate_df.alp$Kingdom_color
ordered_color_list.alp <- c(ordered_color_list.alp)

plot(phona.alp$phona_graph, layout = layout.alp, edge.color = phona.edge.2, vertex.color = ordered_color_list.alp, 
      vertex.size = 12, vertex.label = "")
title_text <- "ALP"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
#PhONA for ARS
phona.ars <- PhONA(
  physeqobj = edaphic.phona.phyobj,
  model = "lasso",
  cordata = core_matrix.edaphic,
  pdata = p_matrix.edaphic,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "ARS",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "ARS",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.ars)

phona.ars.sum <- as.data.frame(summarizePhONA(phona.ars$phona_graph, phona.ars$roles))
phona.ars.role <- as.data.frame(phona.ars$roles)
# Create the Domain column based on the Kingdom column
phona.ars.role$Domain <- ifelse(phona.ars.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.ars.role$metric <- "ARS"

##Add hub score
phona.ars.hub <- as.data.frame(hub_score(phona.ars$phona_graph))

phona.ars.hub$name <- rownames(phona.ars.hub)

##Merge hub and role by 'name'
phona.ars.role.2 <- merge(phona.ars.hub, phona.ars.role, by = c('name'))

##Clean up visualization
layout.ars <- layout.sphere(phona.ars$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.ars <- names(V(phona.ars$phona_graph))
print(vertex_names.ars)

vertex_names.ars.2 <- vertex_names.ars[-1]

# Filter tax.data.2 based on genus_order
intermediate_df.ars <- edaphic.tax[edaphic.tax$Genus %in% vertex_names.ars.2, ]

# Order the intermediate dataframe based on genus_order
intermediate_df.ars <- intermediate_df.ars[match(vertex_names.ars.2, intermediate_df.ars$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.ars <- intermediate_df.ars$Kingdom_color
ordered_color_list.ars <- c(ordered_color_list.ars)

plot(phona.ars$phona_graph, layout = layout.ars, edge.color = phona.edge.2, vertex.color = ordered_color_list.ars, 
      vertex.size = 12, vertex.label = "")
title_text <- "ARS"
title_col <- "black"
title_line <- 0
title_offset <- 2
mtext(title_text, side = 3, line = title_line, col = title_col, font = 2, cex = 1.5)
```

```{r}
phona.edaphic.heat <- rbind(phona.no3.role.2, phona.p.role.2, phona.k.role.2, phona.ca.role.2, phona.mg.role.2, phona.s.role.2,
                            phona.fe.role.2, phona.cu.role.2, phona.mn.role.2, phona.b.role.2, phona.zn.role.2, phona.na.role.2,
                            phona.ph.role.2, phona.buf.role.2, phona.som.role.2, phona.cec.role.2, phona.kmg.role.2, phona.camg.role.2,
                            phona.alp.role.2, phona.acp.role.2, phona.pde.role.2, phona.nag.role.2, phona.ars.role.2, phona.gba.role.2)


##Create composite score based on Kleinberg's Hub Score (vector), modularity, and participation coefficient
##Normalizing the metrics
phona.edaphic.heat$normalized_vector <- (phona.edaphic.heat$vector - min(phona.edaphic.heat$vector)) / (max(phona.edaphic.heat$vector) - min(phona.edaphic.heat$vector))

phona.edaphic.heat$normalized_module <- (phona.edaphic.heat$module - min(phona.edaphic.heat$module)) / (max(phona.edaphic.heat$module) - min(phona.edaphic.heat$module))

phona.edaphic.heat$normalized_participation <- (phona.edaphic.heat$participation - min(phona.edaphic.heat$participation)) / (max(phona.edaphic.heat$participation) - min(phona.edaphic.heat$participation))

##Calculating the composite score
phona.edaphic.heat$composite_score <- (phona.edaphic.heat$normalized_vector + phona.edaphic.heat$normalized_module + phona.edaphic.heat$normalized_participation) / 3

phona.edaphic.heat.2 <- acast(data = phona.edaphic.heat, formula = name ~ metric, value.var = "composite_score")

phona.edaphic.heat.t <- t(phona.edaphic.heat.2)

phona_palette <- colorRamp2(c(0, 1), c("antiquewhite", "lightblue3")) ##, transparency = 0.5) 

lgd_list.phona <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12),title = "Composite Score",
        border = "black", lwd = 1,  
        title_position = "topcenter",
        at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0), legend_width = unit(10, "cm"),
        direction = "horizontal",
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"))
        #labels = c("","Hub node", "", "", "", ""))

##Create column annotation
phona.edaphic.col <- unique(phona.edaphic.heat[c("name", "Domain")])

rownames(phona.edaphic.col) <- phona.edaphic.col$name
phona.edaphic.col$name <- NULL
phona.edaphic.col <- phona.edaphic.col[order(row.names(phona.edaphic.col)), , drop = FALSE]

colours.phona <- list('Domain' = c('Prokaryote' = "#a8c0a8", 'Eukaryote' = "#a890a8"))

colAnn.edaphic.phona <- HeatmapAnnotation(df = phona.edaphic.col,
  which = 'col',
  col = colours.phona,
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), simple_anno_size = unit(0.2, "cm"), show_legend = FALSE, show_annotation_name = FALSE)

##Let's create right annotation
edaphic.row.anno <- phona.edaphic.heat %>%
  group_by(metric) %>%
  summarise(Prokaryote = sum(Domain == "Prokaryote"),
            Eukaryote = sum(Domain == "Eukaryote"))

##Right annotation only considering non-zero particpation scores- don't need any longer
#edaphic.row.anno <- phona.edaphic.heat %>%
  #filter(participation != 0) %>%
  #group_by(metric) %>%
  #summarise(Prokaryote = sum(Domain == "Prokaryote"),
            #Eukaryote = sum(Domain == "Eukaryote"))

edaphic.row.anno <- as.data.frame(edaphic.row.anno)
rownames(edaphic.row.anno) <- edaphic.row.anno[, 1]
edaphic.row.anno <- edaphic.row.anno[, -1]

row_order <- rownames(phona.edaphic.heat.t)

##Reorder the rows of edaphic.row.anno according to row_order
edaphic.row.anno_reordered <- edaphic.row.anno[row_order, ]

edaphic.row.anno.right = HeatmapAnnotation("# unique obs"= anno_barplot(edaphic.row.anno_reordered, gp = gpar(fill = c("#a8c0a8","#a890a8"))), 
                               which = c("row"), show_annotation_name = FALSE)

##Let's create top annotation
edaphic.col.anno <- as.data.frame(phona.edaphic.heat.2)
edaphic.col.anno.2 <- edaphic.col.anno %>%
  rowwise() %>%
  mutate(mean.composite = mean(c_across(everything()), na.rm = TRUE))  ##Mean composite score

##add back rownames
rownames(edaphic.col.anno.2) <- rownames(edaphic.col.anno)


##keep just composite score
edaphic.col.anno.3 <- edaphic.col.anno.2[, -c(1:24)]
rownames(edaphic.col.anno.3) <- rownames(edaphic.col.anno)

edaphic.col.stack = HeatmapAnnotation("# net"= anno_barplot(edaphic.col.anno.3, axis_param = list(side = "left"), 
                      gp = gpar(fill = c("#8c3800"), lwd = 0)), 
                       which = c("column"),   show_annotation_name = FALSE)

edaphic.heat.1 <- Heatmap(phona.edaphic.heat.t, cluster_columns = TRUE, cluster_rows = TRUE, na_col = "gray57",
        border = TRUE, col=phona_palette,
       column_names_gp = gpar(fontface = "italic", fontsize = 6),
       heatmap_legend_param = lgd_list.phona,
       right_annotation = edaphic.row.anno.right,
       bottom_annotation = colAnn.edaphic.phona,
       top_annotation = edaphic.col.stack,
       column_split = phona.edaphic.col$Domain,
       row_gap = unit(c(2), "mm"),
       row_title = NULL,
       column_names_rot = 45, 
       row_names_gp = gpar(fontsize = 10))

##second version without column names
edaphic.heat.1 <- Heatmap(
  phona.edaphic.heat.t,
  cluster_columns = TRUE,
  cluster_rows = TRUE,
  na_col = "gray57",
  border = TRUE,
  #rect_gp = gpar(col = "black", lwd = 0.5),
  col = phona_palette,
  show_column_names = FALSE,
  heatmap_legend_param = lgd_list.phona,
  right_annotation = edaphic.row.anno.right,
  bottom_annotation = colAnn.edaphic.phona,
  top_annotation = edaphic.col.stack,
  column_split = phona.edaphic.col$Domain,
  row_gap = unit(c(2), "mm"),
  row_title = NULL,
  column_title = NULL,
  column_names_rot = 45,
  row_names_gp = gpar(fontsize = 10))

edaphic.heat.2 <- draw(edaphic.heat.1, heatmap_legend_side = "bottom", legend_grouping = "original")

tiff("phona.edaphic.heatmap.2.tiff", width = 12, height = 8, units = "in", res = 900)
edaphic.heat.2
dev.off() 
```

```{r}
##Summary for manuscript
phona.edaphic.domain.summary <- phona.edaphic.heat

# Subset the dataframe
sub_df <- phona.edaphic.domain.summary[, c(1, 37)] %>%
  distinct()

# Count the occurrences of each domain
prokaryote_count <- sum(sub_df$Domain == "Prokaryote", na.rm = TRUE)
eukaryote_count <- sum(sub_df$Domain == "Eukaryote", na.rm = TRUE)

# Print the counts
print(paste("Number of Prokaryote:", prokaryote_count))
print(paste("Number of Eukaryote:", eukaryote_count))

##mean and SEM across networks
# Compute mean and SEM for Eukaryote
mean_eukaryote <- mean(edaphic.row.anno_reordered$Eukaryote, na.rm = TRUE)
sem_eukaryote <- sd(edaphic.row.anno_reordered$Eukaryote, na.rm = TRUE) / sqrt(sum(!is.na(edaphic.row.anno_reordered$Eukaryote)))

# Compute mean and SEM for Prokaryote
mean_prokaryote <- mean(edaphic.row.anno_reordered$Prokaryote, na.rm = TRUE)
sem_prokaryote <- sd(edaphic.row.anno_reordered$Prokaryote, na.rm = TRUE) / sqrt(sum(!is.na(edaphic.row.anno_reordered$Prokaryote)))

# Compute mean and SEM for combined data (sum of Eukaryote and Prokaryote for each row)
combined_data <- edaphic.row.anno_reordered$Eukaryote + edaphic.row.anno_reordered$Prokaryote
mean_combined <- mean(combined_data, na.rm = TRUE)
sem_combined <- sd(combined_data, na.rm = TRUE) / sqrt(sum(!is.na(combined_data)))

list(
  Eukaryote = list(mean = mean_eukaryote, SEM = sem_eukaryote),
  Prokaryote = list(mean = mean_prokaryote, SEM = sem_prokaryote),
  Combined = list(mean = mean_combined, SEM = sem_combined)
)

##Get edge count across all networks- phona.ars.sum
phona.edaphic.edge <- rbind(phona.no3.sum, phona.p.sum, phona.k.sum, phona.ca.sum, phona.mg.sum, phona.s.sum,
                            phona.fe.sum, phona.cu.sum, phona.mn.sum, phona.b.sum, phona.zn.sum, phona.na.sum,
                            phona.ph.sum, phona.buf.sum, phona.som.sum, phona.cec.sum, phona.kmg.sum, phona.camg.sum,
                            phona.alp.sum, phona.acp.sum, phona.pde.sum, phona.nag.sum, phona.ars.sum, phona.gba.sum)


mean_edge <- mean(phona.edaphic.edge$edge, na.rm = TRUE)
sem_edge <- sd(phona.edaphic.edge$edge, na.rm = TRUE) / sqrt(length(na.omit(phona.edaphic.edge$edge)))

results <- data.frame(
  Metric = c("Edge"),
  Mean = c(mean_edge),
  SEM = c(sem_edge)
)

print(results)
```

```{r}
##Let's look at edaphic phona members further
##First, let's get relative abundance data for all members used for phona analysis
##Will obtain separately for prokaryote and eukaryote

##Prokaryote##
phona.prok.genus <- conglomerate_taxa(prok_decontam_norm, "genus")
##RA
phona.prok.genus.ra <- microbiome::transform(phona.prok.genus, "compositional")
##Get needed info
phona.prok.genus.otu <- as.data.frame(otu_table(phona.prok.genus.ra))
phona.prok.genus.tax <- as.data.frame(tax_table(phona.prok.genus.ra))
##Merge them
merged_phona_prok_genus <- merge(phona.prok.genus.tax, phona.prok.genus.otu, by = "row.names", all = TRUE)
merged_phona_prok_genus <- merged_phona_prok_genus[, -1]
##Filter to only those used for phona analysis
##Step 1: Add the 'genus' column to edaphic.otu.filtered
edaphic.otu.filtered$genus <- rownames(edaphic.otu.filtered)
##Step 2: Merge edaphic.otu.filtered and merged_phona_prok_genus by the 'genus' column
merged_edaphic_prok_genus <- merge(edaphic.otu.filtered, merged_phona_prok_genus, by = 'genus')
##Filter columns
columns_to_remove <- grep("\\.x$", names(merged_edaphic_prok_genus))
merged_edaphic_prok_genus <- merged_edaphic_prok_genus[, -columns_to_remove]
##Clean up sample names
# Remove trailing .y from column names
names(merged_edaphic_prok_genus) <- gsub("\\.y$", "", names(merged_edaphic_prok_genus))

##Eukaryote##
phona.euk.genus <- conglomerate_taxa(fun_decontam_filter_norm, "genus")
##RA
phona.euk.genus.ra <- microbiome::transform(phona.euk.genus, "compositional")
##Get needed info
phona.euk.genus.otu <- as.data.frame(otu_table(phona.euk.genus.ra))
phona.euk.genus.tax <- as.data.frame(tax_table(phona.euk.genus.ra))
##Merge them
merged_phona_euk_genus <- merge(phona.euk.genus.tax, phona.euk.genus.otu, by = "row.names", all = TRUE)
merged_phona_euk_genus <- merged_phona_euk_genus[, -1]
##Filter to only those used for phona analysis
##Step 2: Merge edaphic.otu.filtered and merged_phona_euk_genus by the 'genus' column
merged_edaphic_euk_genus <- merge(edaphic.otu.filtered, merged_phona_euk_genus, by = 'genus')
##Filter columns
columns_to_remove <- grep("\\.x$", names(merged_edaphic_euk_genus))
merged_edaphic_euk_genus <- merged_edaphic_euk_genus[, -columns_to_remove]
##Clean up sample names
# Remove trailing .y from column names
names(merged_edaphic_euk_genus) <- gsub("\\.y$", "", names(merged_edaphic_euk_genus))

##Combine the two
edaphic.ra <- rbind(merged_edaphic_euk_genus, merged_edaphic_prok_genus)
edaphic.ra.2 <- edaphic.ra[, -c(2:6)]
rownames(edaphic.ra.2) <- edaphic.ra$genus
edaphic.ra.2$genus <- NULL
edaphic.ra.2.t <- t(edaphic.ra.2)

##Reorder rows
rownames_numeric <- as.numeric(sub("^sample_", "", rownames(edaphic.ra.2.t)))
edaphic.ra.2.t <- edaphic.ra.2.t[order(rownames_numeric), ]

##reorder columns
edaphic.ra.2.t <- edaphic.ra.2.t[, order(colnames(edaphic.ra.2.t))]

##row annotation- core.col.ann from previous heatmap
edaphic.ra.row <- core.col.ann
rownames(edaphic.ra.row) <- paste0("sample_", 1:64)
edaphic.ra.row$`Growth Stage` <- as.character(edaphic.ra.row$`Growth Stage`)
edaphic.ra.row$`Growth Stage`[edaphic.ra.row$`Growth Stage` == "R3"] <- "R2"
edaphic.ra.row$`Growth Stage` <- as.factor(edaphic.ra.row$`Growth Stage`)
new_order <- c("V1", "V6", "R2", "R6")
edaphic.ra.row$`Growth Stage` <- factor(edaphic.ra.row$`Growth Stage`, levels = new_order)

edaphic.ra.rowAnn <- HeatmapAnnotation(df = edaphic.ra.row,
  which = 'row',
  col = core.colours, ##from core members heatmap, can specify as wished
  annotation_width = unit(c(1, 2), 'cm'),
  gap = unit(0.25, 'mm'), show_annotation_name = FALSE, simple_anno_size = unit(0.2, "cm"))

##column annotation
##phona.edaphic.col from above code

##Top annotation- Composite score
##Can use some from earlier
edaphic.col.stack.ra = HeatmapAnnotation("# net"= anno_barplot(edaphic.col.anno.3, axis_param = list(side = "left"), 
                      gp = gpar(fill = c("lightblue3"), lwd = 0)), 
                       which = c("column"),   show_annotation_name = FALSE)

##color palette     
edaphic.ra.pal = colorRamp2(c(0, 0.05), c("antiquewhite", "#8c3800"))

##Legend
lgd_list.edaphic.ra <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12),title = "Relative Abundance",
        border = "black", lwd = 1,  
        title_position = "topcenter",
        at = c(0, 0.01, 0.02, 0.03, 0.04, 0.05), legend_width = unit(10, "cm"),
        direction = "horizontal",
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"))
        #labels = c("","Hub node", "", "", "", ""))

##Heatmap
edaphic.ra.heat <- Heatmap(
  edaphic.ra.2.t,
  cluster_columns = TRUE,
  cluster_rows = TRUE,
  na_col = "gray57",
  border = TRUE,
  #rect_gp = gpar(col = "black", lwd = 0.5),
  col = edaphic.ra.pal,
  show_column_names = FALSE,
  show_row_names = FALSE,
  heatmap_legend_param = lgd_list.edaphic.ra,
  right_annotation = edaphic.ra.rowAnn,
  bottom_annotation = colAnn.edaphic.phona, ##same as previous heatmap
  top_annotation = edaphic.col.stack.ra,
  column_split = phona.edaphic.col$Domain,##same as previous heatmap
  row_gap = unit(c(2), "mm"),
  row_title = NULL,
  column_title = NULL,
  column_names_rot = 45,
  row_names_gp = gpar(fontsize = 10))

edaphic.ra.heat.2 <- draw(edaphic.ra.heat, heatmap_legend_side = "bottom", legend_grouping = "original")
```

```{r}
##Let's pick some genera to look at more deeply
##To prioritize microbes, we will first get RA and participation in the same dataframe
#view(edaphic.col.anno.3)
#view(edaphic.ra.2)
edaphic.col.anno.3$RowNames <- rownames(edaphic.col.anno.3)
edaphic.ra.2$RowNames <- rownames(edaphic.ra.2)
merged_edaphic <- merge(edaphic.col.anno.3, edaphic.ra.2, by = "RowNames")
rownames(merged_edaphic) <- merged_edaphic$RowNames
merged_edaphic$RowNames <- NULL

##keep only the rows where
##there are nonzero values in at least 20% of samples
cols <- 2:65
## Filtering rows with non-zero values in at least 20% of columns 2-65
merged_edaphic.filtered <- merged_edaphic[rowSums(merged_edaphic[,cols] != 0) >= 0.2*length(cols), ]
##Selecting the top 20 rows by values in column mean.composite
merged_edaphic.2 <- merged_edaphic.filtered[order(merged_edaphic.filtered$mean.composite, decreasing = TRUE), ][1:20, ] %>%
slice_head(n = 20) 

##Let's look at the correlation between genera
##I want to use CSS-normalized counts instead of RA, since CSS was used for co-occurrence networks
edaphic.otu.filtered.2 <- edaphic.otu.filtered
edaphic.otu.filtered.2$genus <- NULL

##Get CSS for selected genera
merged_edaphic_rownames <- rownames(merged_edaphic.2)
edaphic_otu_rownames <- rownames(edaphic.otu.filtered.2)
## Filter edaphic.otu.filtered.2 to rows whose row names are present in merged_edaphic.2
filtered_edaphic_otu <- edaphic.otu.filtered.2[edaphic_otu_rownames %in% merged_edaphic_rownames, ]
##Transpose
genera.cor <- t(filtered_edaphic_otu)
##Correlation
edaphic.core <- rcorr(as.matrix(genera.cor), type = "spearman")
view(edaphic.core$r)

##Visualize
corrplot(edaphic.core$r, p.mat = edaphic.core$P, method = 'color', diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
         insig = 'label_sig', pch.col = 'black', order = 'AOE', outline = TRUE, tl.col = "black", tl.srt=45, font = 3, 
         tl.cex = 0.5, col=colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10))

##Identify trends
# Counting the number of significant associations for each genus
significant_counts <- rowSums(edaphic.core$P < 0.05, na.rm = TRUE)

# Identifying genera with the highest number of significant associations
max_associations <- max(significant_counts, na.rm = TRUE)
top_genus <- which(significant_counts == max_associations)

# For each top genus, counting the number of positive and negative correlations from the significant associations
get_signs <- function(index) {
  sig_cols <- which(edaphic.core$P[index,] < 0.05)
  sig_corrs <- edaphic.core$r[index, sig_cols]
  
  list(
    positive = sum(sig_corrs > 0, na.rm = TRUE),
    negative = sum(sig_corrs < 0, na.rm = TRUE),
    total = length(sig_corrs)
  )
}

# Get the results for each top genus
results <- lapply(top_genus, get_signs)

list(
  top_genera = rownames(edaphic.core$P)[top_genus],
  associations_counts = results
)

##Total number in correlation
# Identifying all the significant associations from the P matrix
significant_indices <- which(edaphic.core$P < 0.05, arr.ind = TRUE)

# Extracting corresponding correlation values for these significant indices from the r matrix
significant_correlations <- edaphic.core$r[significant_indices]

# Counting the number of positive and negative correlations among the significant ones
positive_count <- sum(significant_correlations > 0, na.rm = TRUE)/2
negative_count <- sum(significant_correlations < 0, na.rm = TRUE)/2

# Summing up the total significant associations
total_significant <- length(significant_correlations)/2

list(
  total_significant_associations = total_significant,
  positive_associations = positive_count,
  negative_associations = negative_count
)

##Now, let's look at the correlation between genera and nutrients
view(merged_edaphic.2.t)
sample.nutrient <- Field_ID.3

##Merge sample.nutrient and genera.cor
view(sample.nutrient)
view(genera.cor)
genera.cor.1 <- as.data.frame(genera.cor)
genera.cor.1$sample <- rownames(genera.cor.1)
merged_nutrient <- merge(sample.nutrient, genera.cor.1, by = c('sample'))

##Decompose and correlate
edaphic_characteristics <- merged_nutrient[8:31]
genera <- merged_nutrient[33:52]

genus.edaphic.cor <- rcorr(as.matrix(edaphic_characteristics), as.matrix(genera), type = "spearman")

##Now we must manually modify the correlation tables
genus.edaphic.cor.r <- as.matrix(genus.edaphic.cor$r)
##Remove the last 17 rows
genus.edaphic.cor.r <- genus.edaphic.cor.r[1:(nrow(genus.edaphic.cor.r) - 20), ]
##Remove the first 24 columns
genus.edaphic.cor.r <- genus.edaphic.cor.r[, 25:ncol(genus.edaphic.cor.r)]

##Do for p.matrix
genus.edaphic.cor.p <- as.matrix(genus.edaphic.cor$P)
##Remove the last 17 rows
genus.edaphic.cor.p <- genus.edaphic.cor.p[1:(nrow(genus.edaphic.cor.p) - 20), ]
##Remove the first 24 columns
genus.edaphic.cor.p <- genus.edaphic.cor.p[, 25:ncol(genus.edaphic.cor.p)]

##Visualize
##Italcize only x-axis
custom_theme <- theme(
  axis.text.x = element_text(face = "italic", color = "black", size = 10, angle = 45)
)

# Use corrplot with custom theme
corrplot(genus.edaphic.cor.r, method = 'color', p.mat = genus.edaphic.cor.p, #diag = FALSE, #type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
         insig = 'label_sig', pch.col = 'black',  outline = TRUE, tl.col = "black", tl.srt=45, 
         tl.cex = 0.5, col=colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10),
         ggtheme = custom_theme)

##Further contextualize top 20
##phona.edaphic.col
top.domain <- phona.edaphic.col
top.domain.2 <- merge(top.domain, merged_edaphic.2, by = "row.names")
top.domain.2 <- top.domain.2 %>%
  rename(genus = Row.names)

top.domain.3 <- merge(top.domain.2, taxon.function.2, by = c("genus"))

top.domain.3 <- top.domain.3 %>%
  select(1, 2, 68:72)

##trends in correlations
# 1. Identify the edaphic properties and genera with the most significant associations:
max_edaphic_associations <- max(rowSums(genus.edaphic.cor.p < 0.05))
max_genus_associations <- max(colSums(genus.edaphic.cor.p < 0.05))

edaphic_most_associations <- which(rowSums(genus.edaphic.cor.p < 0.05) == max_edaphic_associations)
genus_most_associations <- which(colSums(genus.edaphic.cor.p < 0.05) == max_genus_associations)

# Function to extract and count positive/negative associations:
get_counts <- function(cor_r, cor_p, indices, margin) {
  if (margin == 1) {  # rows
    significant_cor <- cor_r[indices, ]
    significant_p <- cor_p[indices, ]
  } else {  # columns
    significant_cor <- cor_r[, indices]
    significant_p <- cor_p[, indices]
  }
  significant_cor <- significant_cor[significant_p < 0.05]
  list(
    positive = sum(significant_cor > 0, na.rm = TRUE),
    negative = sum(significant_cor < 0, na.rm = TRUE),
    total = length(significant_cor)
  )
}

# 2. Extract counts for each edaphic property and genus with ties:
edaphic_counts <- lapply(edaphic_most_associations, get_counts, cor_r = genus.edaphic.cor.r, cor_p = genus.edaphic.cor.p, margin = 1)
genus_counts <- lapply(genus_most_associations, get_counts, cor_r = genus.edaphic.cor.r, cor_p = genus.edaphic.cor.p, margin = 2)

list(
  edaphic_most_associations = rownames(genus.edaphic.cor.p)[edaphic_most_associations],
  edaphic_counts = edaphic_counts,
  
  genus_most_associations = colnames(genus.edaphic.cor.p)[genus_most_associations],
  genus_counts = genus_counts
)

##How many of the top 20 overlap with the top 20 nodes for agronomic properties?
##Will need to run agronomic phona code before this will execute
# Get overlapping rownames
overlapping_rownames <- intersect(rownames(merged_edaphic.2), rownames(agro.cor.2))

# Count the number of overlapping rownames
number_overlapping <- length(overlapping_rownames)
number_overlapping
```

```{r}
##Now, let's look at agronomic data. We will first modify the data as needed, visualize, and construct GLMMs.
##"Yield_Metric", "Plant_Data_GLMM", and  "X100_count_seed_weight" are provided in the .RData file.
##Start with yield
##Convert "Location" to a factor variable
loc <- as.data.frame(sampleData(filtered_obj))
Yield_Metric$Treatment[Yield_Metric$Treatment == 'Prebiotic'] <- 'Biostimulant'
Yield_Metric.2 <- Yield_Metric
Yield_Metric.2$Location <- ifelse(Yield_Metric.2$Block %in% c("A", "B"), "North",
                                  ifelse(Yield_Metric.2$Block %in% c("C", "D", "E", "F"), "Middle", "South"))
Yield_Metric.2$Location <- factor(Yield_Metric.2$Location)

shapiro_result.yield <- shapiro.test(Yield_Metric.2$kg_ha) ##W = 0.88039, p-value = 0.04808

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.yield$p.value < 0.05) {
  
  distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Yield_Metric.2$kg_ha, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- gamma
}

yield <- Yield_Metric.2$kg_ha + 1  # Create new dataframe with yield + 1

###Double check with qqp###
#nbinom.yield <- fitdistr(yield, "Negative Binomial")
#qqp(yield, "nbinom", size = nbinom.yield$estimate[[1]], mu = nbinom.yield$estimate[[2]])

gamma.yield <- fitdistr(yield, "gamma")
qqp(yield, "gamma", shape = gamma.yield$estimate[[1]], rate = gamma.yield$estimate[[2]])

qqp(yield, "lnorm")
qqp(yield, "norm")

##Gamma still not a great fit, but best

full_model.yield <- glmmTMB(kg_ha ~ (Treatment + Cultivar)^2 + (1 | Location), data = Yield_Metric.2,
                              family = Gamma(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.yield <- dredge(full_model.yield, m.min = 1, fixed = c("cond(Cultivar)", "cond(Treatment)"))

##Print all models
print(models.yield)

##Select the best model based on AIC
best_model.yield <- get.models(models.yield, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.yield)

###Marginalize the coefficients due to the use of a nonlinear link function###
yield.margins <- as.data.frame(marginal_effects(best_model.yield, type = "response"))

##Print model output
tab_model(best_model.yield)

####powerSim: estimate power by simulation
power.sim.yield.cult <- powerSim(best_model.yield, fixed("Cultivar"))
power.sim.yield.cult

power.sim.yield.trt <- powerSim(best_model.yield, fixed("Treatment"))
power.sim.yield.trt

##Decompose R2 to get contribution of each fixed effect
yield.r2 <- glmm.hp(best_model.yield, type = "adjR2", commonality = FALSE)

print(yield.r2)
plot.glmmhp(yield.r2)

##Asess model performance
standardized.resid.yield <- resid(best_model.yield, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.yield <- sum(resid(best_model.yield, type = "pearson")^2)
1 - pchisq(dat.resid.yield, df.residual(best_model.yield))


# Assessing Deviance (G2)
deviance_value.yield <- 1 - pchisq(as.numeric(-2 * logLik(best_model.yield)), df.residual(best_model.yield))

# Simulating datasets
dat.sim.yield <- simulate(best_model.yield, nsim = 250)

# Empirical cumulative density function calculations
resid.list.yield <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.yield)) {
    e.yield <- ecdf(dat.sim.yield[[i]] + runif(length(dat.sim.yield[[i]]), -0.5, 0.5))
    resid.list.yield[[i]] <- e.yield(resid(best_model.yield) + runif(length(resid(best_model.yield)), -0.5, 0.5))
    plot(e.yield, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.yield <- do.call(c, resid.list.yield)

# Quantile-quantile plot
qqnorm(all.resid.yield, main = "QQ Plot")
qqline(all.resid.yield, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.yield), standardized.resid.yield, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

##Visualize GlMM
metric <- c("dydx_CultivarCZ4979X", "dydx_TreatmentControl")
##Create the mean_est column with corresponding mean values
mean_est <- c(mean(yield.margins$dydx_CultivarCZ4979X), mean(yield.margins$dydx_TreatmentControl))
##Create the yield.viz dataframe
yield.viz <- data.frame(metric, mean_est)
##Add delta R2
yield.viz$delta <- c(4.85, 95.15)

yield.viz$metric[yield.viz$metric == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
yield.viz$metric[yield.viz$metric  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'

##Plot
ggplot(yield.viz, aes(x = metric, y = mean_est)) +
  geom_col(width = 0.075, fill = "#a890a8", colour = 'black') +
  geom_point(shape = 21, fill = "#a890a8", colour = "black", aes(size = delta)) +
  scale_size(range = c(8, 15)) + 
  geom_hline(yintercept = 0, linetype = "dashed", lwd = 1) +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  #scale_fill_manual(values = c("Control vs Biostimulant" = "#a8a890", "CZ4979X vs CZ4810X" = "#c07848")) +
  #scale_color_manual(values = c("Control vs Biostimulant" = "#a8a890", "CZ4979X vs CZ4810X" = "#c07848")) +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 18, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 20, face = 'bold'),
        axis.text.x = element_text(size = 18, color = "black"),
        strip.text = element_text(size = 20, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 18),
        legend.position = 'none')

##Boxplot
ggplot(Yield_Metric.2, aes(x = Cultivar, y = kg_ha, fill = Treatment)) +
  geom_boxplot(alpha = 0.7) +
  geom_dotplot(binaxis = 'y', alpha = 0.8, stackdir = 'center', position = position_dodge(0.75)) +
  scale_fill_manual(values = c("#a8a890", "#c07848")) +
  #labs(y = expression(bold(Theoretical ~ yield ~ (kg ~ ha^{"−1"})))) +
  labs(y = "Theoretical Yield (kg/ha)") +
  theme_bw() +
  theme(axis.text = element_text(color = "black", size = 18),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 20, face = "bold"),
        legend.position = "none")
```

```{r}
##GLMM and visualization for root biomass
Plant_Data_GLMM$Location <- ifelse(Plant_Data_GLMM$Block %in% c("A", "B"), "North",
                                  ifelse(Plant_Data_GLMM$Block %in% c("C", "D", "E", "F"), "Middle", "South"))

Plant_Data_GLMM$Location <- factor(Plant_Data_GLMM$Location)

Plant_Data_GLMM$Treatment[Plant_Data_GLMM$Treatment == 'Prebiotic'] <- 'Biostimulant'

Plant_Data_GLMM$Grouping <- factor(Plant_Data_GLMM$Grouping)

shapiro_result.root <- shapiro.test(Plant_Data_GLMM$Root_Biomass) ##W = 0.94242, p-value = 0.02013

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.root$p.value < 0.05) {
  
  distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Plant_Data_GLMM$Root_Biomass, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}

root <- Plant_Data_GLMM$Root_Biomass + 1

###Double check with qqp###
#nbinom.root <- fitdistr(root, "Negative Binomial")

gamma.root <- fitdistr(root, "gamma")
qqp(root, "gamma", shape = gamma.root$estimate[[1]], rate = gamma.root$estimate[[2]])

qqp(root, "lnorm")
qqp(root, "norm")

##lnorm is reported as best, but qqp suggests gamma; will use gamma

full_model.root <- glmmTMB(Root_Biomass ~ (Treatment + Cultivar)^2 + (1 | Location/Grouping), data = Plant_Data_GLMM,
                              family = Gamma(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.root <- dredge(full_model.root, m.min = 1, fixed = c("cond(Cultivar)", "cond(Treatment)"))

##Print all models
print(models.root)

##Select the best model based on AIC
best_model.root <- get.models(models.root, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.root)

###Marginalize the coefficients due to the use of a nonlinear link function###
root.margins <- as.data.frame(marginal_effects(best_model.root, type = "response"))

##Print model output
tab_model(best_model.root)

####powerSim: estimate power by simulation
power.sim.root.cult <- powerSim(best_model.root, fixed("Cultivar"))
power.sim.root.cult

power.sim.root.trt <- powerSim(best_model.root, fixed("Treatment"))
power.sim.root.trt

##Decompose R2 to get contribution of each fixed effect
root.r2 <- glmm.hp(best_model.root, type = "adjR2", commonality = FALSE)
print(root.r2)
plot.glmmhp(root.r2)

##Asess model performance
standardized.resid.root <- resid(best_model.root, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.root <- sum(resid(best_model.root, type = "pearson")^2)
1 - pchisq(dat.resid.root, df.residual(best_model.root))


# Assessing Deviance (G2)
deviance_value.root <- 1 - pchisq(as.numeric(-2 * logLik(best_model.root)), df.residual(best_model.root))

# Simulating datasets
dat.sim.root <- simulate(best_model.root, nsim = 250)

# Empirical cumulative density function calculations
resid.list.root <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.root)) {
    e.root <- ecdf(dat.sim.root[[i]] + runif(length(dat.sim.root[[i]]), -0.5, 0.5))
    resid.list.root[[i]] <- e.root(resid(best_model.root) + runif(length(resid(best_model.root)), -0.5, 0.5))
    plot(e.root, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.root <- do.call(c, resid.list.root)

# Quantile-quantile plot
qqnorm(all.resid.root, main = "QQ Plot")
qqline(all.resid.root, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.root), standardized.resid.root, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

##Visualize GlMM
##Create the mean_est column with corresponding mean values
mean_est.root <- c(mean(root.margins$dydx_CultivarCZ4979X), mean(root.margins$dydx_TreatmentControl))
##Create the root.viz dataframe
root.viz <- data.frame(metric, mean_est.root)
root.viz$metric[root.viz$metric == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
root.viz$metric[root.viz$metric  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'

##Add delta R2
root.viz$delta <- c(0, 100) 

ggplot(root.viz, aes(x = metric, y = mean_est.root)) +
  geom_col(width = 0.075, fill = "#a890a8", colour = 'black') +
   geom_point(shape = 21, fill = "#a890a8", colour = "black", aes(size = delta)) +
  scale_size(range = c(8, 15)) +  
  geom_hline(yintercept = 0, linetype = "dashed", lwd = 1) +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 18, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 20, face = 'bold'),
        axis.text.x = element_text(size = 18, color = "black"),
        strip.text = element_text(size = 20, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 20),
        legend.position = 'none')

##Boxplot
ggplot(Plant_Data_GLMM, aes(x = Cultivar, y = Root_Biomass, fill = Treatment)) +
  geom_boxplot(alpha = 0.7) +
  geom_dotplot(binaxis = 'y', alpha = 0.8, stackdir = 'center', position = position_dodge(0.75)) +
  scale_fill_manual(values = c("#a8a890", "#c07848")) +
  labs(y = "Belowground Biomass (g)") +
  theme_bw() +
  theme(axis.text = element_text(color = "black", size = 18),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 20, face = "bold"),
        legend.position = "none")
```

```{r}
##GLMM and visualization for aboveground biomass
shapiro_result.shoot <- shapiro.test(Plant_Data_GLMM$Shoot_Biomass) ##W = 0.89365, p-value = 0.0003947

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.shoot$p.value < 0.05) {
  
  distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(Plant_Data_GLMM$Shoot_Biomass, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}

shoot <- Plant_Data_GLMM$Shoot_Biomass + 1

###Double check with qqp###
#nbinom.shoot <- fitdistr(shoot, "Negative Binomial")

gamma.shoot <- fitdistr(shoot, "gamma")
qqp(shoot, "gamma", shape = gamma.shoot$estimate[[1]], rate = gamma.shoot$estimate[[2]])

qqp(shoot, "lnorm")
qqp(shoot, "norm")

##lnorm is reported as best, but qqp suggests gamma; will use gamma

full_model.shoot <- glmmTMB(Shoot_Biomass ~ (Treatment + Cultivar)^2 + (1 | Location/Grouping), data = Plant_Data_GLMM,
                              family = Gamma(link = "log"), na.action = "na.fail")

##Perform model selection using dredge
models.shoot <- dredge(full_model.shoot, m.min = 1, fixed = c("cond(Cultivar)", "cond(Treatment)"))

##Print all models
print(models.shoot)

##Select the best model based on AIC
best_model.shoot <- get.models(models.shoot, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.shoot) ##Cultivar is statistically significant

###Marginalize the coefficients due to the use of a nonlinear link function###
shoot.margins <- as.data.frame(marginal_effects(best_model.shoot, type = "response"))

####powerSim: estimate power by simulation
power.sim.shoot.cult <- powerSim(best_model.shoot, fixed("Cultivar"))
power.sim.shoot.cult

power.sim.shoot.trt <- powerSim(best_model.shoot, fixed("Treatment"))
power.sim.shoot.trt

##Decompose R2 to get contribution of each fixed effect
shoot.r2 <- glmm.hp(best_model.shoot, type = "adjR2", commonality = FALSE)

print(shoot.r2)
plot.glmmhp(shoot.r2)

##Asess model performance
standardized.resid.shoot <- resid(best_model.shoot, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.shoot <- sum(resid(best_model.shoot, type = "pearson")^2)
1 - pchisq(dat.resid.shoot, df.residual(best_model.shoot))


# Assessing Deviance (G2)
deviance_value.shoot <- 1 - pchisq(as.numeric(-2 * logLik(best_model.shoot)), df.residual(best_model.shoot))

# Simulating datasets
dat.sim.shoot <- simulate(best_model.shoot, nsim = 250)

# Empirical cumulative density function calculations
resid.list.shoot <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.shoot)) {
    e.shoot <- ecdf(dat.sim.shoot[[i]] + runif(length(dat.sim.shoot[[i]]), -0.5, 0.5))
    resid.list.shoot[[i]] <- e.shoot(resid(best_model.shoot) + runif(length(resid(best_model.shoot)), -0.5, 0.5))
    plot(e.shoot, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.shoot <- do.call(c, resid.list.shoot)

# Quantile-quantile plot
qqnorm(all.resid.shoot, main = "QQ Plot")
qqline(all.resid.shoot, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.shoot), standardized.resid.shoot, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

##Visualize GlMM
##Create the mean_est column with corresponding mean values
mean_est.shoot <- c(mean(shoot.margins$dydx_CultivarCZ4979X), mean(shoot.margins$dydx_TreatmentControl))
##Create the root.viz dataframe
shoot.viz <- data.frame(metric, mean_est.shoot)
shoot.viz$metric[shoot.viz$metric == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
shoot.viz$metric[shoot.viz$metric  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'

##Add delta R2
shoot.viz$delta <- c(85.03, 14.97)

ggplot(shoot.viz, aes(x = metric, y = mean_est.shoot)) +
  geom_col(width = 0.075, fill = "#a890a8", colour = "black") +
  geom_point(shape = 21, fill = "#a890a8", colour = "black", aes(size = delta)) +
  scale_size(range = c(8, 15)) +  
  geom_hline(yintercept = 0, linetype = "dashed", lwd = 1) +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 18, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 20, face = 'bold'),
        axis.text.x = element_text(size = 18, color = "black"),
        strip.text = element_text(size = 20, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 18),
        legend.position = 'none')

##Boxplot
ggplot(Plant_Data_GLMM, aes(x = Cultivar, y = Shoot_Biomass, fill = Treatment)) +
  geom_boxplot(alpha = 0.7) +
  geom_dotplot(binaxis = 'y', alpha = 0.8, stackdir = 'center', position = position_dodge(0.75)) +
  scale_fill_manual(values = c("#a8a890", "#c07848")) +
  labs(y = "Aboveground Biomass (g)") +
  theme_bw() +
  theme(axis.text = element_text(color = "black", size = 18),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 20, face = "bold"),
        legend.position = "top")
```

```{r}
##GLMM and visualization for Pods/plant
shapiro_result.pod <- shapiro.test(Plant_Data_GLMM$Pod_Count) #W = 0.93393, p-value = 0.009543

###Will use a Poisson model since we are dealing with discrete data

full_model.pod <- glmmTMB(Pod_Count ~ (Treatment + Cultivar)^2 + (1 | Location/Grouping),
                          data = Plant_Data_GLMM,
                          family = poisson(link = "log"),
                          na.action = "na.fail")

##Perform model selection using dredge
models.pod <- dredge(full_model.pod, m.min = 1, fixed = c("cond(Cultivar)", "cond(Treatment)"))

##Print all models
print(models.pod)

##Select the best model based on AIC
best_model.pod <- get.models(models.pod, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.pod) ##Cultivar is statistically significant

###Marginalize the coefficients due to the use of a nonlinear link function###
pod.margins <- as.data.frame(marginal_effects(best_model.pod, type = "response"))

####powerSim: estimate power by simulation
power.sim.pod.cult <- powerSim(best_model.pod, fixed("Cultivar"))
power.sim.pod.cult

power.sim.pod.trt <- powerSim(best_model.pod, fixed("Treatment"))
power.sim.pod.trt

##Decompose R2 to get contribution of each fixed effect
pod.r2 <- glmm.hp(best_model.pod, type = "adjR2", commonality = FALSE)

print(pod.r2)
plot.glmmhp(pod.r2)

##Assess model performance
standardized.resid.pod <- resid(best_model.pod, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.pod <- sum(resid(best_model.pod, type = "pearson")^2)
1 - pchisq(dat.resid.pod, df.residual(best_model.pod))

# Assessing Deviance (G2)
deviance_value.pod <- 1 - pchisq(as.numeric(-2 * logLik(best_model.pod)), df.residual(best_model.pod))

# Simulating datasets
dat.sim.pod <- simulate(best_model.pod, nsim = 250)

# Empirical cumulative density function calculations
resid.list.pod <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.pod)) {
    e.pod <- ecdf(dat.sim.pod[[i]] + runif(length(dat.sim.pod[[i]]), -0.5, 0.5))
    resid.list.pod[[i]] <- e.pod(resid(best_model.pod) + runif(length(resid(best_model.pod)), -0.5, 0.5))
    plot(e.pod, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.pod <- do.call(c, resid.list.pod)

# Quantile-quantile plot
qqnorm(all.resid.pod, main = "QQ Plot")
qqline(all.resid.pod, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.pod), standardized.resid.pod, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

##Visualize GlMM
##Create the mean_est column with corresponding mean values
mean_est.pod <- c(mean(pod.margins$dydx_CultivarCZ4979X), mean(pod.margins$dydx_TreatmentControl))
##Create the root.viz dataframe
pod.viz <- data.frame(metric, mean_est.pod)
pod.viz$metric[pod.viz$metric == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
pod.viz$metric[pod.viz$metric  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'

##Add delta R2
pod.viz$delta <- c(71.11, 28.89)

ggplot(pod.viz, aes(x = metric, y = mean_est.pod)) +
  geom_col(width = 0.075, fill = "#a890a8", colour = "black") +
  geom_point(shape = 21, fill = "#a890a8", colour = "black", aes(size = delta)) +
  scale_size(range = c(8, 15)) +  
  geom_hline(yintercept = 0, linetype = "dashed", lwd = 1) +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 18, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 20, face = 'bold'),
        axis.text.x = element_text(size = 18, color = "black"),
        strip.text = element_text(size = 20, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 18),
        legend.position = 'none')


##Boxplot
ggplot(Plant_Data_GLMM, aes(x = Cultivar, y = Pod_Count, fill = Treatment)) +
  geom_boxplot(alpha = 0.7) +
  geom_dotplot(binaxis = 'y', alpha = 0.8, stackdir = 'center', position = position_dodge(0.75)) +
  scale_fill_manual(values = c("#a8a890", "#c07848")) +
  labs(y = "Pods/Plant") +
  theme_bw() +
  theme(axis.text = element_text(color = "black", size = 18),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 20, face = "bold"),
        legend.position = "none")
```

```{r}
##GLMM and visualization for 100-seed weight
# Create a new column "Location" based on the conditions
X100_count_seed_weight$Location <- ifelse(X100_count_seed_weight$Number %in% 1:4, "North",
                                          ifelse(X100_count_seed_weight$Number %in% 5:12, "Middle",
                                                 ifelse(X100_count_seed_weight$Number %in% 13:16, "South", NA)))



X100_count_seed_weight$Location <- factor(X100_count_seed_weight$Location)

X100_count_seed_weight$Treatment[X100_count_seed_weight$Treatment == 'Prebiotic'] <- 'Biostimulant'

X100_count_seed_weight$Sample <- rep(1:16, each = 3)

##shapiro test
shapiro_result.seed <- shapiro.test(X100_count_seed_weight$Corrected_Weight) ##W = 0.93347, p-value = 0.009173

###Determining the best-fitting distribution (if departure from normality)###
if (shapiro_result.seed$p.value < 0.05) {
  
  distributions <- c("lnorm", "gamma", "nbinom", "pois", "binom")
  best_fit <- NULL
  best_fit_AIC <- Inf
  
  for (dist in distributions) {
    fit <- tryCatch(
      fitdist(X100_count_seed_weight$Corrected_Weight, dist),
      error = function(e) NA
    )
    
    if (!is.na(fit) && fit$aic < best_fit_AIC) {
      best_fit <- dist
      best_fit_AIC <- fit$aic
    }
  }
  
  best_fit  # Check the best-fitting distribution- lnorm
}

seed <- X100_count_seed_weight$Corrected_Weight + 1

###Double check with qqp###
#nbinom.seed<- fitdistr(seed, "Negative Binomial")

gamma.seed <- fitdistr(seed, "gamma")
qqp(seed, "gamma", shape = gamma.seed$estimate[[1]], rate = gamma.seed$estimate[[2]])

qqp(seed, "lnorm")
qqp(seed, "norm")

##lnorm is reported as best, but qqp suggests gamma; will use gamma

full_model.seed <- glmmTMB(Corrected_Weight ~ (Treatment + Cultivar)^2 + (1 | Location/Number),
                           data = X100_count_seed_weight,
                           family = Gamma(link = "log"),
                           na.action = "na.fail")

##Perform model selection using dredge
models.seed <- dredge(full_model.seed, m.min = 1, fixed = c("cond(Cultivar)", "cond(Treatment)"))

##Print all models
print(models.seed)

##Select the best model based on AIC
best_model.seed <- get.models(models.seed, subset = delta < 2)[[1]]

##Print the best model
summary(best_model.seed) ##Treatment is statistically significant

###Marginalize the coefficients due to the use of a nonlinear link function###
seed.margins <- as.data.frame(marginal_effects(best_model.seed, type = "response"))

####powerSim: estimate power by simulation
power.sim.seed.cult <- powerSim(best_model.seed, fixed("Cultivar"))

power.sim.seed.cult #30.10%

power.sim.seed.trt <- powerSim(best_model.seed, fixed("Treatment"))

power.sim.seed.trt #30.10%

##Decompose R2 to get contribution of each fixed effect
seed.r2 <- glmm.hp(best_model.seed, type = "adjR2", commonality = FALSE)

print(seed.r2)
plot.glmmhp(seed.r2)

##Assess model performance
standardized.resid.seed <- resid(best_model.seed, type = "pearson")

# Assessing Pearson's χ2 residuals
dat.resid.seed <- sum(resid(best_model.seed, type = "pearson")^2)
1 - pchisq(dat.resid.seed, df.residual(best_model.seed))

# Assessing Deviance (G2)
deviance_value.seed <- 1 - pchisq(as.numeric(-2 * logLik(best_model.seed)), df.residual(best_model.seed))

# Simulating datasets
dat.sim.seed <- simulate(best_model.seed, nsim = 250)

# Empirical cumulative density function calculations
resid.list.seed <- list()

par(mfrow = c(5, 5), mar = c(3, 3, 1, 1)) 

for (i in 1:length(dat.sim.seed)) {
    e.seed <- ecdf(dat.sim.seed[[i]] + runif(length(dat.sim.seed[[i]]), -0.5, 0.5))
    resid.list.seed[[i]] <- e.seed(resid(best_model.seed) + runif(length(resid(best_model.seed)), -0.5, 0.5))
    plot(e.seed, main = paste("Sim", i), las = 1)
}

# Combining residuals from all simulations
all.resid.seed <- do.call(c, resid.list.seed)

# Quantile-quantile plot
qqnorm(all.resid.seed, main = "QQ Plot")
qqline(all.resid.seed, col = "red")

# Residuals vs Fitted values plot
plot(fitted(best_model.seed), standardized.resid.seed, 
     main = "Residuals vs Fitted", xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

##Visualize GlMM
##Create the mean_est column with corresponding mean values
mean_est.seed <- c(mean(seed.margins$dydx_CultivarCZ4979X), mean(seed.margins$dydx_TreatmentControl))
##Create the root.viz dataframe
seed.viz <- data.frame(metric, mean_est.seed)
seed.viz$metric[seed.viz$metric == 'dydx_TreatmentControl'] <- 'Control vs Biostimulant'
seed.viz$metric[seed.viz$metric  == 'dydx_CultivarCZ4979X'] <- 'CZ4979X vs CZ4810X'

##Add delta R2
seed.viz$delta <- c(43.54, 56.46)

ggplot(seed.viz, aes(x = metric, y = mean_est.seed)) +
  geom_col(width = 0.075, fill = "#a890a8", colour = "black") +
  geom_point(shape = 21, fill = "#a890a8", colour = "black", aes(size = delta)) +
  scale_size(range = c(8, 15)) +
  geom_hline(yintercept = 0, linetype = "dashed", lwd = 1) +
  labs(x = "Predictor", y = "Mean Estimate") +
  theme_bw() +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 18, color = "black", face = 'bold'),
        axis.title.x = element_text(size = 20, face = 'bold'),
        axis.text.x = element_text(size = 18, color = "black"),
        strip.text = element_text(size = 20, face = "bold"),
        legend.title = element_blank(),
        legend.text = element_text(size = 18),
        legend.position = 'none')

##Boxplot
ggplot(X100_count_seed_weight, aes(x = Cultivar, y = Corrected_Weight, fill = Treatment)) +
  geom_boxplot(alpha = 0.7) +
  geom_dotplot(binaxis = 'y', alpha = 0.8, stackdir = 'center', position = position_dodge(0.75)) +
  scale_fill_manual(values = c("#a8a890", "#c07848")) +
  labs(y = "100-seed Weight (g)") +
  theme_bw() +
  theme(axis.text = element_text(color = "black", size = 18),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 20, face = "bold"),
        legend.position = "none")
```

```{r}
##Let's try phenotype-operational taxonomic unit (OTU) network analysis (PhONA) with our agronomic data
##https://ravinpoudel.github.io/PhONA/index.html
##https://journals.asm.org/doi/10.1128/aem.01843-22 


##installation- must download and install locally 
install.packages("~/Downloads/ravinpoudel-PhONA-v0.2-2-g7722c4c.tar.gz", repos = NULL, type = "source")
library(PhONA)

##We will have to select a single timepoint for this analysis; thus, we will use R6. 
##Let's filter the global, conglomerated phyloseq object to the R6 growth stage.
##We can then decompose the new phyloseq object in order to include agronomic data in
##The metadata file

##Filter the global phyloseq object
##Remember that this code was used to conglomerate the global phyloseq object
##at genus level: filtered_obj <- conglomerate_taxa(merged_phylo, "genus")


##One plot was removed from the yield analysis, so we must remove the corresponding sample here as well
phona.1 <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)
phona.1 <- prune_samples(!sample_data(phona.1)$ID == "Trt_Cultivar2_Rep_4", phona.1)

###Decompose phona.1###

##Sample data
phona.1.sample.data <- sampleData(phona.1)
phona.1.sample.data.frame <- as.data.frame(sampleData(phona.1))

##Merge yield data and phyloseq sample data
phona.2.sample.data <- merge(phona.1.sample.data.frame, Yield_Metric, by = c("ID", "Treatment", "Cultivar"), keep_order = 1)

##Get the order of rows in phona.1.sample.data.frame$ID
order <- match(phona.1.sample.data.frame$ID, phona.2.sample.data$ID)

##Reorder the rows of phona.2.sample.data using the obtained order
phona.2.sample.data <- phona.2.sample.data[order, ]


##provide rownames
row.names(phona.2.sample.data) <- c("sample_49", "sample_50", "sample_51", "sample_52", "sample_53",
                                    "sample_54", "sample_55", "sample_56", "sample_57", "sample_58",
                                    "sample_59", "sample_60", "sample_61", "sample_62", "sample_63")

##Count table
phona.1.count <- as.data.frame(otu_table(phona.1))

##Make rownames a column
phona.1.count$genus <- rownames(phona.1.count)

##Remove rows if genus is not found in correlation table
allowed_values <- unique(c(all.pairs.3$X, all.pairs.3$Y))

##Subset phona.1.count based on matching values in the genus column
phona.1.count.filtered <- phona.1.count[phona.1.count$genus %in% allowed_values, ]

##Remove genus column
phona.1.count.filtered <- phona.1.count.filtered[, -which(names(phona.1.count.filtered) == "genus")]

##Filter taxa to those found in at least 20% of samples-not doing this at this point; will filter after network construction
#nonzero_mat <- phona.1.count.filtered != 0
#nonzero_counts <- rowSums(nonzero_mat)
#num_columns <- ncol(phona.1.count.filtered)
#phona.1.count.filtered <- phona.1.count.filtered[nonzero_counts >= (0.2 * num_columns), ]

##Convert back to phyloseq object
phona.1.count.1 <- otu_table(phona.1.count.filtered, taxa_are_rows = TRUE)

##Now for tax table
phona.1.tax <- as.data.frame(tax_table(phona.1))

##Subset phona.1.count based on matching values in the genus column
phona.1.tax.filtered <- phona.1.tax[phona.1.tax$genus %in% allowed_values, ]
phona.1.tax.filtered <- phona.1.tax.filtered[rownames(phona.1.tax.filtered) %in% rownames(phona.1.count.filtered), ]

##Capitalize column names
colnames(phona.1.tax.filtered) <- sapply(colnames(phona.1.tax.filtered), function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
})

##Add dummy column for species
set.seed(123)
##Generate random character strings
n <- nrow(phona.1.tax.filtered)
random_species <- replicate(n, paste0(sample(letters, 5, replace = TRUE), collapse = ""))
##Add the "Species" column to the dataframe
phona.1.tax.filtered$Species <- random_species

phona.1.tax.filtered <-as.matrix(phona.1.tax.filtered)

##Convert back to phyloseq object
phona.1.tax.1 <- tax_table(phona.1.tax.filtered)

##Create phyloseq object from this 
yield.phona <- phyloseq(phona.1.count.1, phona.1.tax.1, phona.yield.sample)

yield.phona_pruned <- prune_taxa(taxa_sums(yield.phona) > 0, yield.phona)

##Get the unique values in otu table
unique.genera <- as.data.frame(row.names(otu_table(yield.phona_pruned)))


##Filter all.pairs.3
table.sp.2 <- all.pairs.3[all.pairs.3$X %in% unique.genera$`row.names(otu_table(yield.phona_pruned))` & 
                      all.pairs.3$Y %in% unique.genera$`row.names(otu_table(yield.phona_pruned))`, ]

test <- unique(c(table.sp.2$X, table.sp.2$Y)) ##Not the same length; two are still unique to phyloseq object
##Remove since no correlation exists

##Get unique values of X and Y
unique_values <- as.data.frame(unique(c(table.sp.2$X, table.sp.2$Y)))

##Filter phyloseq object once again
##OTU table
phona.1.count.2 <- as.data.frame(otu_table(yield.phona_pruned))
phona.1.count.2$genus <- rownames(phona.1.count.2)
phona.1.count.2.filtered <- phona.1.count.2[phona.1.count.2$genus %in% unique_values$`unique(c(table.sp.2$X, table.sp.2$Y))`, ]
##Remove genus column
phona.1.count.2.filtered <- phona.1.count.2.filtered[, -which(names(phona.1.count.2.filtered) == "genus")]
##Convert back to phyloseq object
phona.1.count.2.filtered.2 <- otu_table(phona.1.count.2.filtered, taxa_are_rows = TRUE)

##Tax table
phona.1.tax.2 <- as.data.frame(tax_table(yield.phona_pruned))
phona.1.tax.2.filtered <- phona.1.tax.2[phona.1.tax.2$Genus %in% unique_values$`unique(c(table.sp.2$X, table.sp.2$Y))`, ]
phona.1.tax.2.filtered <-as.matrix(phona.1.tax.2.filtered)
##Convert back to phyloseq object
phona.1.tax.2 <- tax_table(phona.1.tax.2.filtered)

##Create phyloseq object from this 
yield.phona_pruned.2 <- phyloseq(phona.1.count.2.filtered.2, phona.1.tax.2, phona.yield.sample)
##Rename columns in the tax_table
#colnames(yield.phona_pruned.2@tax_table)[colnames(yield.phona_pruned.2@tax_table) == 'kingdom'] <- 'Kingdom'
#colnames(yield.phona_pruned.2@tax_table)[colnames(yield.phona_pruned.2@tax_table) == 'genus'] <- 'Genus'


phyobj.yield = taxacolor(phyobj = yield.phona_pruned.2, coloredby = "Kingdom")

##Now, let's create the correlation matrices
##Get unique values from table.sp.2$X and table.sp.2$Y
unique_values <- unique(c(table.sp.2$X, table.sp.2$Y))

##Create an empty matrix with dimensions equal to the length of unique values
matrix_size <- length(unique_values)
matrix_result <- matrix(0, nrow = matrix_size, ncol = matrix_size)

##Set row and column names of the matrix
rownames(matrix_result) <- unique_values
colnames(matrix_result) <- unique_values

##Fill the matrix with corresponding values from table.sp.2$rho
for (i in 1:nrow(table.sp.2)) {
  x <- match(table.sp.2$X[i], unique_values)
  y <- match(table.sp.2$Y[i], unique_values)
  matrix_result[x, y] <- table.sp.2$rho[i]
}

##Replace NA values with 0
matrix_result[is.na(matrix_result)] <- 0

## Replace 0 values with 0.1
#matrix_result[matrix_result == 0] <- 0.1

##Fill diagonal elements with 1
diag(matrix_result) <- 1

##Convert the matrix to a square matrix
core_matrix <- as.matrix(matrix_result)

##Now for the p-value matrix
##Get unique values from table.sp.2$X and table.sp.2$Y
unique_values <- unique(c(table.sp.2$X, table.sp.2$Y))

##Create an empty matrix with dimensions equal to the length of unique values
matrix_size <- length(unique_values)
matrix_result <- matrix(0, nrow = matrix_size, ncol = matrix_size)

##Set row and column names of the matrix
rownames(matrix_result) <- unique_values
colnames(matrix_result) <- unique_values

##Fill the matrix with corresponding values from table.sp.2$rho
for (i in 1:nrow(table.sp.2)) {
  x <- match(table.sp.2$X[i], unique_values)
  y <- match(table.sp.2$Y[i], unique_values)
  matrix_result[x, y] <- table.sp.2$p[i]
}

##Replace NA values with 1
matrix_result[is.na(matrix_result)] <- 1

##Fill diagonal elements with 0
diag(matrix_result) <- 0

##Convert the matrix to a square matrix
p_matrix <- as.matrix(matrix_result)

##Now attempt phona- ##had to use lm
phona.yield <- PhONA(
  physeqobj = phyobj.yield,
  model = "lm",
  cordata = core_matrix,
  pdata = p_matrix,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "kg_ha",
  defineTreatment = "bio_or_control",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Yield",
  nodesize = 10,
  Pheno2OTUedgecolor = "gray",
  netlayout = layout.circle)

##Role Plot
rolePlot(phona.yield)

phona.yield.sum <- as.data.frame(summarizePhONA(phona.yield$phona_graph, phona.yield$roles))
phona.yield.role <- as.data.frame(phona.yield$roles)
# Create the Domain column based on the Kingdom column
phona.yield.role$Domain <- ifelse(phona.yield.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.yield.role$metric <- "Yield"

##Add hubscore
phona.yield.hub <- as.data.frame(hub_score(phona.yield$phona_graph))

phona.yield.hub$name <- rownames(phona.yield.hub)

##Merge phona.yield.hub and phona.yield.role by 'name'
phona.yield.role.2 <- merge(phona.yield.hub, phona.yield.role, by = c('name'))


##Clean up visualization
layout.yield <- layout.sphere(phona.yield$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)


##Check order of vertices
vertex_names.yield <- names(V(phona.yield$phona_graph))
print(vertex_names.yield)
vertex_names.yield.2 <- vertex_names.yield[-1]

# Filter and change color
yield.tax.data <- as.data.frame(tax_table(phyobj.yield))
yield.tax.data <- yield.tax.data %>% 
  mutate(Kingdom_color = case_when(
    Kingdom_color == "#BB5CD9" ~ "#a8a890",
    Kingdom_color == "#A99FD6" ~ "#c07848",
    Kingdom_color == "#D7CA58" ~ "#a8c0a8",
    Kingdom_color == "#DD6E7C" ~ "#a890a8",
    Kingdom_color == "#94E585" ~ "#786060",
    Kingdom_color == "#D9D0BA" ~ "#c0c0c0",
    Kingdom_color == "#80D5D2" ~ "#d8c0a8",
    TRUE ~ Kingdom_color
  ))

intermediate_df.yield <- yield.tax.data[yield.tax.data$Genus %in% vertex_names.yield.2, ]

# Order the intermediate dataframe based on genus_order.pod
intermediate_df.yield <- intermediate_df.yield[match(vertex_names.yield.2, intermediate_df.yield$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.yield <- intermediate_df.yield$Kingdom_color
ordered_color_list.yield <- c(ordered_color_list.yield)

plot(phona.yield$phona_graph, layout = layout.yield, edge.color = phona.edge.2, vertex.color = ordered_color_list.yield, 
      vertex.size = 12, vertex.label = "")
```

```{r}
##PhONA for agronomic properties in Plant_Data_GLMM
phona.ag <- prune_samples(sample_data(filtered_obj)$Developmental_Stage == "R6", filtered_obj)

###Decompose phona.ag###
##ASV table
phona.ag.count <- as.data.frame(otu_table(phona.ag))

##Tax table
phona.ag.tax <- as.data.frame(tax_table(phona.ag))

##Sample data
phona.ag.sample.data <- sampleData(phona.ag)
phona.ag.sample.data.frame <- as.data.frame(sampleData(phona.ag))

##We cannot have multiple rownames with the same rowname; thus, we must take the mean
##across pseudoreplicates for each metric
# Group by 'Grouping' column and calculate sum for numeric columns
plant.data.glmm.2 <- Plant_Data_GLMM %>%
  group_by(Grouping) %>%
  summarise(across(where(is.numeric), sum, na.rm = TRUE),
            across(where(is.character), first))
##provide rownames
row.names(plant.data.glmm.2) <- c("sample_49", "sample_50", "sample_51", "sample_52", "sample_53",
                                    "sample_54", "sample_55", "sample_56", "sample_57", "sample_58",
                                    "sample_59", "sample_60", "sample_61", "sample_62", "sample_63", "sample_64")

##Make phyloseq object again##
phona.ag.sample.data.2 <- sample_data(plant.data.glmm.2)

rownames(phona.ag.sample.data.2) <- c("sample_49", "sample_50", "sample_51", "sample_52", "sample_53",
                                      "sample_54", "sample_55", "sample_56", "sample_57", "sample_58",
                                      "sample_59", "sample_60", "sample_61", "sample_62", "sample_63", "sample_64")

##Remove rows if genus is not found in correlation table
allowed_values <- unique(c(all.pairs.3$X, all.pairs.3$Y))

##add rownames as genus column
phona.ag.count$genus <- rownames(phona.ag.count)

##Subset phona.1.count based on matching values in the genus column
phona.ag.count.filtered <- phona.ag.count[phona.ag.count$genus %in% allowed_values, ]

##Remove genus column
phona.ag.count.filtered <- phona.ag.count.filtered[, -which(names(phona.ag.count.filtered) == "genus")]

##Filter taxa to those found in at least 20% of samples-not doing this at this point; will filter after network construction
#nonzero_mat <- phona.ag.count.filtered != 0
#nonzero_counts <- rowSums(nonzero_mat)
#num_columns <- ncol(phona.ag.count.filtered)
#phona.ag.count.filtered <- phona.ag.count.filtered[nonzero_counts >= (0.2 * num_columns), ]

##Convert back to phyloseq object##
phona.ag.count.1 <- otu_table(phona.ag.count.filtered, taxa_are_rows = TRUE)

##Now for tax table
##Subset phona.1.count based on matching values in the genus column
phona.ag.tax.filtered <- phona.ag.tax[phona.ag.tax$genus %in% allowed_values, ]
phona.ag.tax.filtered <- phona.ag.tax.filtered[rownames(phona.ag.tax.filtered) %in% rownames(phona.ag.count.filtered), ]


##Capitalize column names
colnames(phona.ag.tax.filtered) <- sapply(colnames(phona.ag.tax.filtered), function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
})

##Add dummy column for species
set.seed(123)
##Generate random character strings
n <- nrow(phona.ag.tax.filtered)
random_species <- replicate(n, paste0(sample(letters, 5, replace = TRUE), collapse = ""))
##Add the "Species" column to the dataframe
phona.ag.tax.filtered$Species <- random_species

phona.ag.tax.filtered <-as.matrix(phona.ag.tax.filtered)

##Convert back to phyloseq object##
phona.ag.tax.1 <- tax_table(phona.ag.tax.filtered)

##Create phyloseq object from this 
ag.phona <- phyloseq(phona.ag.count.1, phona.ag.tax.1, phona.ag.sample.data.2)
ag.phona_pruned <- prune_taxa(taxa_sums(ag.phona) > 0, ag.phona)

phyobj.ag = taxacolor(phyobj = ag.phona_pruned, coloredby = "Kingdom")

##Filter all.pairs.3
unique.genera <- as.data.frame(row.names(otu_table(phyobj.ag)))

table.sp.2 <- all.pairs.3[all.pairs.3$X %in% unique.genera$`row.names(otu_table(phyobj.ag))` & 
                      all.pairs.3$Y %in% unique.genera$`row.names(otu_table(phyobj.ag))`, ]

test <- unique(c(table.sp.2$X, table.sp.2$Y)) 

##Get unique values of X and Y
unique_values <- as.data.frame(unique(c(table.sp.2$X, table.sp.2$Y)))

##Filter phyloseq object once again
##OTU table
phona.ag.count.2 <- as.data.frame(otu_table(phyobj.ag))
phona.ag.count.2$genus <- rownames(phona.ag.count.2)
phona.ag.count.2.filtered <- phona.ag.count.2[phona.ag.count.2$genus %in% unique_values$`unique(c(table.sp.2$X, table.sp.2$Y))`, ]
##Remove genus column
phona.ag.count.2.filtered <- phona.ag.count.2.filtered[, -which(names(phona.ag.count.2.filtered) == "genus")]
##Convert back to phyloseq object
phona.ag.count.2.filtered.2 <- otu_table(phona.ag.count.2.filtered, taxa_are_rows = TRUE)

##Tax table
phona.ag.tax.2 <- as.data.frame(tax_table(phyobj.ag))
phona.ag.tax.2.filtered <- phona.ag.tax.2[phona.ag.tax.2$Genus %in% unique_values$`unique(c(table.sp.2$X, table.sp.2$Y))`, ]
phona.ag.tax.2.filtered <-as.matrix(phona.ag.tax.2.filtered)

##Convert back to phyloseq object
phona.ag.tax.2 <- tax_table(phona.ag.tax.2.filtered)

##Create phyloseq object from this 
ag.phona_pruned.2 <- phyloseq(phona.ag.count.2.filtered.2, phona.ag.tax.2, phona.ag.sample.data.2)

##Let's modify kingdom colors
tax.data.2 <- as.data.frame(tax_table(ag.phona_pruned.2))

tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#AD9CD5'] <- '#a8a890'
tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#A1D0D9'] <- '#c07848'
tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#96E2B1'] <- '#a890a8'
tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#DD6089'] <- '#c0c0c0'
tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#DDB489'] <- '#a8c0a8'
tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#B9E05E'] <- '#786060'
tax.data.2$Kingdom_color[tax.data.2$Kingdom_color == '#B65CDF'] <- '#d8c0a8'

##Create phyloseq object from this
tax.data.2 <- as.matrix(tax.data.2)
tax.data.3 <- tax_table(tax.data.2)
ag.phona_pruned.3 <- phyloseq(phona.ag.count.2.filtered.2, tax.data.3, phona.ag.sample.data.2)

##Now, let's create the correlation matrices
##Get unique values from table.sp.2$X and table.sp.2$Y
unique_values <- unique(c(table.sp.2$X, table.sp.2$Y))

##Create an empty matrix with dimensions equal to the length of unique values
matrix_size <- length(unique_values)
matrix_result <- matrix(0, nrow = matrix_size, ncol = matrix_size)

##Set row and column names of the matrix
rownames(matrix_result) <- unique_values
colnames(matrix_result) <- unique_values

##Fill the matrix with corresponding values from table.sp.2$rho
for (i in 1:nrow(table.sp.2)) {
  x <- match(table.sp.2$X[i], unique_values)
  y <- match(table.sp.2$Y[i], unique_values)
  matrix_result[x, y] <- table.sp.2$rho[i]
}

##Replace NA values with 0
matrix_result[is.na(matrix_result)] <- 0

## Replace 0 values with 0.1
#matrix_result[matrix_result == 0] <- 0.1

##Fill diagonal elements with 1
diag(matrix_result) <- 1

##Convert the matrix to a square matrix
core_matrix.ag <- as.matrix(matrix_result)

##Now for the p-value matrix
##Get unique values from table.sp.2$X and table.sp.2$Y
unique_values <- unique(c(table.sp.2$X, table.sp.2$Y))

##Create an empty matrix with dimensions equal to the length of unique values
matrix_size <- length(unique_values)
matrix_result <- matrix(0, nrow = matrix_size, ncol = matrix_size)

##Set row and column names of the matrix
rownames(matrix_result) <- unique_values
colnames(matrix_result) <- unique_values

##Fill the matrix with corresponding values from table.sp.2$rho
for (i in 1:nrow(table.sp.2)) {
  x <- match(table.sp.2$X[i], unique_values)
  y <- match(table.sp.2$Y[i], unique_values)
  matrix_result[x, y] <- table.sp.2$p[i]
}

##Replace NA values with 1
matrix_result[is.na(matrix_result)] <- 1

##Fill diagonal elements with 0
diag(matrix_result) <- 0

##Convert the matrix to a square matrix
p_matrix.ag <- as.matrix(matrix_result)
```

```{r}
#PhONA for root biomass
phona.root <- PhONA(
  physeqobj = ag.phona_pruned.3,
  model = "lasso", 
  cordata = core_matrix.ag,
  pdata = p_matrix.ag,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Root_Biomass",
  defineTreatment = "Plant_Replicate",
  PhenoNodecolor = "#a8c0a8",
  PhenoNodesize = 20,
  PhenoNodelabel = "Belowground",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere)

##Role plot
rolePlot(phona.root)

phona.root.sum <- as.data.frame(summarizePhONA(phona.root$phona_graph, phona.root$roles))
phona.root.role <- as.data.frame(phona.root$roles)
# Create the Domain column based on the Kingdom column
phona.root.role$Domain <- ifelse(phona.root.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.root.role$metric <- "Belowground Biomass"

##Add hub score
phona.root.hub <- as.data.frame(hub_score(phona.root$phona_graph))

phona.root.hub$name <- rownames(phona.root.hub)

##Merge phona.root.hub and phona.root.role by 'name'
phona.root.role.2 <- merge(phona.root.hub, phona.root.role, by = c('name'))


##Clean up visualization
layout.root <- layout.sphere(phona.root$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.root <- names(V(phona.root$phona_graph))
print(vertex_names.root)
vertex_names.root.2 <- vertex_names.root[-1]

# Filter tax.data.2 based on vertex_names.root.2
tax.data.2 <- as.data.frame(tax.data.2)
intermediate_df.root <- tax.data.2[tax.data.2$Genus %in% vertex_names.root.2, ]

# Order the intermediate dataframe based on vertex_names.root.2
intermediate_df.root <- intermediate_df.root[match(vertex_names.root.2, intermediate_df.root$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.root <- intermediate_df.root$Kingdom_color
ordered_color_list.root <- c(ordered_color_list.root)

plot(phona.root$phona_graph, layout = layout.root, edge.color = phona.edge.2, vertex.color = ordered_color_list.root, 
      vertex.size = 12, vertex.label = "")
```

```{r}
#PhONA for shoot biomass
phona.shoot <- PhONA(
  physeqobj = ag.phona_pruned.3,
  model = "lasso",
  cordata = core_matrix.ag,
  pdata = p_matrix.ag,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Shoot_Biomass",
  defineTreatment = "Plant_Replicate",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Aboveground",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.circle) 

##Role plot
rolePlot(phona.shoot)

phona.shoot.sum <- as.data.frame(summarizePhONA(phona.shoot$phona_graph, phona.shoot$roles))
phona.shoot.role <- as.data.frame(phona.shoot$roles)
# Create the Domain column based on the Kingdom column
phona.shoot.role$Domain <- ifelse(phona.shoot.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.shoot.role$metric <- "Aboveground Biomass"

##Add hub score
phona.shoot.hub <- as.data.frame(hub_score(phona.shoot$phona_graph))

phona.shoot.hub$name <- rownames(phona.shoot.hub)

##Merge phona.shoot.hub and phona.shoot.role by 'name'
phona.shoot.role.2 <- merge(phona.shoot.hub, phona.shoot.role, by = c('name'))

##Clean up visualization
layout.shoot <- layout.sphere(phona.shoot$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.shoot <- names(V(phona.shoot$phona_graph))
print(vertex_names.shoot)
vertex_names.shoot.2 <- vertex_names.shoot[-1]

# Filter tax.data.2 based on vertex_names.shoot.2
intermediate_df.shoot <- tax.data.2[tax.data.2$Genus %in% vertex_names.shoot.2, ]

# Order the intermediate dataframe based on vertex_names.shoot.2
intermediate_df.shoot <- intermediate_df.shoot[match(vertex_names.shoot.2, intermediate_df.shoot$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.shoot <- intermediate_df.shoot$Kingdom_color
ordered_color_list.shoot <- c(ordered_color_list.shoot)

plot(phona.shoot$phona_graph, layout = layout.shoot, edge.color = phona.edge.2, vertex.color = ordered_color_list.shoot, 
      vertex.size = 12, vertex.label = "")
```

```{r}
#PhONA for pod count
phona.pod <- PhONA(
  physeqobj = ag.phona_pruned.3, 
  model = "lm",##Come back to this one
  cordata = core_matrix.ag,
  pdata = p_matrix.ag,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Pod_Count",
  defineTreatment = "Plant_Replicate",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Pod Count",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.pod)

phona.pod.sum <- as.data.frame(summarizePhONA(phona.pod$phona_graph, phona.pod$roles))
phona.pod.role <- as.data.frame(phona.pod$roles)
# Create the Domain column based on the Kingdom column
phona.pod.role$Domain <- ifelse(phona.pod.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.pod.role$metric <- "Pods/Plant"

##Add hub score
phona.pod.hub <- as.data.frame(hub_score(phona.pod$phona_graph))

phona.pod.hub$name <- rownames(phona.pod.hub)

##Merge phona.pod.hub and phona.pod.role by 'name'
phona.pod.role.2 <- merge(phona.pod.hub, phona.pod.role, by = c('name'))

##Clean up visualization
layout.pod <- layout.sphere(phona.pod$phona_graph)
#phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.pod <- names(V(phona.pod$phona_graph))
print(vertex_names.pod)
      
vertex_names.pod.2 <- vertex_names.pod[-1]

# Filter tax.data.2 based on vertex_names.pod.2
intermediate_df.pod <- tax.data.2[tax.data.2$Genus %in% vertex_names.pod.2, ]

# Order the intermediate dataframe based on vertex_names.pod.2
intermediate_df.pod <- intermediate_df.pod[match(vertex_names.pod.2, intermediate_df.pod$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.pod <- intermediate_df.pod$Kingdom_color
ordered_color_list.pod <- c(ordered_color_list.pod)

plot(phona.pod$phona_graph, layout = layout.pod, edge.color = phona.edge.2, vertex.color = ordered_color_list.pod, 
      vertex.size = 12, vertex.label = "") 
```

```{r}
##Phona for seed weight
##Need to create phyloseq object with sample data modified to have 100-seed weight
yield.sample.data <- as.data.frame(sampleData(ag.phona_pruned.3))

##We cannot have multiple rownames with the same rowname; thus, we must take the sum
##across pseudoreplicates c
# Group by 'Grouping' column and calculate mean for numeric columns
seed.weight <- X100_count_seed_weight %>%
  group_by(Sample) %>%
  summarise(across(where(is.numeric), sum, na.rm = TRUE),
            across(where(is.character), first))

##create phyloseq object from seed.weight
seed.phona.sample <- sampleData(seed.weight)

##Add rownames
rownames(seed.phona.sample) <- c("sample_49", "sample_50", "sample_51", "sample_52", "sample_53",
                                      "sample_54", "sample_55", "sample_56", "sample_57", "sample_58",
                                      "sample_59", "sample_60", "sample_61", "sample_62", "sample_63", "sample_64")

##Create phyloseq object 
phona.seed.phylo <- phyloseq(phona.ag.count.2.filtered.2, phona.ag.tax.2, seed.phona.sample)

phona.seed <- PhONA(
  physeqobj = phona.seed.phylo,
  model = "lasso",
  cordata = core_matrix.ag,
  pdata = p_matrix.ag,
  iters=2,
  OTU_OTU_pvalue = 0.05,
  OTU_OTU_rvalue = 0.6,
  OTU_Phenotype_pvalue = 0.1,
  definePhenotype = "Corrected_Weight",
  defineTreatment = "Plant_Replicate",
  PhenoNodecolor = "black",
  PhenoNodesize = 20,
  PhenoNodelabel = "Seed Weight",
  nodesize = 10,
  Pheno2OTUedgecolor = "black",
  netlayout = layout.sphere) 

##Role plot
rolePlot(phona.seed)

phona.seed.sum <- as.data.frame(summarizePhONA(phona.seed$phona_graph, phona.seed$roles))
phona.seed.role <- as.data.frame(phona.seed$roles)
# Create the Domain column based on the Kingdom column
phona.seed.role$Domain <- ifelse(phona.seed.role$Kingdom == "Bacteria", "Prokaryote", "Eukaryote")
phona.seed.role$metric <- "100-seed Weight"

##Add hub score
phona.seed.hub <- as.data.frame(hub_score(phona.seed$phona_graph))

phona.seed.hub$name <- rownames(phona.seed.hub)

##Merge phona.seed.hub and phona.seed.role by 'name'
phona.seed.role.2 <- merge(phona.seed.hub, phona.seed.role, by = c('name'))

##Clean up visualization
layout.seed <- layout.sphere(phona.seed$phona_graph)
phona.edge <- c("black")
#phona.edge.2 <- adjust_transparency(phona.edge, alpha = 0.5)

##Check order of vertices
vertex_names.seed <- names(V(phona.seed$phona_graph))
print(vertex_names.seed)

vertex_names.seed.2 <- vertex_names.seed[-1]

#Filter and change colors
seed.tax.data <- as.data.frame(tax_table(phona.seed.phylo))

seed.tax.data <- seed.tax.data %>% 
  mutate(Kingdom_color = case_when(
    Kingdom_color == '#DC826D' ~ "#a8a890",
    Kingdom_color == '#BD59D2' ~ "#c07848",
    Kingdom_color == '#7A9ED6' ~ "#a8c0a8",
    Kingdom_color == '#D9A5CB' ~ "#a890a8",
    Kingdom_color == '#DBD86A' ~ "#786060",
    Kingdom_color == '#B0DACA' ~ "#c0c0c0",
    Kingdom_color == '#7EE282' ~ "#d8c0a8",
    TRUE ~ Kingdom_color
  ))

view(seed.tax.data)
intermediate_df.seed <- seed.tax.data[seed.tax.data$Genus %in% vertex_names.seed.2, ]

# Order the intermediate dataframe based on vertex_names.seed.2
intermediate_df.seed <- intermediate_df.seed[match(vertex_names.seed.2, intermediate_df.seed$Genus), ]

# Extract the values from the Kingdom_color column
ordered_color_list.seed <- intermediate_df.seed$Kingdom_color
ordered_color_list.seed <- c(ordered_color_list.seed)

plot(phona.seed$phona_graph, layout = layout.seed, edge.color = phona.edge.2, vertex.color = ordered_color_list.seed, 
      vertex.size = 12, vertex.label = "")
```

```{r}
##PhONA heatmap
phona.heat <- rbind(phona.yield.role.2, phona.shoot.role.2, phona.pod.role.2, phona.seed.role.2, phona.root.role.2)

##Create composite score based on Kleinberg's Hub Score (vector), modularity, and participation coefficient
##Normalizing the metrics
phona.heat$normalized_vector <- (phona.heat$vector - min(phona.heat$vector)) / (max(phona.heat$vector) - min(phona.heat$vector))
phona.heat$normalized_module <- (phona.heat$module - min(phona.heat$module)) / (max(phona.heat$module) - min(phona.heat$module))
phona.heat$normalized_participation <- (phona.heat$participation - min(phona.heat$participation)) / (max(phona.heat$participation) - min(phona.heat$participation))

##Calculating the composite score
phona.heat$composite_score <- (phona.heat$normalized_vector + phona.heat$normalized_module + phona.heat$normalized_participation) / 3

phona.heat.2 <- acast(data = phona.heat, formula = name ~ metric, value.var = "composite_score")
colnames(phona.heat.2)[colnames(phona.heat.2) == 'Yield'] <- 'Theoretical Yield'

#phona.heat.2 <- as.data.frame(phona.heat.2[, c('100-seed Weight', 'Aboveground Biomass', 'Belowground Biomass', 'Pods/Plant', 'Theoretical Yield')])

phona_palette <- colorRamp2(c(0, 1), c("antiquewhite", "lightblue3")) ##, transparency = 0.5) 

lgd_list.phona <- list(title_gp = gpar(fontsize = 16), 
        labels_gp = gpar(fontsize = 12),title = "Composite Score",
        border = "black", lwd = 1,  
        title_position = "topcenter",
        at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0), legend_width = unit(10, "cm"),
        direction = "horizontal",
        x = unit(0.5, "cm"), y = unit(0.5, "cm"),
        legend_height = unit(5, "cm"))
        #labels = c("","Hub node", "", "", "", ""))

##Create row annotation
phona.row <- unique(phona.heat[c("name", "Domain")])
rownames(phona.row) <- phona.row$name
phona.row$name <- NULL
phona.row <- phona.row[order(row.names(phona.row)), , drop = FALSE]

colours.phona <- list('Domain' = c('Prokaryote' = "#a8c0a8", 'Eukaryote' = "#a890a8"))

rowAnn.phona <- HeatmapAnnotation(df = phona.row,
  which = 'row',
  col = colours.phona,
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), simple_anno_size = unit(0.2, "cm"), show_legend = FALSE, show_annotation_name = FALSE)

##Create top annotation only considering non-zero particpation scores
##Let's create top annotation #1
da.top.1.phona <- phona.heat %>%
  #filter(participation != 0) %>%
  group_by(metric) %>%
  summarise(Prokaryote = sum(Domain == "Prokaryote"),
            Eukaryote = sum(Domain == "Eukaryote"))

da.top.1.phona <- as.data.frame(da.top.1.phona)
da.top.1.phona <- da.top.1.phona[order(factor(da.top.1.phona$metric, levels = c('100-seed Weight', 'Aboveground Biomass', 'Belowground Biomass', 'Pods/Plant', 'Theoretical Yield'))), ]

rownames(da.top.1.phona) <- da.top.1.phona[, 1]
da.top.1.phona <- da.top.1.phona[, -1]

da.heat.top.1.phona = HeatmapAnnotation("# unique obs"= anno_barplot(da.top.1.phona, gp = gpar(fill = c("#a8c0a8","#a890a8"))), 
                               which = c("column"), show_annotation_name = FALSE)

##Let's create right annotation, showing mean composite score across phenotypes

phona.left.anno <- as.data.frame(phona.heat.2)

phona.left.anno.2 <- phona.left.anno %>%
  rowwise() %>%
  mutate(mean.composite = mean(c_across(everything()), na.rm = TRUE))

##Remove columns other than mean.composite
phona.left.anno.3 <- as.data.frame(phona.left.anno.2[, -c(1:5)])
##Add rownames
rownames(phona.left.anno.3) <- rownames(phona.heat.2)


phona.row.left = HeatmapAnnotation("# net"= anno_barplot(phona.left.anno.3, axis_param = list(side = "bottom"), 
                      gp = gpar(fill = c("#8c3800"), lwd = 0)), 
                       which = c("row"),   show_annotation_name = FALSE)

phona.heat.1 <- Heatmap(phona.heat.2, cluster_columns = TRUE, cluster_rows = TRUE, na_col = "gray57", ##Do with and without clustering rows
        border = TRUE, col=phona_palette,
       row_names_gp = gpar(fontface = "italic", fontsize = 9), #rect_gp = gpar(col = "black", lwd = 1),
       heatmap_legend_param = lgd_list.phona,
       left_annotation = rowAnn.phona,
       right_annotation = phona.row.left,
       top_annotation = da.heat.top.1.phona,
       show_row_names = FALSE,
       row_split = phona.row$Domain,
       row_gap = unit(c(2), "mm"),
       row_title = NULL,
       column_names_rot = 45)

phona.heatmap.2 <- draw(phona.heat.1, heatmap_legend_side = "top", legend_grouping = "original", padding = unit(c(0, 8, 5, 2), "mm"))

tiff("phona.heatmap.composite.tiff", width = 5, height = 9, units = "in", res = 900)
phona.heatmap.2
dev.off() 
```

```{r}
##Network stats
##Number of prokaryotes and eukaryotes
##phona.row
sum(phona.row$Domain == "Eukaryote")
sum(phona.row$Domain == "Prokaryote")

##Node means for networks
# Calculate mean and SEM for Eukaryote column
eukaryote_mean <- mean(da.top.1.phona$Eukaryote, na.rm = TRUE)
eukaryote_sem <- sd(da.top.1.phona$Eukaryote, na.rm = TRUE) / sqrt(length(na.omit(da.top.1.phona$Eukaryote)))

# Calculate mean and SEM for Prokaryote column
prokaryote_mean <- mean(da.top.1.phona$Prokaryote, na.rm = TRUE)
prokaryote_sem <- sd(da.top.1.phona$Prokaryote, na.rm = TRUE) / sqrt(length(na.omit(da.top.1.phona$Prokaryote)))

list(
  Eukaryote = list(Mean = eukaryote_mean, SEM = eukaryote_sem),
  Prokaryote = list(Mean = prokaryote_mean, SEM = prokaryote_sem)
)

##Now for overall node average
node.agro <- da.top.1.phona
# Calculate the total per row without modifying the original dataframe
node.agro$Total <- rowSums(node.agro[, c("Eukaryote", "Prokaryote")], na.rm = TRUE)

# Calculate mean and SEM for Total column
total_mean <- mean(node.agro$Total, na.rm = TRUE)
total_sem <- sd(node.agro$Total, na.rm = TRUE) / sqrt(length(na.omit(node.agro$Total)))

list(
  Total = list(Mean = total_mean, SEM = total_sem)
)

##Now, get edge info
phona.agro.edge.info <- rbind(phona.yield.sum, phona.shoot.sum, phona.pod.sum, phona.seed.sum, phona.root.sum)

mean_edge <- mean(phona.agro.edge.info$edge, na.rm = TRUE)
sem_edge <- sd(phona.agro.edge.info$edge, na.rm = TRUE) / sqrt(length(na.omit(phona.agro.edge.info$edge)))

list(
  Edge = list(Mean = mean_edge, SEM = sem_edge)
)
```

```{r}
##Let's connect the dots by looking at RA of genera in relation to participant
##First, let's get genera used
phona.net.tax <- phona.left.anno
phona.net.tax$genus <- rownames (phona.net.tax)

##merge
#view(phona.prok.genus.otu)
phona.prok.ra <- merge(phona.net.tax, phona.prok.genus.otu, by = "row.names")
phona.euk.ra <- merge(phona.net.tax, phona.euk.genus.otu, by = "row.names")
phona.phenotype.ra <- rbind(phona.prok.ra, phona.euk.ra)

##Format to simplify heatmap
rownames(phona.phenotype.ra) <- phona.phenotype.ra$Row.names
phona.phenotype.ra.2 <- phona.phenotype.ra[, -c(1:7)]
phona.phenotype.ra.2 <- phona.phenotype.ra.2[order(rownames(phona.phenotype.ra.2)), ]
phona.phenotype.ra.t <- t(phona.phenotype.ra.2)

##bottom annotation
##get Domain annotation
agro.domain <- as.data.frame(phona.1.tax.2.filtered)
agro.domain <- as.data.frame(tax_table(phona.seed.phylo))
agro.domain <- agro.domain %>%
  mutate(Domain = ifelse(Kingdom == "Bacteria", "Prokaryote", "Eukaryote"))
agro.domain <- dplyr::select(agro.domain, -(1:8))
colours.phona <- list('Domain' = c('Prokaryote' = "#a8c0a8", 'Eukaryote' = "#a890a8"))
##Reorder rows to match heatmap
column_order <- colnames(phona.phenotype.ra.t)
# Reorder agro.domain according to this column order
agro.domain <- agro.domain[column_order, , drop = FALSE]

##Need to recreate right annotation
colAnn.agro.bottom <- HeatmapAnnotation(df = agro.domain,
  which = 'col',
  col = colours.phona,
  annotation_width = unit(c(1, 4), 'cm'),
  gap = unit(0.25, 'mm'), simple_anno_size = unit(0.2, "cm"), show_legend = FALSE, show_annotation_name = FALSE)

##Top annotation obtained from phona.row.left
agro.top.anno = HeatmapAnnotation("# net"= anno_barplot(phona.left.anno.3, axis_param = list(side = "left"), 
                      gp = gpar(fill = c("lightblue3"), lwd = 0.1)), 
                       which = c("column"),   show_annotation_name = FALSE)

phona.ra.heat.1 <- Heatmap(
  phona.phenotype.ra.t,
  cluster_columns = TRUE,
  cluster_rows = TRUE,
  na_col = "gray57",
  border = TRUE,
  #rect_gp = gpar(col = "black", lwd = 0.5),
  col = edaphic.ra.pal,
  show_column_names = FALSE,
  show_row_names = FALSE,
  heatmap_legend_param = lgd_list.edaphic.ra,
 right_annotation = edaphic.ra.rowAnn, ##same as heatmaps for edaphic phona
  bottom_annotation = colAnn.agro.bottom,
  top_annotation = agro.top.anno,
  column_split = agro.domain$Domain,
  row_gap = unit(c(2), "mm"),
  row_title = NULL,
  column_title = NULL,
  column_names_rot = 45,
  row_names_gp = gpar(fontsize = 10))

phona.ra.heat.2 <- draw(phona.ra.heat.1, heatmap_legend_side = "bottom", legend_grouping = "original")

tiff("phona.agro.heatmap.ra.tiff", width = 12, height = 8, units = "in", res = 900)
phona.ra.heat.2
dev.off() 
```

```{r}
##Let's pick some genera to look at more deeply
##To prioritize microbes, we will first get RA and mean.composite in the same dataframe
view(phona.left.anno.3)
view(phona.phenotype.ra.2)
agro.cor <- merge(phona.left.anno.3, phona.phenotype.ra.2, by = "row.names")
rownames(agro.cor) <- agro.cor$Row.names
agro.cor$Row.names <- NULL

##Filter those to 0.2% prevalence
## Filtering rows with non-zero values in at least 20% of columns 2-65
cols <- 2:65

## Filtering rows with non-zero values in at least 20% of columns 2-65
agro.cor.1 <- agro.cor[rowSums(agro.cor[,cols] != 0) >= 0.2*length(cols), ]

##take top 20 taxa by composite score
agro.cor.2 <- agro.cor.1 %>%
  arrange(desc(mean.composite)) %>%
  slice_head(n = 20) %>%
  mutate(RowNames = rownames(.))

rownames(agro.cor.2) <- agro.cor.2$RowNames

agro.cor.2 <- agro.cor.2 %>% select(-RowNames)

##Let's look at the correlation between genera
##I want to use CSS instead of RA, since CSS was used for co-occurence networks

##Get CSS
css <- as.data.frame(otu_table(filtered_obj))
##Keep only that for top 20
rows_to_keep <- rownames(css) %in% rownames(agro.cor.2)
agro.cor.3 <- css[rows_to_keep, ]

##Transpose
agro.cor.3.t <- t(agro.cor.3)
##Correlation
agro.core <- rcorr(as.matrix(agro.cor.3.t), type = "spearman")
#view(agro.core$r)

##Visualize
corrplot(agro.core$r, p.mat = agro.core$P, method = 'color', diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
         insig = 'label_sig', pch.col = 'black', order = 'AOE', outline = TRUE, tl.col = "black", tl.srt=45, font = 3, 
         tl.cex = 0.5, col=colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10))

##Now, let's look at the correlation between genera and agronomic properties
agro.cor.gen <- as.data.frame(agro.cor.3.t)

##Remove samples for which we do not have agronomic data. Let's start by doing correlation for yield, so only keep sample_49-sample_63
rows_to_exclude <- c(paste0("sample_", 1:48), "sample_64")
agro.cor.gen.yield <- agro.cor.gen[!rownames(agro.cor.gen) %in% rows_to_exclude, ]
##Merge with yield data
agro.cor.gen.yield.2 <- merge(phona.2.sample.data, agro.cor.gen.yield, by = "row.names") 
agro.cor.gen.yield.3 <- dplyr::select(agro.cor.gen.yield.2, -c(1:35))

##Correlate
yield.corr <- rcorr(as.matrix(agro.cor.gen.yield.3), type = "spearman")
yield.corr.r <- as.data.frame(yield.corr$r)
yield.corr.p <- as.data.frame(yield.corr$P)

##Filter
yield.corr.r <- yield.corr.r[-c(2:22), -1]
yield.corr.p <- yield.corr.p[-c(2:22), -1]

##Let's do for 100-seed weight
seed.cor <- as.data.frame(sampleData(phona.seed.phylo))
rows_to_exclude.seed <- c(paste0("sample_", 1:48))
agro.cor.gen.seed <- agro.cor.gen[!rownames(agro.cor.gen) %in% rows_to_exclude.seed, ]

##Merge
agro.cor.seed <- merge(agro.cor.gen.seed, seed.cor, by = "row.names")
agro.cor.seed.1 <- agro.cor.seed[, -c(1, 22:25, 27:29)]

##Correlate
seed.corr <- rcorr(as.matrix(agro.cor.seed.1), type = "spearman")
seed.corr.r <- as.data.frame(seed.corr$r)
seed.corr.p <- as.data.frame(seed.corr$P)

##Filter
seed.corr.r <- seed.corr.r[-(1:20), ]
seed.corr.r <- seed.corr.r[, -21]

seed.corr.p <- seed.corr.p[-(1:20), ]
seed.corr.p <- seed.corr.p[, -21]

##Now for remaining agronomic properties
plant.data.glmm.2.df <- as.data.frame(plant.data.glmm.2)
rownames(plant.data.glmm.2.df) <- rownames(plant.data.glmm.2)
agro.cor.remain <- merge(agro.cor.gen.seed, plant.data.glmm.2.df, by = "row.names")
agro.cor.remain.2 <- agro.cor.remain[, -c(1, 22:24, 28:33)]

##Correlate
agro.corr <- rcorr(as.matrix(agro.cor.remain.2), type = "spearman")
agro.corr.r <- as.data.frame(agro.corr$r)
agro.corr.p <- as.data.frame(agro.corr$P)

##Filter
agro.corr.r <- agro.corr.r[-(1:20), -(21:23)]
agro.corr.p <- agro.corr.p[-(1:20), -(21:23)]

##Combine and visualize
agronomic.correlation.r <- rbind(seed.corr.r, yield.corr.r, agro.corr.r)
agronomic.correlation.p <- rbind(seed.corr.p, yield.corr.p, agro.corr.p)

# Rename rownames in agronomic.correlation.r
rownames(agronomic.correlation.r)[rownames(agronomic.correlation.r) == "Corrected_Weight"] <- "100-seed Weight"
rownames(agronomic.correlation.r)[rownames(agronomic.correlation.r) == "kg_ha"] <- "Theoretical Yield"
rownames(agronomic.correlation.r)[rownames(agronomic.correlation.r) == "Pod_Count"] <- "Pods/Plant"
rownames(agronomic.correlation.r)[rownames(agronomic.correlation.r) == "Root_Biomass"] <- "Belowground Biomass"
rownames(agronomic.correlation.r)[rownames(agronomic.correlation.r) == "Shoot_Biomass"] <- "Aboveground Biomass"

# Rename rownames in agronomic.correlation.p
rownames(agronomic.correlation.p)[rownames(agronomic.correlation.p) == "Corrected_Weight"] <- "100-seed Weight"
rownames(agronomic.correlation.p)[rownames(agronomic.correlation.p) == "kg_ha"] <- "Theoretical Yield"
rownames(agronomic.correlation.p)[rownames(agronomic.correlation.p) == "Pod_Count"] <- "Pods/Plant"
rownames(agronomic.correlation.p)[rownames(agronomic.correlation.p) == "Root_Biomass"] <- "Belowground Biomass"
rownames(agronomic.correlation.p)[rownames(agronomic.correlation.p) == "Shoot_Biomass"] <- "Aboveground Biomass"

agronomic.correlation.r.t <- t(agronomic.correlation.r)
agronomic.correlation.r.p <- t(agronomic.correlation.p)

##Visualize
genus.genus <- corrplot(as.matrix(agronomic.correlation.r), method = 'color', p.mat = as.matrix(agronomic.correlation.p), #diag = FALSE, #type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
         insig = 'label_sig', pch.col = 'black',  outline = TRUE, tl.col = "black", tl.srt=45, 
         tl.cex = 0.5, col=colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10))

custom_theme.2 <- theme(
  axis.text.y = element_text(face = "italic", color = "black", size = 10))

corrplot(agronomic.correlation.r.t, method = 'color', p.mat = agronomic.correlation.r.p, #type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
         insig = 'label_sig', pch.col = 'black',  outline = TRUE, tl.col = "black", tl.srt=45, 
         tl.cex = 0.5, col=colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10))

corrplot(agronomic.correlation.r.t, 
         method = 'color', 
         p.mat = agronomic.correlation.r.p, 
         sig.level = c(0.001, 0.01, 0.05), 
         pch.cex = 0.9,
         insig = 'label_sig', 
         pch.col = 'black',  
         outline = TRUE, 
         tl.col = "black", 
         tl.srt=45, 
         tl.cex = 0.5, 
         cl.pos = "n",  # Position the color legend at the bottom
         col = colorRampPalette(c("lightblue3", "antiquewhite", "#8c3800"))(10))
```

```{r}
##Info on top 20
agronomic.top.20 <- agro.cor.2

agronomic.top.20$genus <- rownames(agronomic.top.20)

agro.top.20 <- merge(agronomic.top.20, taxon.function.2, by = c("genus"))

##Let's generate additional correlation info
##First, for genus-genus
# Extract statistically significant associations from agro.core$P
significant_associations <- which(agro.core$P < 0.05, arr.ind = TRUE)

# Total number of significant associations
total_significant <- nrow(significant_associations)/2

# Extract the corresponding values from agro.core$r using the significant_associations indices
r_values_for_significant <- agro.core$r[significant_associations]

# Count positive and negative associations using the extracted r values, with na.omit
positive_associations_count <- sum(na.omit(r_values_for_significant) > 0)/2
negative_associations_count <- sum(na.omit(r_values_for_significant) < 0)/2

list(
  Total_Significant = total_significant,
  Positive_Associations = positive_associations_count,
  Negative_Associations = negative_associations_count
)

##Which node has the most associaitons?
# Calculate number of significant associations per genus
significant_counts <- rowSums(agro.core$P < 0.05, na.rm = TRUE)

# Identify genus (or genera) with the most significant associations
max_significant <- max(significant_counts, na.rm = TRUE)
top_genus <- names(which(significant_counts == max_significant))

# Define a helper function to get positive and negative association counts for a genus
get_signs <- function(genus_index) {
  significant_r_values <- agro.core$r[genus_index, ][agro.core$P[genus_index, ] < 0.05]
  
  list(
    Genus = rownames(agro.core$P)[genus_index],
    Total_Significant = sum(!is.na(significant_r_values)),
    Positive_Associations = sum(significant_r_values > 0, na.rm = TRUE),
    Negative_Associations = sum(significant_r_values < 0, na.rm = TRUE)
  )
}

# Get results for top genera
results <- lapply(which(rownames(agro.core$P) %in% top_genus), get_signs)

results

##Now, examine correlations between agronomic data and nodes
# Total number of significant correlations
significant_mask <- agronomic.correlation.r.p < 0.05
total_significant <- sum(significant_mask, na.rm = TRUE)

# Positive and negative correlations among these
positive_correlations <- sum(agronomic.correlation.r.t[significant_mask] > 0, na.rm = TRUE)
negative_correlations <- total_significant - positive_correlations

# Genus with the most significant associations
genus_counts <- rowSums(significant_mask, na.rm = TRUE)
max_genus_counts <- max(genus_counts, na.rm = TRUE)
top_genera <- rownames(agronomic.correlation.r.p)[genus_counts == max_genus_counts]

# Function to get details for each genus
get_genus_details <- function(genus) {
  index <- which(rownames(agronomic.correlation.r.p) == genus)
  sig_r_values <- agronomic.correlation.r.t[index, significant_mask[index, , drop = FALSE]]
  
  list(
    Genus = genus,
    Total_Significant = length(sig_r_values),
    Positive = sum(sig_r_values > 0, na.rm = TRUE),
    Negative = sum(sig_r_values < 0, na.rm = TRUE)
  )
}

genus_details <- lapply(top_genera, get_genus_details)

# Agronomic parameter with the most significant associations
param_counts <- colSums(significant_mask, na.rm = TRUE)
max_param_counts <- max(param_counts, na.rm = TRUE)
top_params <- colnames(agronomic.correlation.r.p)[param_counts == max_param_counts]

# Function to get details for each parameter
get_param_details <- function(param) {
  index <- which(colnames(agronomic.correlation.r.p) == param)
  sig_r_values <- agronomic.correlation.r.t[significant_mask[, index], index]
  
  list(
    Parameter = param,
    Total_Significant = length(sig_r_values),
    Positive = sum(sig_r_values > 0, na.rm = TRUE),
    Negative = sum(sig_r_values < 0, na.rm = TRUE)
  )
}

param_details <- lapply(top_params, get_param_details)

list(
  Total_Significant = total_significant,
  Positive = positive_correlations,
  Negative = negative_correlations,
  Genus_Details = genus_details,
  Param_Details = param_details
)
```

```{r}
# List of specific objects to be saved
specific_objects <- c("prok_phyloseq", "fun_phyloseq", "Field_ID", 
                      "Field_ID.2", "Field_ID.3", "Yield_Metric", 
                      "Plant_Data_GLMM", "X100_count_seed_weight", "samples_df")

# Find objects that contain both 'sample' and 'data'
sample_data_objects <- ls()[grepl("sample", ls()) & grepl("data", ls())]

# Find objects that contain both 'tax' and 'mat'
tax_mat_objects <- ls()[grepl("tax", ls()) & grepl("mat", ls())]

# Combine all objects to save
objects_to_save <- unique(c(specific_objects, sample_data_objects, tax_mat_objects))

# Save objects to .RData file
save(list = objects_to_save, file = "Hale_Data.RData")
```

